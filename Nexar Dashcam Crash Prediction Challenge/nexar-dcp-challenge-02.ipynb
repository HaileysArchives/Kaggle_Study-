{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92399,"databundleVersionId":11038207,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **ğŸ’¡Reference Notebook - Fernandosr85 - Dashcam Collision Prediction Project ğŸš—**\n#### **https://www.kaggle.com/code/fernandosr85/dashcam-collision-prediction-project**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nmax_files = 10 \n\ncount = 0\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        count += 1\n        if count >= max_files:\n            break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:57:55.459205Z","iopub.execute_input":"2025-05-01T13:57:55.459431Z","iopub.status.idle":"2025-05-01T13:58:00.057840Z","shell.execute_reply.started":"2025-05-01T13:57:55.459412Z","shell.execute_reply":"2025-05-01T13:58:00.057035Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nexar-collision-prediction/sample_submission.csv\n/kaggle/input/nexar-collision-prediction/train.csv\n/kaggle/input/nexar-collision-prediction/test.csv\n/kaggle/input/nexar-collision-prediction/test/02772.mp4\n/kaggle/input/nexar-collision-prediction/test/02807.mp4\n/kaggle/input/nexar-collision-prediction/test/02509.mp4\n/kaggle/input/nexar-collision-prediction/test/00350.mp4\n/kaggle/input/nexar-collision-prediction/test/02163.mp4\n/kaggle/input/nexar-collision-prediction/test/02707.mp4\n/kaggle/input/nexar-collision-prediction/test/02741.mp4\n/kaggle/input/nexar-collision-prediction/train/02059.mp4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport warnings\nfrom multiprocessing import Pool\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.models\n\nwarnings.filterwarnings(\"ignore\")\n\n# Check GPU availability and set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:00.059235Z","iopub.execute_input":"2025-05-01T13:58:00.059567Z","iopub.status.idle":"2025-05-01T13:58:07.899750Z","shell.execute_reply.started":"2025-05-01T13:58:00.059548Z","shell.execute_reply":"2025-05-01T13:58:07.899092Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Suppress unnecessary formatting warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Paths to the CSV files\ntrain_csv_path = '/kaggle/input/nexar-collision-prediction/train.csv'\ntest_csv_path = '/kaggle/input/nexar-collision-prediction/test.csv'\nsubmission_csv_path = '/kaggle/input/nexar-collision-prediction/sample_submission.csv'\n\n# Paths to the video directories\ntrain_video_dir = '/kaggle/input/nexar-collision-prediction/train'\ntest_video_dir = '/kaggle/input/nexar-collision-prediction/test'\n\n# Load the CSV files\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\nsubmission_df = pd.read_csv(submission_csv_path)\n\n# (ì¶”ê°€) id ì»¬ëŸ¼ì„ ë¬¸ìì—´(str)ë¡œ ë³€í™˜í•´ì„œ .0 ë¬¸ì œ ì—†ì• ê¸°\ntrain_df['id'] = train_df['id'].astype(str)\n\n# Display the first few rows of the DataFrames\nprint(\"Train.csv:\")\nprint(train_df.head())\n\nprint(\"\\nTest.csv:\")\nprint(test_df.head())\n\nprint(\"\\nSample Submission:\")\nprint(submission_df.head())\n\n# Optional: handle NaN values if needed, filling with zero or another value\ntrain_df['time_of_event'] = train_df['time_of_event'].fillna(0)\ntrain_df['time_of_alert'] = train_df['time_of_alert'].fillna(0)\n\n# (ì¶”ê°€) Check the video directory paths\nprint(\"\\nVideo Directory Paths:\")\nprint(f\"Train videos are located at: {train_video_dir}\")\nprint(f\"Test videos are located at: {test_video_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:07.900477Z","iopub.execute_input":"2025-05-01T13:58:07.900816Z","iopub.status.idle":"2025-05-01T13:58:07.941252Z","shell.execute_reply.started":"2025-05-01T13:58:07.900790Z","shell.execute_reply":"2025-05-01T13:58:07.940624Z"}},"outputs":[{"name":"stdout","text":"Train.csv:\n     id  time_of_event  time_of_alert  target\n0  1924            NaN            NaN       0\n1   822           19.5         18.633       1\n2  1429            NaN            NaN       0\n3   208           19.8         19.233       1\n4  1904            NaN            NaN       0\n\nTest.csv:\n    id\n0  204\n1   30\n2  146\n3   20\n4  511\n\nSample Submission:\n    id  target\n0  204       0\n1   30       0\n2  146       0\n3   20       0\n4  511       0\n\nVideo Directory Paths:\nTrain videos are located at: /kaggle/input/nexar-collision-prediction/train\nTest videos are located at: /kaggle/input/nexar-collision-prediction/test\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## **Data Preprocessing and Feature Extraction** ","metadata":{}},{"cell_type":"code","source":"# ì¼ë°˜ì ìœ¼ë¡œ ì¶©ëŒì´ ë°œìƒí•˜ëŠ” ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì´ˆì ì„ ë§ì¶° ë¹„ë””ì˜¤ì—ì„œ ì£¼ìš” í”„ë ˆì„ì„ ì¶”ì¶œ\n# ì§€ìˆ˜ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ì— ê°€ê¹Œìš´ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬\n\ndef extract_keyframes(video_path, num_frames=12, target_size=(160, 160)):\n    \"\"\"\n    Extracts key frames from the video, focusing on the final part where collisions typically occur.\n    Uses exponential distribution to give more weight to frames closer to the end.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path) # ë™ì˜ìƒì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ OpenCVì˜ videoCapture ê°ì²´ ìƒì„± \n\n    # íŒŒì¼ì´ ì œëŒ€ë¡œ ì—´ë¦¬ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬\n    if not cap.isOpened():\n        print(f\"Could not open the video: {video_path}\")\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n\n    # ì´ í”„ë ˆì„ ìˆ˜ì™€ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜(FPS)ë¥¼ ê°€ì ¸ì˜¤ê¸° \n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    if total_frames <= 0:\n        print(f\"Video without frames: {video_path}\")\n        cap.release()\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n    \n    # ì˜ìƒ ê¸¸ì´(ì´ˆ ë‹¨ìœ„) ê³„ì‚°\n    duration = total_frames / fps if fps > 0 else 0\n    \n    # ì§§ì€ ì˜ìƒ (10ì´ˆ ë¯¸ë§Œ): ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ\n    if duration < 10:\n        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n\n    # ê¸´ ì˜ìƒ (10ì´ˆ ì´ìƒ): í›„ë°˜ë¶€ì— ë” ì§‘ì¤‘í•´ì„œ ì¶”ì¶œ\n    else:\n        # ë§ˆì§€ë§‰ 3ì´ˆ ë™ì•ˆ í”„ë ˆì„ì˜ 80% ì§‘ì¤‘(ì¤‘ìš” ì˜ì—­)\n        end_frames = int(num_frames * 0.8)\n        start_frames = num_frames - end_frames\n        \n        # ì§€ë‚œ 3ì´ˆ ë™ì•ˆì˜ ì‹œì‘ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°\n        last_seconds = 3\n        last_frame_count = min(int(fps * last_seconds), total_frames - 1)\n        start_idx = max(0, total_frames - last_frame_count)\n        \n        # ë§ˆì§€ë§‰ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì§€ìˆ˜ ë¶„í¬\n        # ì´ë ‡ê²Œ í•˜ë©´ ë§ˆì§€ë§‰ì— ë” ë°€ì§‘ëœ ì¸ë±ìŠ¤ê°€ ìƒì„±ëœë‹¤ (\"í”„ë ˆì„ì„ ë½‘ëŠ” ê°„ê²©\"ìì²´ë¥¼ ì¡°ì ˆ â†’ ëë¶€ë¶„ì— ë” ë§ì´ ëª°ë¦¬ê²Œ ë§Œë“œëŠ” ë°©ì‹)\n        end_indices = np.array([\n            start_idx + int((total_frames - start_idx - 1) * (i/end_frames)**2) \n            for i in range(1, end_frames + 1)\n        ])\n        \n        # contextì— ë§ê²Œ ê· ì¼í•˜ê²Œ ë°°í¬ëœ ì´ˆê¸° í”„ë ˆì„ (ì´ˆë°˜ë¶€ì—ì„œ ê· ë“±í•˜ê²Œ ì¶”ì¶œí•œ í”„ë ˆì„ë“¤)\n        # contextë€? ì‚¬ê³  ì§ì „ì— ì–´ë–¤ ìƒí™©ì´ í¼ì³ì¡ŒëŠ”ì§€ì— ëŒ€í•œ íë¦„, ë°°ê²½, ë§¥ë½ \n        begin_indices = np.linspace(0, start_idx - 1, start_frames, dtype=int) if start_idx > 0 else np.zeros(start_frames, dtype=int)\n        \n        # ì¸ë±ìŠ¤ ê²°í•©\n        frame_indices = np.concatenate([begin_indices, end_indices])\n    \n    # ì„ íƒí•œ í”„ë ˆì„ ì¶”ì¶œ \n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            # Use higher resolution and better interpolation\n            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        else:\n            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n    \n    cap.release()\n    return np.array(frames, dtype=np.uint8)\n\n# ë¨¼ì €, ì „ì—­ ë²”ìœ„ì—ì„œ ë³€í™˜ í´ë˜ìŠ¤ë¥¼ ì •ì˜ \n# ì…ë ¥ëœ ì˜ìƒ í”„ë ˆì„ì„ ì¼ì • í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ì‹œì¼œì„œ, ë°ì´í„° ë‹¤ì–‘ì„±ì„ ëŠ˜ë¦¬ëŠ” ì—­í• \nclass RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n        \n    def __call__(self, frames):\n        if np.random.random() < self.p:\n            return frames[:, :, ::-1, :].copy()  # horizontally flip each frame\n        return frames\n\n# ì˜ìƒ í”„ë ˆì„ì˜ ë°ê¸°ì™€ ëŒ€ë¹„ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°ì •í•´, ë‹¤ì–‘í•œ ì¡°ëª… í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ì¦ê°• í´ë˜ìŠ¤\nclass ColorJitter(object):\n    def __init__(self, brightness=0, contrast=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        \n    def __call__(self, frames):\n        # Apply brightness jitter\n        if self.brightness > 0:\n            brightness_factor = np.random.uniform(max(0, 1-self.brightness), 1+self.brightness)\n            frames = frames * brightness_factor\n            frames = np.clip(frames, 0, 255)\n        \n        # Apply contrast jitter\n        if self.contrast > 0:\n            contrast_factor = np.random.uniform(max(0, 1-self.contrast), 1+self.contrast)\n            frames = (frames - 128) * contrast_factor + 128\n            frames = np.clip(frames, 0, 255)\n            \n        return frames\n\n# í”„ë ˆì„ì— íë¦¿í•œ ì•ˆê°œ íš¨ê³¼ë¥¼ ë„£ì–´, ì‹œì•¼ê°€ ë‚˜ìœ ë‚ ì”¨ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\nclass AddFog(object):\n    def __call__(self, frames):\n        fog = np.random.uniform(0.7, 0.9, frames.shape).astype(np.float32)\n        return frames * 0.8 + fog * 50  # Adjusted for 0-255 scale\n\n# í”„ë ˆì„ì— í°ìƒ‰ ì„ í˜• ë…¸ì´ì¦ˆ(ë¹—ë°©ìš¸)ë¥¼ ì¶”ê°€í•´ ë¹„ ì˜¤ëŠ” ë‚ ì”¨ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\nclass AddRain(object):\n    def __call__(self, frames):\n        h, w = frames.shape[1:3]\n        rain = np.random.uniform(0, 1, (len(frames), h, w, 1)).astype(np.float32)\n        rain = (rain > 0.97).astype(np.float32) * 200  # White rain drops\n        return np.clip(frames * 0.9 + rain, 0, 255)  # Darken a bit and add drops\n\n# ì§€ì •ëœ í™•ë¥ ì— ë”°ë¼ ì–´ë–¤ ë³€í™˜ì„ ì ìš©í• ì§€ ë§ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê²°ì •í•˜ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ í´ë˜ìŠ¤(ëœë¤ì„± ë¶€ì—¬)\nclass RandomApply(object):\n    def __init__(self, transform, p=0.5):\n        self.transform = transform\n        self.p = p\n        \n    def __call__(self, frames):\n        if np.random.random() < self.p:\n            return self.transform(frames)\n        return frames\n\n# ì—¬ëŸ¬ ê°œì˜ ë³€í™˜(Flip, Jitter, Fog ë“±)ì„ ìˆœì„œëŒ€ë¡œ ì ìš©í•˜ëŠ” ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def __call__(self, frames):\n        for t in self.transforms:\n            frames = t(frames)\n        return frames\n\n# ì˜ìƒ í”„ë ˆì„ ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë°”ê¾¸ê³ , í”½ì…€ ê°’ì„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ëŠ” í´ë˜ìŠ¤\nclass ToTensor(object):\n    def __call__(self, frames):\n        # Convert from (T, H, W, C) to (T, C, H, W)\n        frames = frames.transpose(0, 3, 1, 2)\n        # Convert to tensor and normalize to [0, 1]\n        return torch.from_numpy(frames).float() / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:07.941942Z","iopub.execute_input":"2025-05-01T13:58:07.942169Z","iopub.status.idle":"2025-05-01T13:58:07.959560Z","shell.execute_reply.started":"2025-05-01T13:58:07.942152Z","shell.execute_reply":"2025-05-01T13:58:07.958763Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ë™ì˜ìƒì—ì„œ ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ ë³€í™˜ì„ ë°˜í™˜\n\ndef get_video_transforms():\n    \"\"\"\n    Returns transformations for data augmentation in videos.\n    \"\"\"\n    return {\n        'train': Compose([\n            RandomHorizontalFlip(p=0.5),\n            ColorJitter(brightness=0.3, contrast=0.3),\n            RandomApply(AddFog(), p=0.15),\n            RandomApply(AddRain(), p=0.15),\n            RandomApply(RandomNoise(0.05), p=0.2), \n            RandomApply(RandomOcclusion(), p=0.1),\n            ToTensor()\n        ]),\n        'val': Compose([\n            ToTensor()  # Only tensor conversion for validation\n        ])\n    }\n\n# ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ë¬´ì‘ìœ„ ê°€ìš°ì‹œì•ˆ(ì •ê·œë¶„í¬) ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬, ì‹¤ì œ ì´¬ì˜ í™˜ê²½ì—ì„œ \n# ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¡ìŒì— ëŒ€í•´ ëª¨ë¸ì´ ë” ê°•ê±´í•´ì§€ë„ë¡ ë§Œë“œëŠ” í´ë˜ìŠ¤\nclass RandomNoise(object):\n    \"\"\"\n    Applies random Gaussian noise to video frames for data augmentation.\n    \n    This transformation helps the model become more robust to noise\n    that may be present in real-world video data.\n    \n    Args:\n        std (float): Standard deviation of the Gaussian noise as a fraction\n                     of the pixel value range (default: 0.05)\n    \"\"\"\n    def __init__(self, std=0.05):\n        self.std = std\n        \n    def __call__(self, frames):\n        \"\"\"\n        Apply random noise to the input frames.\n        \n        Args:\n            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n                                   where T is number of frames\n        \n        Returns:\n            numpy.ndarray: Noise-augmented frames, clipped to valid pixel range [0, 255]\n        \"\"\"\n        # ì§€ì •ëœ í‘œì¤€ í¸ì°¨ë¥¼ ê°€ì§„ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±\n        noise = np.random.normal(0, self.std * 255, frames.shape).astype(np.float32)\n        \n        # ìœ íš¨í•œ í”½ì…€ ë²”ìœ„ì— ë…¸ì´ì¦ˆ ë° í´ë¦½ ì¶”ê°€í•˜ê¸°\n        # ì˜ìƒì€ ì •ìˆ˜í˜• ë°ì´í„°ì—¬ì•¼ í•˜ë¯€ë¡œ í˜• ë³€í™˜ (astype)\n        return np.clip(frames + noise, 0, 255).astype(np.uint8)\n\n# ì˜ìƒ í”„ë ˆì„ì— ê²€ì€ìƒ‰ ì‚¬ê°í˜•ì„ ë¬´ì‘ìœ„ë¡œ ë®ì–´ ì”Œì›Œ, ì¼ë¶€ ì •ë³´ê°€ ê°€ë ¤ì¡Œì„ ë•Œë„ ëª¨ë¸ì´ ê²¬ë”œ ìˆ˜ ìˆë„ë¡ í›ˆë ¨ì‹œí‚¤ëŠ” í´ë˜ìŠ¤\nclass RandomOcclusion(object):\n    \"\"\"\n    Simulates occlusion in video frames by adding black rectangles.\n    \n    This transformation helps the model learn to handle partial occlusions\n    that may occur in real-world scenarios when objects block the camera view.\n    \"\"\"\n    def __call__(self, frames):\n        \"\"\"\n        Apply random occlusion to the input frames.\n        \n        Args:\n            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n                                   where T is number of frames\n        \n        Returns:\n            numpy.ndarray: Frames with random occlusion applied\n        \"\"\"\n        # í”„ë ˆì„ í•˜ë‚˜ì˜ ì„¸ë¡œ(h), ê°€ë¡œ(w) ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°\n        h, w = frames.shape[1:3]\n        \n        # ì „ì²´ í”„ë ˆì„ í¬ê¸°ì˜ 10%~25% ì‚¬ì´ í¬ê¸°ì˜ ê°€ë¦¼ ì˜ì—­ í¬ê¸° ì„¤ì •\n        occl_h = np.random.randint(int(h * 0.1), int(h * 0.25))\n        occl_w = np.random.randint(int(w * 0.1), int(w * 0.25))\n        \n        # ì´ ê°€ë¦¼ ì˜ì—­ì´ ë“¤ì–´ê°ˆ ë¬´ì‘ìœ„ ìœ„ì¹˜ ì¢Œí‘œ ì„¤ì • \n        occl_x = np.random.randint(0, w - occl_w)\n        occl_y = np.random.randint(0, h - occl_h)\n        \n        # ì›ë³¸ í”„ë ˆì„ì„ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ ë³µì‚¬ë³¸ ë§Œë“¤ê¸°\n        frames_copy = frames.copy()\n        \n        # í”½ì…€ì„ 0(ê²€ì •ìƒ‰)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  í”„ë ˆì„ì— occlusion ì ìš©\n        for i in range(len(frames)):\n            frames_copy[i, occl_y:occl_y+occl_h, occl_x:occl_x+occl_w, :] = 0\n            \n        return frames_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:07.961507Z","iopub.execute_input":"2025-05-01T13:58:07.961719Z","iopub.status.idle":"2025-05-01T13:58:07.975489Z","shell.execute_reply.started":"2025-05-01T13:58:07.961701Z","shell.execute_reply":"2025-05-01T13:58:07.974821Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ë¹„ë””ì˜¤ í”„ë ˆì„ ê°„ ì›€ì§ì„(ëª¨ì…˜)ì„ ì¶”ì í•˜ëŠ” 'optical_flow'ë¥¼ ê³„ì‚°í•´, ê°ì²´ë‚˜ ë°°ê²½ì˜ ì´ë™ ë°©í–¥ê³¼ ì†ë„ë¥¼ ë²¡í„° í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n# ë‘ ì—°ì†ëœ ì´ë¯¸ì§€(ë˜ëŠ” í”„ë ˆì„) ì‚¬ì´ì—ì„œ, ê° í”½ì…€ì´ ì–´ë–»ê²Œ ì´ë™í–ˆëŠ”ì§€ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ  -> optical_flow\n# Farneback ë°©ì‹ë§Œ ì‚¬ìš©\n# \"ëª¨ë“  í”½ì…€ì˜ ë°©í–¥ + ì†ë„ ì •ë³´ë¥¼ ë‹¤ ë‚¨ê¹€\"\ndef compute_optical_flow_sequence(frames, skip_frames=1):\n    \"\"\"\n    Calculates per-frame optical flow magnitudes as a sequence.\n    \n    Args:\n        frames (numpy.ndarray): (T, H, W, C)\n        \n    Returns:\n        numpy.ndarray: (T, 1) array of flow magnitudes (first frame is 0)\n    \"\"\"\n    T = len(frames)\n    if T < 2:\n        return np.zeros((T, 1), dtype=np.float32)\n    \n    magnitudes = [0.0]  # ì²« í”„ë ˆì„ì€ optical flowê°€ ì—†ìœ¼ë‹ˆ 0ìœ¼ë¡œ ì±„ì›€\n\n    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n    \n    for i in range(1, T, skip_frames):\n        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n        try:\n            flow = cv2.calcOpticalFlowFarneback(\n                prev_gray, curr_gray,\n                None, 0.5, 3, 15, 3, 5, 1.2, 0\n            )\n            flow_magnitude = np.linalg.norm(flow, axis=-1).mean()  # (H, W) â†’ scalar mean\n            magnitudes.append(flow_magnitude)\n        except Exception as e:\n            print(f\"Error calculating optical flow: {str(e)}\")\n            magnitudes.append(0.0)\n        \n        prev_gray = curr_gray\n\n    # ê¸¸ì´ê°€ ë¶€ì¡±í•˜ë©´ padding\n    while len(magnitudes) < T:\n        magnitudes.append(0.0)\n    \n    return np.array(magnitudes, dtype=np.float32).reshape(T, 1)  # (T, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:07.976222Z","iopub.execute_input":"2025-05-01T13:58:07.976481Z","iopub.status.idle":"2025-05-01T13:58:07.986888Z","shell.execute_reply.started":"2025-05-01T13:58:07.976457Z","shell.execute_reply":"2025-05-01T13:58:07.986171Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:07.987688Z","iopub.execute_input":"2025-05-01T13:58:07.987948Z","iopub.status.idle":"2025-05-01T13:58:21.189617Z","shell.execute_reply.started":"2025-05-01T13:58:07.987931Z","shell.execute_reply":"2025-05-01T13:58:21.188853Z"}},"outputs":[{"name":"stderr","text":"2025-05-01 13:58:09.278280: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746107889.457874      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746107889.509611      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"**Positiveì´ë©´ alert_event ì‚¬ì´ë§Œ ë½‘ê³ , Negativeì´ë©´ ë§ˆì§€ë§‰ 3ì´ˆ êµ¬ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ**\n**ê·¸ë¦¬ê³  num_framesë§Œí¼ ê· ë“±í•˜ê²Œ ë½‘ê³  CNN + Optical Flow ë‘˜ ë‹¤ ê³„ì‚°** ","metadata":{}},{"cell_type":"code","source":"# InceptionV3 ëª¨ë¸ë¡œ íŠ¹ì„± ì¶”ì¶œ\nbase_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\ncnn_feature_dim = base_model.output_shape[-1]\n\ndef get_hybrid_feature_sequence(video_path, num_frames=12):\n    \"\"\"\n    Extract per-frame hybrid features (CNN + Optical flow) as a sequence.\n    \n    Args:\n        video_path (str): Path to video file.\n        num_frames (int): Number of frames to extract.\n    \n    Returns:\n        np.ndarray: (T, 1281) array of per-frame features.\n    \"\"\"\n    # 1. í”„ë ˆì„ ì¶”ì¶œ\n    frames = extract_keyframes(video_path, num_frames=num_frames, target_size=(160,160))\n    \n    if len(frames) == 0:\n        print(f\"Skipping {video_path}: no frames\")\n        return np.zeros((num_frames, 1281), dtype=np.float32)\n\n    # 2. CNN feature per frame (Inception expects (N, H, W, C))\n    spatial_features = base_model.predict(\n        preprocess_input(frames.astype('float32')),\n        batch_size=32,\n        verbose=0\n    )  # shape: (T, 1280)\n\n    # 3. Optical flow sequence\n    flow_magnitudes = compute_optical_flow_sequence(frames)  # shape: (T, 1)\n\n    # 4. Concatenate per frame\n    hybrid_features = np.concatenate([spatial_features, flow_magnitudes], axis=1)  # (T, 1281)\n\n    return hybrid_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:21.190525Z","iopub.execute_input":"2025-05-01T13:58:21.191366Z","iopub.status.idle":"2025-05-01T13:58:25.952768Z","shell.execute_reply.started":"2025-05-01T13:58:21.191332Z","shell.execute_reply":"2025-05-01T13:58:25.952126Z"}},"outputs":[{"name":"stderr","text":"2025-05-01 13:58:21.201063: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def get_hybrid_feature_sequence_from_frames(frames):\n    \"\"\"\n    Extract per-frame hybrid features (CNN + Optical flow) from pre-loaded frames.\n    \n    Args:\n        frames (torch.Tensor): (T, 3, 160, 160) tensor (after transform).\n    \n    Returns:\n        np.ndarray: (T, 1281) array of per-frame features.\n    \"\"\"\n    if len(frames) == 0:\n        print(\"Warning: empty frames input\")\n        return np.zeros((1, 1281), dtype=np.float32)\n\n    # 1ï¸âƒ£ PyTorch tensor â†’ numpy (T, 160, 160, 3), [0, 255] scale\n    frames_np = frames.permute(0, 2, 3, 1).numpy() * 255.0  # [0,1] â†’ [0,255]\n    frames_np = frames_np.astype(np.uint8)\n\n    # 2ï¸âƒ£ CNN Features per frame\n    spatial_features = base_model.predict(\n        preprocess_input(frames_np.astype('float32')),\n        batch_size=32,\n        verbose=0\n    )  # shape: (T, 1280)\n\n    # 3ï¸âƒ£ Optical Flow per frame\n    flow_magnitudes = compute_optical_flow_sequence(frames_np)  # shape: (T, 1)\n\n    # 4ï¸âƒ£ Concatenate â†’ (T, 1281)\n    hybrid_features = np.concatenate([spatial_features, flow_magnitudes], axis=1)\n\n    return hybrid_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:25.953550Z","iopub.execute_input":"2025-05-01T13:58:25.953800Z","iopub.status.idle":"2025-05-01T13:58:25.960695Z","shell.execute_reply.started":"2025-05-01T13:58:25.953776Z","shell.execute_reply":"2025-05-01T13:58:25.960017Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def compute_optical_flow_sequence(frames, skip_frames=1):\n    \"\"\"\n    Computes per-frame optical flow magnitudes.\n    \n    Args:\n        frames (np.ndarray): (T, H, W, 3) numpy array of frames.\n    \n    Returns:\n        np.ndarray: (T, 1) array of per-frame optical flow magnitudes.\n    \"\"\"\n    T = len(frames)\n    if T < 2:\n        return np.zeros((T, 1), dtype=np.float32)\n\n    magnitudes = []\n\n    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n\n    for i in range(1, T, skip_frames):\n        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n        try:\n            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray,\n                                                None, 0.5, 3, 15, 3, 5, 1.2, 0)\n            # magnitude = sqrt(u^2 + v^2)\n            mag = np.linalg.norm(flow, axis=-1)  # shape (H, W)\n            avg_mag = np.mean(mag)  # scalar\n            magnitudes.append(avg_mag)\n        except Exception as e:\n            print(f\"Error calculating flow at frame {i}: {str(e)}\")\n            magnitudes.append(0.0)\n\n        prev_gray = curr_gray\n\n    # ë§ˆì§€ë§‰ ê¸¸ì´ ë§ì¶¤ (T, 1)\n    if len(magnitudes) < T:\n        magnitudes.append(0.0)  # ë§ˆì§€ë§‰ í”„ë ˆì„ì€ flowê°€ ì—†ìŒ\n\n    magnitudes = np.array(magnitudes, dtype=np.float32).reshape(-1, 1)  # (T, 1)\n\n    return magnitudes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:25.961573Z","iopub.execute_input":"2025-05-01T13:58:25.961777Z","iopub.status.idle":"2025-05-01T13:58:26.006936Z","shell.execute_reply.started":"2025-05-01T13:58:25.961760Z","shell.execute_reply":"2025-05-01T13:58:26.005994Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ì•„ì§ë„ ìµœì¢…ì ìœ¼ë¡œ Transformerì— ë„£ì€ (T, 1281) ì‹œí€€ìŠ¤ëŠ” ë§Œë“¤ì–´ì§€ì§€ ì•ŠìŒ\n# 1. í”„ë ˆì„ë³„ CNN Feature ì¶”ì¶œ (InceptionV3) ì¶”ì¶œ\n# 2. optical flow sequence (compute_optical_flow_sequence) ì¶”ì¶œ\n# 3. ë‘ ê²°ê³¼ë¬¼ concat\n# 4. ì´ê±¸ Transformerì˜ input ì‹œí€€ìŠ¤ë¡œ ì‚¬ìš©\n\n# ì „ì²´ ì²˜ë¦¬ í•¨ìˆ˜ (ì´ì œ ë‘˜ì„ ê²°í•©í•˜ëŠ” í•¨ìˆ˜ ìƒì„±)\n# CNN + optical flow ë¶™ì—¬ì„œ (T, 1281) ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜ \n\ndef prepare_transformer_input(video_path, num_frames=12, target_size=(160, 160)):\n    \"\"\"\n    Prepares (T, 1281) input sequence combining CNN features + optical flow\n    for a given video.\n    \"\"\"\n    # === Step 1: Extract frames ===\n    frames = extract_keyframes(video_path, num_frames=num_frames, target_size=target_size)\n    if frames.shape[0] == 0:\n        print(f\"Skipping video {video_path} (no frames)\")\n        return None  # or np.zeros((num_frames, 1281)) as fallback\n\n    # === Step 2: Extract CNN (spatial) features per frame ===\n    frames_float = preprocess_input(frames.astype('float32'))  # preprocess for InceptionV3\n    spatial_features = base_model.predict(frames_float, batch_size=32, verbose=0)  # (T, 1280)\n\n    # === Step 3: Compute optical flow sequence ===\n    optical_flow_sequence = compute_optical_flow_sequence(frames)  # (T, 1)\n\n    # === Step 4: Combine both ===\n    combined_features = np.concatenate([spatial_features, optical_flow_sequence], axis=1)  # (T, 1281)\n\n    return combined_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T13:58:26.007810Z","iopub.execute_input":"2025-05-01T13:58:26.008107Z","iopub.status.idle":"2025-05-01T13:58:26.020674Z","shell.execute_reply.started":"2025-05-01T13:58:26.008082Z","shell.execute_reply":"2025-05-01T13:58:26.020026Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ì¤‘ê°„ ì €ì¥ í¬í•¨ ì½”ë“œ\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\n# 1ï¸âƒ£ transform ì¤€ë¹„\ntransforms = get_video_transforms()\ntrain_transform = transforms['train']\n\n# 2ï¸âƒ£ ì €ì¥í•  í´ë” ì„¤ì •\noutput_dir = '/kaggle/working/'\nos.makedirs(output_dir, exist_ok=True)\n\n# 3ï¸âƒ£ feature ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\nall_sequences = []\n\n# 4ï¸âƒ£ ì¤‘ê°„ ì €ì¥ ì£¼ê¸°\nsave_every = 50\n\n# 5ï¸âƒ£ ë°˜ë³µ\nfor idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n    video_id = row['id']\n    video_path = os.path.join(train_video_dir, f\"{int(video_id):05d}.mp4\")\n\n    sequence = prepare_transformer_input(video_path, num_frames=12)\n\n    if sequence is None:\n        print(f\"Skipping video {video_id} (no valid sequence)\")\n        continue\n\n    all_sequences.append(sequence)\n\n    # ğŸ”¥ Nê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥\n    if (idx + 1) % save_every == 0:\n        partial_path = os.path.join(output_dir, f'all_sequences_partial_{idx+1}.npy')\n        np.save(partial_path, np.array(all_sequences))\n        print(f\"Saved {idx + 1} sequences â†’ {partial_path}\")\n\n# 6ï¸âƒ£ ìµœì¢… ì €ì¥\nfinal_path = os.path.join(output_dir, 'all_sequences_final.npy')\nnp.save(final_path, np.array(all_sequences))\nprint(f\"\\nFinal saved â†’ {final_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:58:55.902868Z","iopub.execute_input":"2025-05-01T23:58:55.903493Z","iopub.status.idle":"2025-05-01T23:58:55.918073Z","shell.execute_reply.started":"2025-05-01T23:58:55.903465Z","shell.execute_reply":"2025-05-01T23:58:55.917008Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3875133653.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1ï¸âƒ£ transform ì¤€ë¹„\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_video_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'get_video_transforms' is not defined"],"ename":"NameError","evalue":"name 'get_video_transforms' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# ë‚˜ì¤‘ì— ì‹¤í–‰í•˜ëŠ” ì½”ë“œ \n\nimport os \nimport glob\nimport numpy as np\n\noutput_dir = '/kaggle/working/'\nall_files = sorted(glob.glob(os.path.join(output_dir, 'all_sequences_partial_*.npy')))\n\n# ê° íŒŒì¼ ë¡œë“œ\nall_loaded = [np.load(f, allow_pickle=True) for f in all_files]\n\n# ì´ì–´ë¶™ì´ê¸°\nfinal_array = np.concatenate(all_loaded, axis=0)\nprint(f'Final combined shape: {final_array.shape}')\n\n# ìµœì¢… combined arrayë¥¼ ë””ìŠ¤í¬ì— ì €ì¥\ncombined_save_path = os.path.join(output_dir, 'all_sequences_combined.npy')\nnp.save(combined_save_path, final_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:54:56.438998Z","iopub.execute_input":"2025-05-01T23:54:56.439242Z","iopub.status.idle":"2025-05-01T23:54:56.511806Z","shell.execute_reply.started":"2025-05-01T23:54:56.439216Z","shell.execute_reply":"2025-05-01T23:54:56.510858Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/675287772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/working/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all_sequences_partial_*.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ê° íŒŒì¼ ë¡œë“œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"],"ename":"NameError","evalue":"name 'os' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# (1) combined feature ë¶ˆëŸ¬ì˜¤ê¸°\nimport numpy as np\nall_sequences = np.load('/kaggle/working/all_sequences_combined.npy')\nprint(all_sequences.shape)  # â†’ (n_videos, 12, 2049) ê°™ì€ ì¶œë ¥ í™•ì¸\n\n# (2) train.csv ë‹¤ì‹œ ë¡œë“œ\nimport pandas as pd\ntrain_df = pd.read_csv('/kaggle/input/nexar-collision-prediction/train.csv')\n\n# (3) label ì¶”ì¶œ\nlabels = train_df['target'].values  # shape (n_videos,)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset í´ë˜ìŠ¤ ì •ì˜\nimport torch\nfrom torch.utils.data import Dataset\n\nclass VideoSequenceDataset(Dataset):\n    def __init__(self, sequences, labels):\n        \"\"\"\n        Args:\n            sequences (numpy.ndarray): shape (n_samples, T, feature_dim)\n            labels (numpy.ndarray or list): shape (n_samples,)\n        \"\"\"\n        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        x = self.sequences[idx]  # (T, feature_dim)\n        y = self.labels[idx]     # scalar or class\n        return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:54:56.512233Z","iopub.status.idle":"2025-05-01T23:54:56.512625Z","shell.execute_reply.started":"2025-05-01T23:54:56.512387Z","shell.execute_reply":"2025-05-01T23:54:56.512401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DataLoader ìƒì„±\nfrom torch.utils.data import DataLoader, random_split\n\ndataset = VideoSequenceDataset(all_sequences, labels)\n\n# Train/Val ë¶„í• \ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:47:15.468801Z","iopub.execute_input":"2025-05-01T15:47:15.469149Z","iopub.status.idle":"2025-05-01T15:47:15.602187Z","shell.execute_reply.started":"2025-05-01T15:47:15.469123Z","shell.execute_reply":"2025-05-01T15:47:15.601324Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Temporal Transformer ëª¨ë¸ ì„¤ê³„\nimport torch\nimport torch.nn as nn\n\nclass TemporalTransformerModel(nn.Module):\n    def __init__(self, input_dim=1281, embed_dim=256, num_heads=4, num_layers=2, dropout=0.1):\n        super(TemporalTransformerModel, self).__init__()\n\n        # 1. Input â†’ embedding layer\n        self.input_proj = nn.Linear(input_dim, embed_dim)\n\n        # 2. Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # 3. Classification head (binary classification)\n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n            nn.Sigmoid()  # Binary output (0~1)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, T, input_dim)\n        \"\"\"\n        # Step 1: Project input features\n        x = self.input_proj(x)  # â†’ (batch_size, T, embed_dim)\n\n        # Step 2: Apply Transformer Encoder\n        x = self.transformer_encoder(x)  # â†’ (batch_size, T, embed_dim)\n\n        # Step 3: Aggregate (mean pooling over time)\n        x = x.mean(dim=1)  # â†’ (batch_size, embed_dim)\n\n        # Step 4: Final classification\n        out = self.classifier(x)  # â†’ (batch_size, 1)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:47:15.603018Z","iopub.execute_input":"2025-05-01T15:47:15.603251Z","iopub.status.idle":"2025-05-01T15:47:15.610876Z","shell.execute_reply.started":"2025-05-01T15:47:15.603234Z","shell.execute_reply":"2025-05-01T15:47:15.610036Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Spatial Transformer ëª¨ë¸ ì„¤ê³„\n\nimport torch\nimport torch.nn as nn\n\nclass SpatialTransformer(nn.Module):\n    def __init__(self, input_dim=1280, hidden_dim=512, num_heads=8, num_layers=2, dropout=0.1):\n        super(SpatialTransformer, self).__init__()\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # ë§ˆì§€ë§‰ summaryë¥¼ ìœ„í•œ pooling ë˜ëŠ” projection\n        self.output_layer = nn.Linear(input_dim, input_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: shape (batch_size, T, input_dim) â†’ per-frame spatial features\n        \n        Returns:\n            out: shape (batch_size, input_dim) â†’ aggregated spatial feature\n        \"\"\"\n        # transformer expects (batch_size, T, input_dim)\n        x_transformed = self.transformer(x)  # (batch_size, T, input_dim)\n\n        # Pooling over time (mean pooling)\n        x_pooled = x_transformed.mean(dim=1)  # (batch_size, input_dim)\n\n        out = self.output_layer(x_pooled)  # (batch_size, input_dim)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:47:15.611769Z","iopub.execute_input":"2025-05-01T15:47:15.612045Z","iopub.status.idle":"2025-05-01T15:47:15.625266Z","shell.execute_reply.started":"2025-05-01T15:47:15.612022Z","shell.execute_reply":"2025-05-01T15:47:15.624462Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class CollisionPredictionModel(nn.Module):\n    def __init__(self, temporal_input_dim=2049, spatial_input_dim=1280, embed_dim=256, dropout=0.1):\n        super(CollisionPredictionModel, self).__init__()\n\n        self.temporal_transformer = TemporalTransformerModel(\n            input_dim=temporal_input_dim, embed_dim=embed_dim\n        )\n        self.spatial_transformer = SpatialTransformer(\n            input_dim=spatial_input_dim\n        )\n\n        fused_dim = embed_dim + spatial_input_dim  # Temporal + Spatial ì¶œë ¥ ì—°ê²°\n\n        self.classifier = nn.Sequential(\n            nn.Linear(fused_dim, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 1),\n            nn.Sigmoid()  # Binary classification\n        )\n\n    def forward(self, temporal_input, spatial_input):\n        \"\"\"\n        temporal_input: (batch, T, 2049)\n        spatial_input: (batch, T, 1280)\n        \"\"\"\n        temporal_out = self.temporal_transformer(temporal_input)  # (batch, embed_dim)\n        spatial_out = self.spatial_transformer(spatial_input)      # (batch, spatial_input_dim)\n\n        # Fusion: concatenate\n        fused = torch.cat([temporal_out, spatial_out], dim=1)  # (batch, fused_dim)\n\n        out = self.classifier(fused)  # (batch, 1)\n\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T15:47:39.716237Z","iopub.execute_input":"2025-05-01T15:47:39.716524Z","iopub.status.idle":"2025-05-01T15:47:39.723858Z","shell.execute_reply.started":"2025-05-01T15:47:39.716502Z","shell.execute_reply":"2025-05-01T15:47:39.723138Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TemporalTransformerModel(nn.Module):\n    def __init__(self, input_dim=2049, embed_dim=256, num_heads=4, num_layers=2, dropout=0.1):\n        super(TemporalTransformerModel, self).__init__()\n\n        self.input_proj = nn.Linear(input_dim, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x):\n        x = self.input_proj(x)            # (batch, T, embed_dim)\n        x = self.transformer_encoder(x)   # (batch, T, embed_dim)\n        x = x.mean(dim=1)                 # (batch, embed_dim)\n        return x\n\n\nclass SpatialTransformer(nn.Module):\n    def __init__(self, input_dim=1280, hidden_dim=512, num_heads=8, num_layers=2, dropout=0.1):\n        super(SpatialTransformer, self).__init__()\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.output_layer = nn.Linear(input_dim, input_dim)\n\n    def forward(self, x):\n        x_transformed = self.transformer(x)  # (batch, T, input_dim)\n        x_pooled = x_transformed.mean(dim=1) # (batch, input_dim)\n        out = self.output_layer(x_pooled)    # (batch, input_dim)\n        return out\n\n\nclass CombinedModel(nn.Module):\n    def __init__(self, temporal_input_dim=2049, spatial_input_dim=1280,\n                 temporal_embed_dim=256, combined_dim=256, dropout=0.1):\n        super(CombinedModel, self).__init__()\n\n        self.temporal_transformer = TemporalTransformerModel(\n            input_dim=temporal_input_dim, embed_dim=temporal_embed_dim\n        )\n        self.spatial_transformer = SpatialTransformer(\n            input_dim=spatial_input_dim\n        )\n\n        # temporal (256) + spatial (1280) = 1536\n        self.classifier = nn.Sequential(\n            nn.Linear(1536, combined_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(combined_dim, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, temporal_input, spatial_input):\n        temporal_out = self.temporal_transformer(temporal_input)  # (batch, 256)\n        spatial_out = self.spatial_transformer(spatial_input)     # (batch, 1280)\n\n        combined = torch.cat([temporal_out, spatial_out], dim=1) # (batch, 1536)\n        out = self.classifier(combined)                          # (batch, 1)\n\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:13:46.838398Z","iopub.execute_input":"2025-05-01T16:13:46.838997Z","iopub.status.idle":"2025-05-01T16:13:46.844925Z","shell.execute_reply.started":"2025-05-01T16:13:46.838949Z","shell.execute_reply":"2025-05-01T16:13:46.844207Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# ëª¨ë¸ ì¤€ë¹„\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CombinedModel(\n    temporal_input_dim=1281, spatial_input_dim=1280,\n    temporal_embed_dim=256, combined_dim=256\n).to(device)\n\n# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n\n    for inputs, labels in train_loader:\n        inputs = inputs.to(device)  # (batch, T, 2049)\n        labels = labels.to(device).unsqueeze(1)  # (batch, 1)\n\n        # temporal_input = spatial(1280) + flow(1) + ì¶”ê°€ optical flowë“¤ â†’ (2049)\n        temporal_input = inputs[:, :, :2049]\n\n        # spatial_input = spatial part only â†’ (1280)\n        spatial_input = inputs[:, :, :1280]\n\n        # forward\n        outputs = model(temporal_input, spatial_input)\n        loss = criterion(outputs, labels)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    avg_loss = running_loss / len(train_loader)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n    # === Validation ===\n    model.eval()\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device).unsqueeze(1)\n    \n            temporal_input = inputs[:, :, :2049]\n            spatial_input = inputs[:, :, :1280]\n    \n            outputs = model(temporal_input, spatial_input)\n            predicted = (outputs > 0.5).float()\n    \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n\n    val_acc = correct / total\n    print(f\"Validation Accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T16:13:48.450554Z","iopub.execute_input":"2025-05-01T16:13:48.451299Z","iopub.status.idle":"2025-05-01T16:13:48.660432Z","shell.execute_reply.started":"2025-05-01T16:13:48.451273Z","shell.execute_reply":"2025-05-01T16:13:48.659411Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2508763012.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporal_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1246925743.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, temporal_input, spatial_input)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemporal_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (batch, 1536)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;31m# (batch, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x1281 and 1536x256)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (16x1281 and 1536x256)","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}