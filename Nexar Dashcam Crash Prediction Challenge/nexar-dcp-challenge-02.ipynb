{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92399,"databundleVersionId":11038207,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **ğŸ’¡Reference Notebook - Fernandosr85 - Dashcam Collision Prediction Project ğŸš—**\n#### **https://www.kaggle.com/code/fernandosr85/dashcam-collision-prediction-project**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nmax_files = 10 \n\ncount = 0\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        count += 1\n        if count >= max_files:\n            break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:30:24.708226Z","iopub.execute_input":"2025-05-01T07:30:24.708523Z","iopub.status.idle":"2025-05-01T07:30:28.944115Z","shell.execute_reply.started":"2025-05-01T07:30:24.708496Z","shell.execute_reply":"2025-05-01T07:30:28.942848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport warnings\nfrom multiprocessing import Pool\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.models\n\nwarnings.filterwarnings(\"ignore\")\n\n# Check GPU availability and set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:30:28.945828Z","iopub.execute_input":"2025-05-01T07:30:28.946372Z","iopub.status.idle":"2025-05-01T07:30:38.044641Z","shell.execute_reply.started":"2025-05-01T07:30:28.946346Z","shell.execute_reply":"2025-05-01T07:30:38.043775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppress unnecessary formatting warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Paths to the CSV files\ntrain_csv_path = '/kaggle/input/nexar-collision-prediction/train.csv'\ntest_csv_path = '/kaggle/input/nexar-collision-prediction/test.csv'\nsubmission_csv_path = '/kaggle/input/nexar-collision-prediction/sample_submission.csv'\n\n# Paths to the video directories\ntrain_video_dir = '/kaggle/input/nexar-collision-prediction/train'\ntest_video_dir = '/kaggle/input/nexar-collision-prediction/test'\n\n# Load the CSV files\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\nsubmission_df = pd.read_csv(submission_csv_path)\n\n# (ì¶”ê°€) id ì»¬ëŸ¼ì„ ë¬¸ìì—´(str)ë¡œ ë³€í™˜í•´ì„œ .0 ë¬¸ì œ ì—†ì• ê¸°\ntrain_df['id'] = train_df['id'].astype(str)\n\n# Display the first few rows of the DataFrames\nprint(\"Train.csv:\")\nprint(train_df.head())\n\nprint(\"\\nTest.csv:\")\nprint(test_df.head())\n\nprint(\"\\nSample Submission:\")\nprint(submission_df.head())\n\n# Optional: handle NaN values if needed, filling with zero or another value\ntrain_df['time_of_event'] = train_df['time_of_event'].fillna(0)\ntrain_df['time_of_alert'] = train_df['time_of_alert'].fillna(0)\n\n# (ì¶”ê°€) Check the video directory paths\nprint(\"\\nVideo Directory Paths:\")\nprint(f\"Train videos are located at: {train_video_dir}\")\nprint(f\"Test videos are located at: {test_video_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:30:38.045746Z","iopub.execute_input":"2025-05-01T07:30:38.046229Z","iopub.status.idle":"2025-05-01T07:30:38.094145Z","shell.execute_reply.started":"2025-05-01T07:30:38.046200Z","shell.execute_reply":"2025-05-01T07:30:38.093205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Data Preprocessing and Feature Extraction** ","metadata":{}},{"cell_type":"code","source":"# ì¼ë°˜ì ìœ¼ë¡œ ì¶©ëŒì´ ë°œìƒí•˜ëŠ” ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì´ˆì ì„ ë§ì¶° ë¹„ë””ì˜¤ì—ì„œ ì£¼ìš” í”„ë ˆì„ì„ ì¶”ì¶œ\n# ì§€ìˆ˜ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ì— ê°€ê¹Œìš´ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬\n\ndef extract_keyframes(video_path, num_frames=12, target_size=(160, 160)):\n    \"\"\"\n    Extracts key frames from the video, focusing on the final part where collisions typically occur.\n    Uses exponential distribution to give more weight to frames closer to the end.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path) # ë™ì˜ìƒì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ OpenCVì˜ videoCapture ê°ì²´ ìƒì„± \n\n    # íŒŒì¼ì´ ì œëŒ€ë¡œ ì—´ë¦¬ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬\n    if not cap.isOpened():\n        print(f\"Could not open the video: {video_path}\")\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n\n    # ì´ í”„ë ˆì„ ìˆ˜ì™€ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜(FPS)ë¥¼ ê°€ì ¸ì˜¤ê¸° \n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    if total_frames <= 0:\n        print(f\"Video without frames: {video_path}\")\n        cap.release()\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n    \n    # ì˜ìƒ ê¸¸ì´(ì´ˆ ë‹¨ìœ„) ê³„ì‚°\n    duration = total_frames / fps if fps > 0 else 0\n    \n    # ì§§ì€ ì˜ìƒ (10ì´ˆ ë¯¸ë§Œ): ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ\n    if duration < 10:\n        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n\n    # ê¸´ ì˜ìƒ (10ì´ˆ ì´ìƒ): í›„ë°˜ë¶€ì— ë” ì§‘ì¤‘í•´ì„œ ì¶”ì¶œ\n    else:\n        # ë§ˆì§€ë§‰ 3ì´ˆ ë™ì•ˆ í”„ë ˆì„ì˜ 80% ì§‘ì¤‘(ì¤‘ìš” ì˜ì—­)\n        end_frames = int(num_frames * 0.8)\n        start_frames = num_frames - end_frames\n        \n        # ì§€ë‚œ 3ì´ˆ ë™ì•ˆì˜ ì‹œì‘ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°\n        last_seconds = 3\n        last_frame_count = min(int(fps * last_seconds), total_frames - 1)\n        start_idx = max(0, total_frames - last_frame_count)\n        \n        # ë§ˆì§€ë§‰ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì§€ìˆ˜ ë¶„í¬\n        # ì´ë ‡ê²Œ í•˜ë©´ ë§ˆì§€ë§‰ì— ë” ë°€ì§‘ëœ ì¸ë±ìŠ¤ê°€ ìƒì„±ëœë‹¤ (\"í”„ë ˆì„ì„ ë½‘ëŠ” ê°„ê²©\"ìì²´ë¥¼ ì¡°ì ˆ â†’ ëë¶€ë¶„ì— ë” ë§ì´ ëª°ë¦¬ê²Œ ë§Œë“œëŠ” ë°©ì‹)\n        end_indices = np.array([\n            start_idx + int((total_frames - start_idx - 1) * (i/end_frames)**2) \n            for i in range(1, end_frames + 1)\n        ])\n        \n        # contextì— ë§ê²Œ ê· ì¼í•˜ê²Œ ë°°í¬ëœ ì´ˆê¸° í”„ë ˆì„ (ì´ˆë°˜ë¶€ì—ì„œ ê· ë“±í•˜ê²Œ ì¶”ì¶œí•œ í”„ë ˆì„ë“¤)\n        # contextë€? ì‚¬ê³  ì§ì „ì— ì–´ë–¤ ìƒí™©ì´ í¼ì³ì¡ŒëŠ”ì§€ì— ëŒ€í•œ íë¦„, ë°°ê²½, ë§¥ë½ \n        begin_indices = np.linspace(0, start_idx - 1, start_frames, dtype=int) if start_idx > 0 else np.zeros(start_frames, dtype=int)\n        \n        # ì¸ë±ìŠ¤ ê²°í•©\n        frame_indices = np.concatenate([begin_indices, end_indices])\n    \n    # ì„ íƒí•œ í”„ë ˆì„ ì¶”ì¶œ \n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            # Use higher resolution and better interpolation\n            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        else:\n            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n    \n    cap.release()\n    return np.array(frames, dtype=np.uint8)\n\n# ë¨¼ì €, ì „ì—­ ë²”ìœ„ì—ì„œ ë³€í™˜ í´ë˜ìŠ¤ë¥¼ ì •ì˜ \n# ì…ë ¥ëœ ì˜ìƒ í”„ë ˆì„ì„ ì¼ì • í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ì‹œì¼œì„œ, ë°ì´í„° ë‹¤ì–‘ì„±ì„ ëŠ˜ë¦¬ëŠ” ì—­í• \nclass RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n        \n    def __call__(self, frames):\n        if np.random.random() < self.p:\n            return frames[:, :, ::-1, :].copy()  # horizontally flip each frame\n        return frames\n\n# ì˜ìƒ í”„ë ˆì„ì˜ ë°ê¸°ì™€ ëŒ€ë¹„ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°ì •í•´, ë‹¤ì–‘í•œ ì¡°ëª… í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ì¦ê°• í´ë˜ìŠ¤\nclass ColorJitter(object):\n    def __init__(self, brightness=0, contrast=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        \n    def __call__(self, frames):\n        # Apply brightness jitter\n        if self.brightness > 0:\n            brightness_factor = np.random.uniform(max(0, 1-self.brightness), 1+self.brightness)\n            frames = frames * brightness_factor\n            frames = np.clip(frames, 0, 255)\n        \n        # Apply contrast jitter\n        if self.contrast > 0:\n            contrast_factor = np.random.uniform(max(0, 1-self.contrast), 1+self.contrast)\n            frames = (frames - 128) * contrast_factor + 128\n            frames = np.clip(frames, 0, 255)\n            \n        return frames\n\n# í”„ë ˆì„ì— íë¦¿í•œ ì•ˆê°œ íš¨ê³¼ë¥¼ ë„£ì–´, ì‹œì•¼ê°€ ë‚˜ìœ ë‚ ì”¨ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\nclass AddFog(object):\n    def __call__(self, frames):\n        fog = np.random.uniform(0.7, 0.9, frames.shape).astype(np.float32)\n        return frames * 0.8 + fog * 50  # Adjusted for 0-255 scale\n\n# í”„ë ˆì„ì— í°ìƒ‰ ì„ í˜• ë…¸ì´ì¦ˆ(ë¹—ë°©ìš¸)ë¥¼ ì¶”ê°€í•´ ë¹„ ì˜¤ëŠ” ë‚ ì”¨ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\nclass AddRain(object):\n    def __call__(self, frames):\n        h, w = frames.shape[1:3]\n        rain = np.random.uniform(0, 1, (len(frames), h, w, 1)).astype(np.float32)\n        rain = (rain > 0.97).astype(np.float32) * 200  # White rain drops\n        return np.clip(frames * 0.9 + rain, 0, 255)  # Darken a bit and add drops\n\n# ì§€ì •ëœ í™•ë¥ ì— ë”°ë¼ ì–´ë–¤ ë³€í™˜ì„ ì ìš©í• ì§€ ë§ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê²°ì •í•˜ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ í´ë˜ìŠ¤(ëœë¤ì„± ë¶€ì—¬)\nclass RandomApply(object):\n    def __init__(self, transform, p=0.5):\n        self.transform = transform\n        self.p = p\n        \n    def __call__(self, frames):\n        if np.random.random() < self.p:\n            return self.transform(frames)\n        return frames\n\n# ì—¬ëŸ¬ ê°œì˜ ë³€í™˜(Flip, Jitter, Fog ë“±)ì„ ìˆœì„œëŒ€ë¡œ ì ìš©í•˜ëŠ” ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def __call__(self, frames):\n        for t in self.transforms:\n            frames = t(frames)\n        return frames\n\n# ì˜ìƒ í”„ë ˆì„ ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë°”ê¾¸ê³ , í”½ì…€ ê°’ì„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ëŠ” í´ë˜ìŠ¤\nclass ToTensor(object):\n    def __call__(self, frames):\n        # Convert from (T, H, W, C) to (T, C, H, W)\n        frames = frames.transpose(0, 3, 1, 2)\n        # Convert to tensor and normalize to [0, 1]\n        return torch.from_numpy(frames).float() / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:30:38.095947Z","iopub.execute_input":"2025-05-01T07:30:38.096208Z","iopub.status.idle":"2025-05-01T07:30:38.117716Z","shell.execute_reply.started":"2025-05-01T07:30:38.096188Z","shell.execute_reply":"2025-05-01T07:30:38.116801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ë™ì˜ìƒì—ì„œ ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ ë³€í™˜ì„ ë°˜í™˜\n\ndef get_video_transforms():\n    \"\"\"\n    Returns transformations for data augmentation in videos.\n    \"\"\"\n    return {\n        'train': Compose([\n            RandomHorizontalFlip(p=0.5),\n            ColorJitter(brightness=0.3, contrast=0.3),\n            RandomApply(AddFog(), p=0.15),\n            RandomApply(AddRain(), p=0.15),\n            RandomApply(RandomNoise(0.05), p=0.2), \n            RandomApply(RandomOcclusion(), p=0.1),\n            ToTensor()\n        ]),\n        'val': Compose([\n            ToTensor()  # Only tensor conversion for validation\n        ])\n    }\n\n# ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ë¬´ì‘ìœ„ ê°€ìš°ì‹œì•ˆ(ì •ê·œë¶„í¬) ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬, ì‹¤ì œ ì´¬ì˜ í™˜ê²½ì—ì„œ \n# ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¡ìŒì— ëŒ€í•´ ëª¨ë¸ì´ ë” ê°•ê±´í•´ì§€ë„ë¡ ë§Œë“œëŠ” í´ë˜ìŠ¤\nclass RandomNoise(object):\n    \"\"\"\n    Applies random Gaussian noise to video frames for data augmentation.\n    \n    This transformation helps the model become more robust to noise\n    that may be present in real-world video data.\n    \n    Args:\n        std (float): Standard deviation of the Gaussian noise as a fraction\n                     of the pixel value range (default: 0.05)\n    \"\"\"\n    def __init__(self, std=0.05):\n        self.std = std\n        \n    def __call__(self, frames):\n        \"\"\"\n        Apply random noise to the input frames.\n        \n        Args:\n            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n                                   where T is number of frames\n        \n        Returns:\n            numpy.ndarray: Noise-augmented frames, clipped to valid pixel range [0, 255]\n        \"\"\"\n        # ì§€ì •ëœ í‘œì¤€ í¸ì°¨ë¥¼ ê°€ì§„ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±\n        noise = np.random.normal(0, self.std * 255, frames.shape).astype(np.float32)\n        \n        # ìœ íš¨í•œ í”½ì…€ ë²”ìœ„ì— ë…¸ì´ì¦ˆ ë° í´ë¦½ ì¶”ê°€í•˜ê¸°\n        # ì˜ìƒì€ ì •ìˆ˜í˜• ë°ì´í„°ì—¬ì•¼ í•˜ë¯€ë¡œ í˜• ë³€í™˜ (astype)\n        return np.clip(frames + noise, 0, 255).astype(np.uint8)\n\n# ì˜ìƒ í”„ë ˆì„ì— ê²€ì€ìƒ‰ ì‚¬ê°í˜•ì„ ë¬´ì‘ìœ„ë¡œ ë®ì–´ ì”Œì›Œ, ì¼ë¶€ ì •ë³´ê°€ ê°€ë ¤ì¡Œì„ ë•Œë„ ëª¨ë¸ì´ ê²¬ë”œ ìˆ˜ ìˆë„ë¡ í›ˆë ¨ì‹œí‚¤ëŠ” í´ë˜ìŠ¤\nclass RandomOcclusion(object):\n    \"\"\"\n    Simulates occlusion in video frames by adding black rectangles.\n    \n    This transformation helps the model learn to handle partial occlusions\n    that may occur in real-world scenarios when objects block the camera view.\n    \"\"\"\n    def __call__(self, frames):\n        \"\"\"\n        Apply random occlusion to the input frames.\n        \n        Args:\n            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n                                   where T is number of frames\n        \n        Returns:\n            numpy.ndarray: Frames with random occlusion applied\n        \"\"\"\n        # í”„ë ˆì„ í•˜ë‚˜ì˜ ì„¸ë¡œ(h), ê°€ë¡œ(w) ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°\n        h, w = frames.shape[1:3]\n        \n        # ì „ì²´ í”„ë ˆì„ í¬ê¸°ì˜ 10%~25% ì‚¬ì´ í¬ê¸°ì˜ ê°€ë¦¼ ì˜ì—­ í¬ê¸° ì„¤ì •\n        occl_h = np.random.randint(int(h * 0.1), int(h * 0.25))\n        occl_w = np.random.randint(int(w * 0.1), int(w * 0.25))\n        \n        # ì´ ê°€ë¦¼ ì˜ì—­ì´ ë“¤ì–´ê°ˆ ë¬´ì‘ìœ„ ìœ„ì¹˜ ì¢Œí‘œ ì„¤ì • \n        occl_x = np.random.randint(0, w - occl_w)\n        occl_y = np.random.randint(0, h - occl_h)\n        \n        # ì›ë³¸ í”„ë ˆì„ì„ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ ë³µì‚¬ë³¸ ë§Œë“¤ê¸°\n        frames_copy = frames.copy()\n        \n        # í”½ì…€ì„ 0(ê²€ì •ìƒ‰)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  í”„ë ˆì„ì— occlusion ì ìš©\n        for i in range(len(frames)):\n            frames_copy[i, occl_y:occl_y+occl_h, occl_x:occl_x+occl_w, :] = 0\n            \n        return frames_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:30:38.118768Z","iopub.execute_input":"2025-05-01T07:30:38.119101Z","iopub.status.idle":"2025-05-01T07:30:38.138210Z","shell.execute_reply.started":"2025-05-01T07:30:38.119072Z","shell.execute_reply":"2025-05-01T07:30:38.137368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ë¹„ë””ì˜¤ í”„ë ˆì„ ê°„ ì›€ì§ì„(ëª¨ì…˜)ì„ ì¶”ì í•˜ëŠ” 'optical_flow'ë¥¼ ê³„ì‚°í•´, ê°ì²´ë‚˜ ë°°ê²½ì˜ ì´ë™ ë°©í–¥ê³¼ ì†ë„ë¥¼ ë²¡í„° í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n# ë‘ ì—°ì†ëœ ì´ë¯¸ì§€(ë˜ëŠ” í”„ë ˆì„) ì‚¬ì´ì—ì„œ, ê° í”½ì…€ì´ ì–´ë–»ê²Œ ì´ë™í–ˆëŠ”ì§€ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ  -> optical_flow\n# Farneback ë°©ì‹ë§Œ ì‚¬ìš©\n# \"ëª¨ë“  í”½ì…€ì˜ ë°©í–¥ + ì†ë„ ì •ë³´ë¥¼ ë‹¤ ë‚¨ê¹€\"\ndef compute_optical_flow_sequence(frames, skip_frames=1):\n    \"\"\"\n    Calculates per-frame optical flow magnitudes as a sequence.\n    \n    Args:\n        frames (numpy.ndarray): (T, H, W, C)\n        \n    Returns:\n        numpy.ndarray: (T, 1) array of flow magnitudes (first frame is 0)\n    \"\"\"\n    T = len(frames)\n    if T < 2:\n        return np.zeros((T, 1), dtype=np.float32)\n    \n    magnitudes = [0.0]  # ì²« í”„ë ˆì„ì€ optical flowê°€ ì—†ìœ¼ë‹ˆ 0ìœ¼ë¡œ ì±„ì›€\n\n    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n    \n    for i in range(1, T, skip_frames):\n        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n        try:\n            flow = cv2.calcOpticalFlowFarneback(\n                prev_gray, curr_gray,\n                None, 0.5, 3, 15, 3, 5, 1.2, 0\n            )\n            flow_magnitude = np.linalg.norm(flow, axis=-1).mean()  # (H, W) â†’ scalar mean\n            magnitudes.append(flow_magnitude)\n        except Exception as e:\n            print(f\"Error calculating optical flow: {str(e)}\")\n            magnitudes.append(0.0)\n        \n        prev_gray = curr_gray\n\n    # ê¸¸ì´ê°€ ë¶€ì¡±í•˜ë©´ padding\n    while len(magnitudes) < T:\n        magnitudes.append(0.0)\n    \n    return np.array(magnitudes, dtype=np.float32).reshape(T, 1)  # (T, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:30:38.139135Z","iopub.execute_input":"2025-05-01T07:30:38.139447Z","iopub.status.idle":"2025-05-01T07:30:38.160157Z","shell.execute_reply.started":"2025-05-01T07:30:38.139417Z","shell.execute_reply":"2025-05-01T07:30:38.159280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:30:38.161046Z","iopub.execute_input":"2025-05-01T07:30:38.161279Z","iopub.status.idle":"2025-05-01T07:30:53.490923Z","shell.execute_reply.started":"2025-05-01T07:30:38.161260Z","shell.execute_reply":"2025-05-01T07:30:53.490226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Positiveì´ë©´ alert_event ì‚¬ì´ë§Œ ë½‘ê³ , Negativeì´ë©´ ë§ˆì§€ë§‰ 3ì´ˆ êµ¬ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ**\n**ê·¸ë¦¬ê³  num_framesë§Œí¼ ê· ë“±í•˜ê²Œ ë½‘ê³  CNN + Optical Flow ë‘˜ ë‹¤ ê³„ì‚°** ","metadata":{}},{"cell_type":"code","source":"# InceptionV3 ëª¨ë¸ë¡œ íŠ¹ì„± ì¶”ì¶œ\nbase_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\ncnn_feature_dim = base_model.output_shape[-1]\n\ndef get_hybrid_feature_sequence(video_path, num_frames=12):\n    \"\"\"\n    Extract per-frame hybrid features (CNN + Optical flow) as a sequence.\n    \n    Args:\n        video_path (str): Path to video file.\n        num_frames (int): Number of frames to extract.\n    \n    Returns:\n        np.ndarray: (T, 1281) array of per-frame features.\n    \"\"\"\n    # 1. í”„ë ˆì„ ì¶”ì¶œ\n    frames = extract_keyframes(video_path, num_frames=num_frames, target_size=(160,160))\n    \n    if len(frames) == 0:\n        print(f\"Skipping {video_path}: no frames\")\n        return np.zeros((num_frames, 1281), dtype=np.float32)\n\n    # 2. CNN feature per frame (Inception expects (N, H, W, C))\n    spatial_features = base_model.predict(\n        preprocess_input(frames.astype('float32')),\n        batch_size=32,\n        verbose=0\n    )  # shape: (T, 1280)\n\n    # 3. Optical flow sequence\n    flow_magnitudes = compute_optical_flow_sequence(frames)  # shape: (T, 1)\n\n    # 4. Concatenate per frame\n    hybrid_features = np.concatenate([spatial_features, flow_magnitudes], axis=1)  # (T, 1281)\n\n    return hybrid_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:30:53.491754Z","iopub.execute_input":"2025-05-01T07:30:53.492344Z","iopub.status.idle":"2025-05-01T07:30:58.416084Z","shell.execute_reply.started":"2025-05-01T07:30:53.492319Z","shell.execute_reply":"2025-05-01T07:30:58.415385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_hybrid_feature_sequence_from_frames(frames):\n    \"\"\"\n    Extract per-frame hybrid features (CNN + Optical flow) from pre-loaded frames.\n    \n    Args:\n        frames (torch.Tensor): (T, 3, 160, 160) tensor (after transform).\n    \n    Returns:\n        np.ndarray: (T, 1281) array of per-frame features.\n    \"\"\"\n    if len(frames) == 0:\n        print(\"Warning: empty frames input\")\n        return np.zeros((1, 1281), dtype=np.float32)\n\n    # 1ï¸âƒ£ PyTorch tensor â†’ numpy (T, 160, 160, 3), [0, 255] scale\n    frames_np = frames.permute(0, 2, 3, 1).numpy() * 255.0  # [0,1] â†’ [0,255]\n    frames_np = frames_np.astype(np.uint8)\n\n    # 2ï¸âƒ£ CNN Features per frame\n    spatial_features = base_model.predict(\n        preprocess_input(frames_np.astype('float32')),\n        batch_size=32,\n        verbose=0\n    )  # shape: (T, 1280)\n\n    # 3ï¸âƒ£ Optical Flow per frame\n    flow_magnitudes = compute_optical_flow_sequence(frames_np)  # shape: (T, 1)\n\n    # 4ï¸âƒ£ Concatenate â†’ (T, 1281)\n    hybrid_features = np.concatenate([spatial_features, flow_magnitudes], axis=1)\n\n    return hybrid_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:34:50.909974Z","iopub.execute_input":"2025-05-01T07:34:50.910298Z","iopub.status.idle":"2025-05-01T07:34:50.916703Z","shell.execute_reply.started":"2025-05-01T07:34:50.910275Z","shell.execute_reply":"2025-05-01T07:34:50.915718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_optical_flow_sequence(frames, skip_frames=1):\n    \"\"\"\n    Computes per-frame optical flow magnitudes.\n    \n    Args:\n        frames (np.ndarray): (T, H, W, 3) numpy array of frames.\n    \n    Returns:\n        np.ndarray: (T, 1) array of per-frame optical flow magnitudes.\n    \"\"\"\n    T = len(frames)\n    if T < 2:\n        return np.zeros((T, 1), dtype=np.float32)\n\n    magnitudes = []\n\n    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n\n    for i in range(1, T, skip_frames):\n        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n        try:\n            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray,\n                                                None, 0.5, 3, 15, 3, 5, 1.2, 0)\n            # magnitude = sqrt(u^2 + v^2)\n            mag = np.linalg.norm(flow, axis=-1)  # shape (H, W)\n            avg_mag = np.mean(mag)  # scalar\n            magnitudes.append(avg_mag)\n        except Exception as e:\n            print(f\"Error calculating flow at frame {i}: {str(e)}\")\n            magnitudes.append(0.0)\n\n        prev_gray = curr_gray\n\n    # ë§ˆì§€ë§‰ ê¸¸ì´ ë§ì¶¤ (T, 1)\n    if len(magnitudes) < T:\n        magnitudes.append(0.0)  # ë§ˆì§€ë§‰ í”„ë ˆì„ì€ flowê°€ ì—†ìŒ\n\n    magnitudes = np.array(magnitudes, dtype=np.float32).reshape(-1, 1)  # (T, 1)\n\n    return magnitudes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:38:28.167353Z","iopub.execute_input":"2025-05-01T07:38:28.167699Z","iopub.status.idle":"2025-05-01T07:38:28.175205Z","shell.execute_reply.started":"2025-05-01T07:38:28.167676Z","shell.execute_reply":"2025-05-01T07:38:28.174182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ì•„ì§ë„ ìµœì¢…ì ìœ¼ë¡œ Transformerì— ë„£ì€ (T, 1281) ì‹œí€€ìŠ¤ëŠ” ë§Œë“¤ì–´ì§€ì§€ ì•ŠìŒ\n# 1. í”„ë ˆì„ë³„ CNN Feature ì¶”ì¶œ (InceptionV3) ì¶”ì¶œ\n# 2. optical flow sequence (compute_optical_flow_sequence) ì¶”ì¶œ\n# 3. ë‘ ê²°ê³¼ë¬¼ concat\n# 4. ì´ê±¸ Transformerì˜ input ì‹œí€€ìŠ¤ë¡œ ì‚¬ìš©\n\n# ì „ì²´ ì²˜ë¦¬ í•¨ìˆ˜ (ì´ì œ ë‘˜ì„ ê²°í•©í•˜ëŠ” í•¨ìˆ˜ ìƒì„±)\n# CNN + optical flow ë¶™ì—¬ì„œ (T, 1281) ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜ \n\ndef prepare_transformer_input(video_path, num_frames=12, target_size=(160, 160)):\n    \"\"\"\n    Prepares (T, 1281) input sequence combining CNN features + optical flow\n    for a given video.\n    \"\"\"\n    # === Step 1: Extract frames ===\n    frames = extract_keyframes(video_path, num_frames=num_frames, target_size=target_size)\n    if frames.shape[0] == 0:\n        print(f\"Skipping video {video_path} (no frames)\")\n        return None  # or np.zeros((num_frames, 1281)) as fallback\n\n    # === Step 2: Extract CNN (spatial) features per frame ===\n    frames_float = preprocess_input(frames.astype('float32'))  # preprocess for InceptionV3\n    spatial_features = base_model.predict(frames_float, batch_size=32, verbose=0)  # (T, 1280)\n\n    # === Step 3: Compute optical flow sequence ===\n    optical_flow_sequence = compute_optical_flow_sequence(frames)  # (T, 1)\n\n    # === Step 4: Combine both ===\n    combined_features = np.concatenate([spatial_features, optical_flow_sequence], axis=1)  # (T, 1281)\n\n    return combined_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:44:39.051548Z","iopub.execute_input":"2025-05-01T07:44:39.052401Z","iopub.status.idle":"2025-05-01T07:44:39.058331Z","shell.execute_reply.started":"2025-05-01T07:44:39.052375Z","shell.execute_reply":"2025-05-01T07:44:39.057502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\n# 1ï¸âƒ£ transform ì¤€ë¹„\ntransforms = get_video_transforms()\ntrain_transform = transforms['train']\n\n# 2ï¸âƒ£ featureë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\nall_sequences = []\n\n# 3ï¸âƒ£ train_df ì „ì²´ ë°˜ë³µ (ì—¬ê¸° tqdm ì ìš©!)\nfor idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n    video_id = row['id']  # 'id' ì‚¬ìš©!\n    video_path = os.path.join(train_video_dir, f\"{int(video_id):05d}.mp4\")  # 5ìë¦¬ zero-padding\n\n    # prepare_transformer_input() í˜¸ì¶œ\n    sequence = prepare_transformer_input(video_path, num_frames=12)\n\n    if sequence is None:\n        print(f\"Skipping video {video_id} (no valid sequence)\")\n        continue\n\n    all_sequences.append(sequence)\n\n# 4ï¸âƒ£ ê²°ê³¼ë¥¼ numpy arrayë¡œ ë³€í™˜\nall_sequences = np.array(all_sequences)\nprint(f\"\\nAll sequences shape: {all_sequences.shape}\")\n# ì˜ˆì‹œ ì¶œë ¥ â†’ (n_videos, 12, 1281)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T07:47:15.893987Z","iopub.execute_input":"2025-05-01T07:47:15.894634Z","iopub.status.idle":"2025-05-01T09:47:14.240928Z","shell.execute_reply.started":"2025-05-01T07:47:15.894612Z","shell.execute_reply":"2025-05-01T09:47:14.239527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset í´ë˜ìŠ¤ ì •ì˜\nimport torch\nfrom torch.utils.data import Dataset\n\nclass VideoSequenceDataset(Dataset):\n    def __init__(self, sequences, labels):\n        \"\"\"\n        Args:\n            sequences (numpy.ndarray): shape (n_samples, T, feature_dim)\n            labels (numpy.ndarray or list): shape (n_samples,)\n        \"\"\"\n        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        x = self.sequences[idx]  # (T, feature_dim)\n        y = self.labels[idx]     # scalar or class\n        return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T12:20:45.436485Z","iopub.execute_input":"2025-05-01T12:20:45.437127Z","iopub.status.idle":"2025-05-01T12:20:49.863767Z","shell.execute_reply.started":"2025-05-01T12:20:45.437101Z","shell.execute_reply":"2025-05-01T12:20:49.863076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DataLoader ë§Œë“¤ê¸° \nfrom torch.utils.data import DataLoader, random_split\n\n# target(label) ì¤€ë¹„ (ì˜ˆ: binary classification)\nlabels = train_df['target'].values  # shape (n_videos,)\n\n# Dataset ê°ì²´ ìƒì„±\ndataset = VideoSequenceDataset(all_sequences, labels)\n\n# Train/Val ë¶„í•  (ì˜ˆ: 80/20)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# DataLoader ìƒì„±\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T12:21:06.017997Z","iopub.execute_input":"2025-05-01T12:21:06.019007Z","iopub.status.idle":"2025-05-01T12:21:06.032249Z","shell.execute_reply.started":"2025-05-01T12:21:06.018978Z","shell.execute_reply":"2025-05-01T12:21:06.031134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Temporal Transformer ëª¨ë¸ ì„¤ê³„\nimport torch\nimport torch.nn as nn\n\nclass TemporalTransformerModel(nn.Module):\n    def __init__(self, input_dim=1281, embed_dim=256, num_heads=4, num_layers=2, dropout=0.1):\n        super(TemporalTransformerModel, self).__init__()\n\n        # 1. Input â†’ embedding layer\n        self.input_proj = nn.Linear(input_dim, embed_dim)\n\n        # 2. Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # 3. Classification head (binary classification)\n        self.classifier = nn.Sequential(\n            nn.Linear(embed_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n            nn.Sigmoid()  # Binary output (0~1)\n        )\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, T, input_dim)\n        \"\"\"\n        # Step 1: Project input features\n        x = self.input_proj(x)  # â†’ (batch_size, T, embed_dim)\n\n        # Step 2: Apply Transformer Encoder\n        x = self.transformer_encoder(x)  # â†’ (batch_size, T, embed_dim)\n\n        # Step 3: Aggregate (mean pooling over time)\n        x = x.mean(dim=1)  # â†’ (batch_size, embed_dim)\n\n        # Step 4: Final classification\n        out = self.classifier(x)  # â†’ (batch_size, 1)\n\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T12:30:05.701966Z","iopub.execute_input":"2025-05-01T12:30:05.702572Z","iopub.status.idle":"2025-05-01T12:30:05.709514Z","shell.execute_reply.started":"2025-05-01T12:30:05.702538Z","shell.execute_reply":"2025-05-01T12:30:05.708520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Spatial Transformer ëª¨ë¸ ì„¤ê³„\n\nimport torch\nimport torch.nn as nn\n\nclass SpatialTransformer(nn.Module):\n    def __init__(self, input_dim=1280, hidden_dim=512, num_heads=8, num_layers=2, dropout=0.1):\n        super(SpatialTransformer, self).__init__()\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        # ë§ˆì§€ë§‰ summaryë¥¼ ìœ„í•œ pooling ë˜ëŠ” projection\n        self.output_layer = nn.Linear(input_dim, input_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: shape (batch_size, T, input_dim) â†’ per-frame spatial features\n        \n        Returns:\n            out: shape (batch_size, input_dim) â†’ aggregated spatial feature\n        \"\"\"\n        # transformer expects (batch_size, T, input_dim)\n        x_transformed = self.transformer(x)  # (batch_size, T, input_dim)\n\n        # Pooling over time (mean pooling)\n        x_pooled = x_transformed.mean(dim=1)  # (batch_size, input_dim)\n\n        out = self.output_layer(x_pooled)  # (batch_size, input_dim)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T12:32:33.862867Z","iopub.execute_input":"2025-05-01T12:32:33.863186Z","iopub.status.idle":"2025-05-01T12:32:33.870394Z","shell.execute_reply.started":"2025-05-01T12:32:33.863164Z","shell.execute_reply":"2025-05-01T12:32:33.869376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **1ë‹¨ê³„: ì¼ë°˜ ì „ì²˜ë¦¬** \n#### **(1) í”„ë ˆì„ ì¶”ì¶œ - extract_keyframes()** \n#### **(2) Optical Flow ê³„ì‚° - compute_optical_flow()** \n#### **(3) ê¸°ë³¸ ì¦ê°• - ColorJitter, AddRain, AddFog, ToTensor() ë“±**\n#### **(4) ë°ì´í„° êµ¬ì„± - DashcamDataset ë˜ëŠ” PreprocessDashcamDatasetìœ¼ë¡œ êµ¬ì„±**","metadata":{}},{"cell_type":"markdown","source":"### **2ë‹¨ê³„: AAT-DA ì „ìš© ì „ì²˜ë¦¬ (ê¸°ì¡´ ë°ì´í„°ì—ì„œ Transformerìš© êµ¬ì¡° ë³€í™˜)**\n##### AAT-DAëŠ” ë‹¨ìˆœí•œ ì˜ìƒ í”„ë ˆì„ì´ ì•„ë‹ˆë¼, \"ê°ì²´ ì¤‘ì‹¬ì˜ ì‹œê³µê°„ Attention ì…ë ¥ êµ¬ì¡°\"ë¥¼ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ê°€ í•„ìš”\n#### **(1) ê°ì²´ ê°ì§€ - í”„ë ˆì„ì—ì„œ ê°ì²´ ê°ì§€ (Cascade R-CNN ë“±)** \n#### **(2) ê°ì²´ íŠ¹ì§• ì¶”ì¶œ - ê°ì§€ëœ ë°•ìŠ¤ë§ˆë‹¤ VGG16 FC7 feature ì¶”ì¶œ (4096-dim)** \n#### **(3) ì‹œì„  ë§µ ì˜ˆì¸¡ - Gate-DAP ë“±ìœ¼ë¡œ driver attention heatmap ìƒì„±**\n#### **(4) ì£¼ì˜ ê°€ì¤‘ì¹˜ ê³„ì‚° - ì‹œì„  ë§µ + ê°ì²´ ìœ„ì¹˜ â†’ ê°ì²´ë³„ attention weight Î±áµ¢ ê³„ì‚°**\n#### **(5) Feature ê°€ì¤‘ - ê°ì²´ feature Î±áµ¢ â†’ ê°•ì¡°ëœ ê°ì²´ feature**\n#### **(6) ì‹œí€€ìŠ¤ êµ¬ì„± - ëª¨ë“  í”„ë ˆì„ì˜ ê²°ê³¼ë¥¼ (T, N, 4096) ì‹œí€€ìŠ¤ë¡œ íŒ¨ë”© ì •ë¦¬**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.151308Z","iopub.status.idle":"2025-04-27T00:00:37.151697Z","shell.execute_reply.started":"2025-04-27T00:00:37.151536Z","shell.execute_reply":"2025-04-27T00:00:37.151551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}