{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb713e2",
   "metadata": {
    "papermill": {
     "duration": 0.0077,
     "end_time": "2025-05-03T06:20:40.491294",
     "exception": false,
     "start_time": "2025-05-03T06:20:40.483594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **ğŸ’¡Reference Notebook - Fernandosr85 - Dashcam Collision Prediction Project ğŸš—**\n",
    "#### **https://www.kaggle.com/code/fernandosr85/dashcam-collision-prediction-project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfa2198",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-03T06:20:40.505193Z",
     "iopub.status.busy": "2025-05-03T06:20:40.504873Z",
     "iopub.status.idle": "2025-05-03T06:20:47.153847Z",
     "shell.execute_reply": "2025-05-03T06:20:47.152858Z"
    },
    "papermill": {
     "duration": 6.657537,
     "end_time": "2025-05-03T06:20:47.155488",
     "exception": false,
     "start_time": "2025-05-03T06:20:40.497951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nexar-collision-prediction/sample_submission.csv\n",
      "/kaggle/input/nexar-collision-prediction/train.csv\n",
      "/kaggle/input/nexar-collision-prediction/test.csv\n",
      "/kaggle/input/nexar-collision-prediction/test/02772.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02807.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02509.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/00350.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02163.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02707.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02741.mp4\n",
      "/kaggle/input/nexar-collision-prediction/train/02059.mp4\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "max_files = 10 \n",
    "\n",
    "count = 0\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        count += 1\n",
    "        if count >= max_files:\n",
    "            break\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3ac151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:20:47.170616Z",
     "iopub.status.busy": "2025-05-03T06:20:47.170203Z",
     "iopub.status.idle": "2025-05-03T06:21:01.264460Z",
     "shell.execute_reply": "2025-05-03T06:21:01.263416Z"
    },
    "papermill": {
     "duration": 14.104202,
     "end_time": "2025-05-03T06:21:01.266035",
     "exception": false,
     "start_time": "2025-05-03T06:20:47.161833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9de6400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.280553Z",
     "iopub.status.busy": "2025-05-03T06:21:01.280083Z",
     "iopub.status.idle": "2025-05-03T06:21:01.330794Z",
     "shell.execute_reply": "2025-05-03T06:21:01.329619Z"
    },
    "papermill": {
     "duration": 0.060245,
     "end_time": "2025-05-03T06:21:01.332815",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.272570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.csv:\n",
      "     id  time_of_event  time_of_alert  target\n",
      "0  1924            NaN            NaN       0\n",
      "1   822           19.5         18.633       1\n",
      "2  1429            NaN            NaN       0\n",
      "3   208           19.8         19.233       1\n",
      "4  1904            NaN            NaN       0\n",
      "\n",
      "Test.csv:\n",
      "    id\n",
      "0  204\n",
      "1   30\n",
      "2  146\n",
      "3   20\n",
      "4  511\n",
      "\n",
      "Sample Submission:\n",
      "    id  target\n",
      "0  204       0\n",
      "1   30       0\n",
      "2  146       0\n",
      "3   20       0\n",
      "4  511       0\n",
      "\n",
      "Video Directory Paths:\n",
      "Train videos are located at: /kaggle/input/nexar-collision-prediction/train\n",
      "Test videos are located at: /kaggle/input/nexar-collision-prediction/test\n"
     ]
    }
   ],
   "source": [
    "# Suppress unnecessary formatting warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Paths to the CSV files\n",
    "train_csv_path = '/kaggle/input/nexar-collision-prediction/train.csv'\n",
    "test_csv_path = '/kaggle/input/nexar-collision-prediction/test.csv'\n",
    "submission_csv_path = '/kaggle/input/nexar-collision-prediction/sample_submission.csv'\n",
    "\n",
    "# Paths to the video directories\n",
    "train_video_dir = '/kaggle/input/nexar-collision-prediction/train'\n",
    "test_video_dir = '/kaggle/input/nexar-collision-prediction/test'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "submission_df = pd.read_csv(submission_csv_path)\n",
    "\n",
    "# (ì¶”ê°€) id ì»¬ëŸ¼ì„ ë¬¸ìì—´(str)ë¡œ ë³€í™˜í•´ì„œ .0 ë¬¸ì œ ì—†ì• ê¸°\n",
    "train_df['id'] = train_df['id'].astype(str)\n",
    "\n",
    "# Display the first few rows of the DataFrames\n",
    "print(\"Train.csv:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest.csv:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\nSample Submission:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Optional: handle NaN values if needed, filling with zero or another value\n",
    "train_df['time_of_event'] = train_df['time_of_event'].fillna(0)\n",
    "train_df['time_of_alert'] = train_df['time_of_alert'].fillna(0)\n",
    "\n",
    "# (ì¶”ê°€) Check the video directory paths\n",
    "print(\"\\nVideo Directory Paths:\")\n",
    "print(f\"Train videos are located at: {train_video_dir}\")\n",
    "print(f\"Test videos are located at: {test_video_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f1650f",
   "metadata": {
    "papermill": {
     "duration": 0.00631,
     "end_time": "2025-05-03T06:21:01.345825",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.339515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Data Preprocessing and Feature Extraction** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57800e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.360208Z",
     "iopub.status.busy": "2025-05-03T06:21:01.359869Z",
     "iopub.status.idle": "2025-05-03T06:21:01.381245Z",
     "shell.execute_reply": "2025-05-03T06:21:01.379973Z"
    },
    "papermill": {
     "duration": 0.030518,
     "end_time": "2025-05-03T06:21:01.382775",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.352257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ì¼ë°˜ì ìœ¼ë¡œ ì¶©ëŒì´ ë°œìƒí•˜ëŠ” ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì´ˆì ì„ ë§ì¶° ë¹„ë””ì˜¤ì—ì„œ ì£¼ìš” í”„ë ˆì„ì„ ì¶”ì¶œ\n",
    "# ì§€ìˆ˜ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ì— ê°€ê¹Œìš´ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬\n",
    "\n",
    "def extract_keyframes(video_path, num_frames=12, target_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Extracts key frames from the video, focusing on the final part where collisions typically occur.\n",
    "    Uses exponential distribution to give more weight to frames closer to the end.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path) # ë™ì˜ìƒì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ OpenCVì˜ videoCapture ê°ì²´ ìƒì„± \n",
    "\n",
    "    # íŒŒì¼ì´ ì œëŒ€ë¡œ ì—´ë¦¬ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open the video: {video_path}\")\n",
    "        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # ì´ í”„ë ˆì„ ìˆ˜ì™€ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜(FPS)ë¥¼ ê°€ì ¸ì˜¤ê¸° \n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    if total_frames <= 0:\n",
    "        print(f\"Video without frames: {video_path}\")\n",
    "        cap.release()\n",
    "        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # ì˜ìƒ ê¸¸ì´(ì´ˆ ë‹¨ìœ„) ê³„ì‚°\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    # ì§§ì€ ì˜ìƒ (10ì´ˆ ë¯¸ë§Œ): ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ\n",
    "    if duration < 10:\n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "    # ê¸´ ì˜ìƒ (10ì´ˆ ì´ìƒ): í›„ë°˜ë¶€ì— ë” ì§‘ì¤‘í•´ì„œ ì¶”ì¶œ\n",
    "    else:\n",
    "        # ë§ˆì§€ë§‰ 3ì´ˆ ë™ì•ˆ í”„ë ˆì„ì˜ 80% ì§‘ì¤‘(ì¤‘ìš” ì˜ì—­)\n",
    "        end_frames = int(num_frames * 0.8)\n",
    "        start_frames = num_frames - end_frames\n",
    "        \n",
    "        # ì§€ë‚œ 3ì´ˆ ë™ì•ˆì˜ ì‹œì‘ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°\n",
    "        last_seconds = 3\n",
    "        last_frame_count = min(int(fps * last_seconds), total_frames - 1)\n",
    "        start_idx = max(0, total_frames - last_frame_count)\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì§€ìˆ˜ ë¶„í¬\n",
    "        # ì´ë ‡ê²Œ í•˜ë©´ ë§ˆì§€ë§‰ì— ë” ë°€ì§‘ëœ ì¸ë±ìŠ¤ê°€ ìƒì„±ëœë‹¤ (\"í”„ë ˆì„ì„ ë½‘ëŠ” ê°„ê²©\"ìì²´ë¥¼ ì¡°ì ˆ â†’ ëë¶€ë¶„ì— ë” ë§ì´ ëª°ë¦¬ê²Œ ë§Œë“œëŠ” ë°©ì‹)\n",
    "        end_indices = np.array([\n",
    "            start_idx + int((total_frames - start_idx - 1) * (i/end_frames)**2) \n",
    "            for i in range(1, end_frames + 1)\n",
    "        ])\n",
    "        \n",
    "        # contextì— ë§ê²Œ ê· ì¼í•˜ê²Œ ë°°í¬ëœ ì´ˆê¸° í”„ë ˆì„ (ì´ˆë°˜ë¶€ì—ì„œ ê· ë“±í•˜ê²Œ ì¶”ì¶œí•œ í”„ë ˆì„ë“¤)\n",
    "        # contextë€? ì‚¬ê³  ì§ì „ì— ì–´ë–¤ ìƒí™©ì´ í¼ì³ì¡ŒëŠ”ì§€ì— ëŒ€í•œ íë¦„, ë°°ê²½, ë§¥ë½ \n",
    "        begin_indices = np.linspace(0, start_idx - 1, start_frames, dtype=int) if start_idx > 0 else np.zeros(start_frames, dtype=int)\n",
    "        \n",
    "        # ì¸ë±ìŠ¤ ê²°í•©\n",
    "        frame_indices = np.concatenate([begin_indices, end_indices])\n",
    "    \n",
    "    # ì„ íƒí•œ í”„ë ˆì„ ì¶”ì¶œ \n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Use higher resolution and better interpolation\n",
    "            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(frames, dtype=np.uint8)\n",
    "\n",
    "# ë¨¼ì €, ì „ì—­ ë²”ìœ„ì—ì„œ ë³€í™˜ í´ë˜ìŠ¤ë¥¼ ì •ì˜ \n",
    "# ì…ë ¥ëœ ì˜ìƒ í”„ë ˆì„ì„ ì¼ì • í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ì‹œì¼œì„œ, ë°ì´í„° ë‹¤ì–‘ì„±ì„ ëŠ˜ë¦¬ëŠ” ì—­í• \n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        if np.random.random() < self.p:\n",
    "            return frames[:, :, ::-1, :].copy()  # horizontally flip each frame\n",
    "        return frames\n",
    "\n",
    "# ì˜ìƒ í”„ë ˆì„ì˜ ë°ê¸°ì™€ ëŒ€ë¹„ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°ì •í•´, ë‹¤ì–‘í•œ ì¡°ëª… í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ì¦ê°• í´ë˜ìŠ¤\n",
    "class ColorJitter(object):\n",
    "    def __init__(self, brightness=0, contrast=0):\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        # Apply brightness jitter\n",
    "        if self.brightness > 0:\n",
    "            brightness_factor = np.random.uniform(max(0, 1-self.brightness), 1+self.brightness)\n",
    "            frames = frames * brightness_factor\n",
    "            frames = np.clip(frames, 0, 255)\n",
    "        \n",
    "        # Apply contrast jitter\n",
    "        if self.contrast > 0:\n",
    "            contrast_factor = np.random.uniform(max(0, 1-self.contrast), 1+self.contrast)\n",
    "            frames = (frames - 128) * contrast_factor + 128\n",
    "            frames = np.clip(frames, 0, 255)\n",
    "            \n",
    "        return frames\n",
    "\n",
    "# í”„ë ˆì„ì— íë¦¿í•œ ì•ˆê°œ íš¨ê³¼ë¥¼ ë„£ì–´, ì‹œì•¼ê°€ ë‚˜ìœ ë‚ ì”¨ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class AddFog(object):\n",
    "    def __call__(self, frames):\n",
    "        fog = np.random.uniform(0.7, 0.9, frames.shape).astype(np.float32)\n",
    "        return frames * 0.8 + fog * 50  # Adjusted for 0-255 scale\n",
    "\n",
    "# í”„ë ˆì„ì— í°ìƒ‰ ì„ í˜• ë…¸ì´ì¦ˆ(ë¹—ë°©ìš¸)ë¥¼ ì¶”ê°€í•´ ë¹„ ì˜¤ëŠ” ë‚ ì”¨ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class AddRain(object):\n",
    "    def __call__(self, frames):\n",
    "        h, w = frames.shape[1:3]\n",
    "        rain = np.random.uniform(0, 1, (len(frames), h, w, 1)).astype(np.float32)\n",
    "        rain = (rain > 0.97).astype(np.float32) * 200  # White rain drops\n",
    "        return np.clip(frames * 0.9 + rain, 0, 255)  # Darken a bit and add drops\n",
    "\n",
    "# ì§€ì •ëœ í™•ë¥ ì— ë”°ë¼ ì–´ë–¤ ë³€í™˜ì„ ì ìš©í• ì§€ ë§ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê²°ì •í•˜ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ í´ë˜ìŠ¤(ëœë¤ì„± ë¶€ì—¬)\n",
    "class RandomApply(object):\n",
    "    def __init__(self, transform, p=0.5):\n",
    "        self.transform = transform\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        if np.random.random() < self.p:\n",
    "            return self.transform(frames)\n",
    "        return frames\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ ë³€í™˜(Flip, Jitter, Fog ë“±)ì„ ìˆœì„œëŒ€ë¡œ ì ìš©í•˜ëŠ” ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        for t in self.transforms:\n",
    "            frames = t(frames)\n",
    "        return frames\n",
    "\n",
    "# ì˜ìƒ í”„ë ˆì„ ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë°”ê¾¸ê³ , í”½ì…€ ê°’ì„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class ToTensor(object):\n",
    "    def __call__(self, frames):\n",
    "        # Convert from (T, H, W, C) to (T, C, H, W)\n",
    "        frames = frames.transpose(0, 3, 1, 2)\n",
    "        # Convert to tensor and normalize to [0, 1]\n",
    "        return torch.from_numpy(frames).float() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cfc1a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.397757Z",
     "iopub.status.busy": "2025-05-03T06:21:01.397460Z",
     "iopub.status.idle": "2025-05-03T06:21:01.407576Z",
     "shell.execute_reply": "2025-05-03T06:21:01.406814Z"
    },
    "papermill": {
     "duration": 0.019064,
     "end_time": "2025-05-03T06:21:01.408931",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.389867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ë™ì˜ìƒì—ì„œ ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ ë³€í™˜ì„ ë°˜í™˜\n",
    "\n",
    "def get_video_transforms():\n",
    "    \"\"\"\n",
    "    Returns transformations for data augmentation in videos.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'train': Compose([\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "            ColorJitter(brightness=0.3, contrast=0.3),\n",
    "            RandomApply(AddFog(), p=0.15),\n",
    "            RandomApply(AddRain(), p=0.15),\n",
    "            RandomApply(RandomNoise(0.05), p=0.2), \n",
    "            RandomApply(RandomOcclusion(), p=0.1),\n",
    "            ToTensor()\n",
    "        ]),\n",
    "        'val': Compose([\n",
    "            ToTensor()  # Only tensor conversion for validation\n",
    "        ])\n",
    "    }\n",
    "\n",
    "# ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ë¬´ì‘ìœ„ ê°€ìš°ì‹œì•ˆ(ì •ê·œë¶„í¬) ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬, ì‹¤ì œ ì´¬ì˜ í™˜ê²½ì—ì„œ \n",
    "# ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¡ìŒì— ëŒ€í•´ ëª¨ë¸ì´ ë” ê°•ê±´í•´ì§€ë„ë¡ ë§Œë“œëŠ” í´ë˜ìŠ¤\n",
    "class RandomNoise(object):\n",
    "    \"\"\"\n",
    "    Applies random Gaussian noise to video frames for data augmentation.\n",
    "    \n",
    "    This transformation helps the model become more robust to noise\n",
    "    that may be present in real-world video data.\n",
    "    \n",
    "    Args:\n",
    "        std (float): Standard deviation of the Gaussian noise as a fraction\n",
    "                     of the pixel value range (default: 0.05)\n",
    "    \"\"\"\n",
    "    def __init__(self, std=0.05):\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"\n",
    "        Apply random noise to the input frames.\n",
    "        \n",
    "        Args:\n",
    "            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n",
    "                                   where T is number of frames\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Noise-augmented frames, clipped to valid pixel range [0, 255]\n",
    "        \"\"\"\n",
    "        # ì§€ì •ëœ í‘œì¤€ í¸ì°¨ë¥¼ ê°€ì§„ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±\n",
    "        noise = np.random.normal(0, self.std * 255, frames.shape).astype(np.float32)\n",
    "        \n",
    "        # ìœ íš¨í•œ í”½ì…€ ë²”ìœ„ì— ë…¸ì´ì¦ˆ ë° í´ë¦½ ì¶”ê°€í•˜ê¸°\n",
    "        # ì˜ìƒì€ ì •ìˆ˜í˜• ë°ì´í„°ì—¬ì•¼ í•˜ë¯€ë¡œ í˜• ë³€í™˜ (astype)\n",
    "        return np.clip(frames + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "# ì˜ìƒ í”„ë ˆì„ì— ê²€ì€ìƒ‰ ì‚¬ê°í˜•ì„ ë¬´ì‘ìœ„ë¡œ ë®ì–´ ì”Œì›Œ, ì¼ë¶€ ì •ë³´ê°€ ê°€ë ¤ì¡Œì„ ë•Œë„ ëª¨ë¸ì´ ê²¬ë”œ ìˆ˜ ìˆë„ë¡ í›ˆë ¨ì‹œí‚¤ëŠ” í´ë˜ìŠ¤\n",
    "class RandomOcclusion(object):\n",
    "    \"\"\"\n",
    "    Simulates occlusion in video frames by adding black rectangles.\n",
    "    \n",
    "    This transformation helps the model learn to handle partial occlusions\n",
    "    that may occur in real-world scenarios when objects block the camera view.\n",
    "    \"\"\"\n",
    "    def __call__(self, frames):\n",
    "        \"\"\"\n",
    "        Apply random occlusion to the input frames.\n",
    "        \n",
    "        Args:\n",
    "            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n",
    "                                   where T is number of frames\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Frames with random occlusion applied\n",
    "        \"\"\"\n",
    "        # í”„ë ˆì„ í•˜ë‚˜ì˜ ì„¸ë¡œ(h), ê°€ë¡œ(w) ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°\n",
    "        h, w = frames.shape[1:3]\n",
    "        \n",
    "        # ì „ì²´ í”„ë ˆì„ í¬ê¸°ì˜ 10%~25% ì‚¬ì´ í¬ê¸°ì˜ ê°€ë¦¼ ì˜ì—­ í¬ê¸° ì„¤ì •\n",
    "        occl_h = np.random.randint(int(h * 0.1), int(h * 0.25))\n",
    "        occl_w = np.random.randint(int(w * 0.1), int(w * 0.25))\n",
    "        \n",
    "        # ì´ ê°€ë¦¼ ì˜ì—­ì´ ë“¤ì–´ê°ˆ ë¬´ì‘ìœ„ ìœ„ì¹˜ ì¢Œí‘œ ì„¤ì • \n",
    "        occl_x = np.random.randint(0, w - occl_w)\n",
    "        occl_y = np.random.randint(0, h - occl_h)\n",
    "        \n",
    "        # ì›ë³¸ í”„ë ˆì„ì„ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ ë³µì‚¬ë³¸ ë§Œë“¤ê¸°\n",
    "        frames_copy = frames.copy()\n",
    "        \n",
    "        # í”½ì…€ì„ 0(ê²€ì •ìƒ‰)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  í”„ë ˆì„ì— occlusion ì ìš©\n",
    "        for i in range(len(frames)):\n",
    "            frames_copy[i, occl_y:occl_y+occl_h, occl_x:occl_x+occl_w, :] = 0\n",
    "            \n",
    "        return frames_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8e09e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.424052Z",
     "iopub.status.busy": "2025-05-03T06:21:01.423737Z",
     "iopub.status.idle": "2025-05-03T06:21:01.432056Z",
     "shell.execute_reply": "2025-05-03T06:21:01.430955Z"
    },
    "papermill": {
     "duration": 0.017922,
     "end_time": "2025-05-03T06:21:01.433971",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.416049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ë¹„ë””ì˜¤ í”„ë ˆì„ ê°„ ì›€ì§ì„(ëª¨ì…˜)ì„ ì¶”ì í•˜ëŠ” 'optical_flow'ë¥¼ ê³„ì‚°í•´, ê°ì²´ë‚˜ ë°°ê²½ì˜ ì´ë™ ë°©í–¥ê³¼ ì†ë„ë¥¼ ë²¡í„° í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "# ë‘ ì—°ì†ëœ ì´ë¯¸ì§€(ë˜ëŠ” í”„ë ˆì„) ì‚¬ì´ì—ì„œ, ê° í”½ì…€ì´ ì–´ë–»ê²Œ ì´ë™í–ˆëŠ”ì§€ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ  -> optical_flow\n",
    "# Farneback ë°©ì‹ë§Œ ì‚¬ìš©\n",
    "# \"ëª¨ë“  í”½ì…€ì˜ ë°©í–¥ + ì†ë„ ì •ë³´ë¥¼ ë‹¤ ë‚¨ê¹€\"\n",
    "def compute_optical_flow_sequence(frames, skip_frames=1):\n",
    "    \"\"\"\n",
    "    Calculates per-frame optical flow magnitudes as a sequence.\n",
    "    \n",
    "    Args:\n",
    "        frames (numpy.ndarray): (T, H, W, C)\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: (T, 1) array of flow magnitudes (first frame is 0)\n",
    "    \"\"\"\n",
    "    T = len(frames)\n",
    "    if T < 2:\n",
    "        return np.zeros((T, 1), dtype=np.float32)\n",
    "    \n",
    "    magnitudes = [0.0]  # ì²« í”„ë ˆì„ì€ optical flowê°€ ì—†ìœ¼ë‹ˆ 0ìœ¼ë¡œ ì±„ì›€\n",
    "\n",
    "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    for i in range(1, T, skip_frames):\n",
    "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
    "        try:\n",
    "            flow = cv2.calcOpticalFlowFarneback(\n",
    "                prev_gray, curr_gray,\n",
    "                None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "            )\n",
    "            flow_magnitude = np.linalg.norm(flow, axis=-1).mean()  # (H, W) â†’ scalar mean\n",
    "            magnitudes.append(flow_magnitude)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating optical flow: {str(e)}\")\n",
    "            magnitudes.append(0.0)\n",
    "        \n",
    "        prev_gray = curr_gray\n",
    "\n",
    "    # ê¸¸ì´ê°€ ë¶€ì¡±í•˜ë©´ padding\n",
    "    while len(magnitudes) < T:\n",
    "        magnitudes.append(0.0)\n",
    "    \n",
    "    return np.array(magnitudes, dtype=np.float32).reshape(T, 1)  # (T, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01debadb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.448744Z",
     "iopub.status.busy": "2025-05-03T06:21:01.448403Z",
     "iopub.status.idle": "2025-05-03T06:21:18.321954Z",
     "shell.execute_reply": "2025-05-03T06:21:18.320944Z"
    },
    "papermill": {
     "duration": 16.88274,
     "end_time": "2025-05-03T06:21:18.323502",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.440762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 06:21:03.293860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746253263.565585      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746253263.640758      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6c61e",
   "metadata": {
    "papermill": {
     "duration": 0.006678,
     "end_time": "2025-05-03T06:21:18.338529",
     "exception": false,
     "start_time": "2025-05-03T06:21:18.331851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Positiveì´ë©´ alert_event ì‚¬ì´ë§Œ ë½‘ê³ , Negativeì´ë©´ ë§ˆì§€ë§‰ 3ì´ˆ êµ¬ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ**\n",
    "**ê·¸ë¦¬ê³  num_framesë§Œí¼ ê· ë“±í•˜ê²Œ ë½‘ê³  CNN + Optical Flow ë‘˜ ë‹¤ ê³„ì‚°** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84225b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:18.353835Z",
     "iopub.status.busy": "2025-05-03T06:21:18.353210Z",
     "iopub.status.idle": "2025-05-03T06:21:21.086649Z",
     "shell.execute_reply": "2025-05-03T06:21:21.085739Z"
    },
    "papermill": {
     "duration": 2.743492,
     "end_time": "2025-05-03T06:21:21.088551",
     "exception": false,
     "start_time": "2025-05-03T06:21:18.345059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 06:21:18.364151: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# InceptionV3 ëª¨ë¸ë¡œ íŠ¹ì„± ì¶”ì¶œ\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "cnn_feature_dim = base_model.output_shape[-1]\n",
    "\n",
    "def get_hybrid_feature_sequence(video_path, num_frames=12):\n",
    "    \"\"\"\n",
    "    Extract per-frame hybrid features (CNN + Optical flow) as a sequence.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to video file.\n",
    "        num_frames (int): Number of frames to extract.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: (T, 1281) array of per-frame features.\n",
    "    \"\"\"\n",
    "    # 1. í”„ë ˆì„ ì¶”ì¶œ\n",
    "    frames = extract_keyframes(video_path, num_frames=num_frames, target_size=(160,160))\n",
    "    \n",
    "    if len(frames) == 0:\n",
    "        print(f\"Skipping {video_path}: no frames\")\n",
    "        return np.zeros((num_frames, 1281), dtype=np.float32)\n",
    "\n",
    "    # 2. CNN feature per frame (Inception expects (N, H, W, C))\n",
    "    spatial_features = base_model.predict(\n",
    "        preprocess_input(frames.astype('float32')),\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )  # shape: (T, 1280)\n",
    "\n",
    "    # 3. Optical flow sequence\n",
    "    flow_magnitudes = compute_optical_flow_sequence(frames)  # shape: (T, 1)\n",
    "\n",
    "    # 4. Concatenate per frame\n",
    "    hybrid_features = np.concatenate([spatial_features, flow_magnitudes], axis=1)  # (T, 1281)\n",
    "\n",
    "    return hybrid_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3792cbf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:21.104402Z",
     "iopub.status.busy": "2025-05-03T06:21:21.104054Z",
     "iopub.status.idle": "2025-05-03T06:21:21.110609Z",
     "shell.execute_reply": "2025-05-03T06:21:21.109786Z"
    },
    "papermill": {
     "duration": 0.015986,
     "end_time": "2025-05-03T06:21:21.112025",
     "exception": false,
     "start_time": "2025-05-03T06:21:21.096039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hybrid_feature_sequence_from_frames(frames):\n",
    "    \"\"\"\n",
    "    Extract per-frame hybrid features (CNN + Optical flow) from pre-loaded frames.\n",
    "    \n",
    "    Args:\n",
    "        frames (torch.Tensor): (T, 3, 160, 160) tensor (after transform).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: (T, 1281) array of per-frame features.\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        print(\"Warning: empty frames input\")\n",
    "        return np.zeros((1, 1281), dtype=np.float32)\n",
    "\n",
    "    # 1ï¸âƒ£ PyTorch tensor â†’ numpy (T, 160, 160, 3), [0, 255] scale\n",
    "    frames_np = frames.permute(0, 2, 3, 1).numpy() * 255.0  # [0,1] â†’ [0,255]\n",
    "    frames_np = frames_np.astype(np.uint8)\n",
    "\n",
    "    # 2ï¸âƒ£ CNN Features per frame\n",
    "    spatial_features = base_model.predict(\n",
    "        preprocess_input(frames_np.astype('float32')),\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )  # shape: (T, 1280)\n",
    "\n",
    "    # 3ï¸âƒ£ Optical Flow per frame\n",
    "    flow_magnitudes = compute_optical_flow_sequence(frames_np)  # shape: (T, 1)\n",
    "\n",
    "    # 4ï¸âƒ£ Concatenate â†’ (T, 1281)\n",
    "    hybrid_features = np.concatenate([spatial_features, flow_magnitudes], axis=1)\n",
    "\n",
    "    return hybrid_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f1ffa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:21.128016Z",
     "iopub.status.busy": "2025-05-03T06:21:21.127704Z",
     "iopub.status.idle": "2025-05-03T06:21:21.135486Z",
     "shell.execute_reply": "2025-05-03T06:21:21.134517Z"
    },
    "papermill": {
     "duration": 0.017468,
     "end_time": "2025-05-03T06:21:21.137072",
     "exception": false,
     "start_time": "2025-05-03T06:21:21.119604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_optical_flow_sequence(frames, skip_frames=1):\n",
    "    \"\"\"\n",
    "    Computes per-frame optical flow magnitudes.\n",
    "    \n",
    "    Args:\n",
    "        frames (np.ndarray): (T, H, W, 3) numpy array of frames.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: (T, 1) array of per-frame optical flow magnitudes.\n",
    "    \"\"\"\n",
    "    T = len(frames)\n",
    "    if T < 2:\n",
    "        return np.zeros((T, 1), dtype=np.float32)\n",
    "\n",
    "    magnitudes = []\n",
    "\n",
    "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    for i in range(1, T, skip_frames):\n",
    "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
    "        try:\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray,\n",
    "                                                None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            # magnitude = sqrt(u^2 + v^2)\n",
    "            mag = np.linalg.norm(flow, axis=-1)  # shape (H, W)\n",
    "            avg_mag = np.mean(mag)  # scalar\n",
    "            magnitudes.append(avg_mag)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating flow at frame {i}: {str(e)}\")\n",
    "            magnitudes.append(0.0)\n",
    "\n",
    "        prev_gray = curr_gray\n",
    "\n",
    "    # ë§ˆì§€ë§‰ ê¸¸ì´ ë§ì¶¤ (T, 1)\n",
    "    if len(magnitudes) < T:\n",
    "        magnitudes.append(0.0)  # ë§ˆì§€ë§‰ í”„ë ˆì„ì€ flowê°€ ì—†ìŒ\n",
    "\n",
    "    magnitudes = np.array(magnitudes, dtype=np.float32).reshape(-1, 1)  # (T, 1)\n",
    "\n",
    "    return magnitudes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70a6929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:21.153202Z",
     "iopub.status.busy": "2025-05-03T06:21:21.152395Z",
     "iopub.status.idle": "2025-05-03T06:21:21.158845Z",
     "shell.execute_reply": "2025-05-03T06:21:21.157954Z"
    },
    "papermill": {
     "duration": 0.015977,
     "end_time": "2025-05-03T06:21:21.160273",
     "exception": false,
     "start_time": "2025-05-03T06:21:21.144296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ì•„ì§ë„ ìµœì¢…ì ìœ¼ë¡œ Transformerì— ë„£ì€ (T, 1281) ì‹œí€€ìŠ¤ëŠ” ë§Œë“¤ì–´ì§€ì§€ ì•ŠìŒ\n",
    "# 1. í”„ë ˆì„ë³„ CNN Feature ì¶”ì¶œ (InceptionV3) ì¶”ì¶œ\n",
    "# 2. optical flow sequence (compute_optical_flow_sequence) ì¶”ì¶œ\n",
    "# 3. ë‘ ê²°ê³¼ë¬¼ concat\n",
    "# 4. ì´ê±¸ Transformerì˜ input ì‹œí€€ìŠ¤ë¡œ ì‚¬ìš©\n",
    "\n",
    "# ì „ì²´ ì²˜ë¦¬ í•¨ìˆ˜ (ì´ì œ ë‘˜ì„ ê²°í•©í•˜ëŠ” í•¨ìˆ˜ ìƒì„±)\n",
    "# CNN + optical flow ë¶™ì—¬ì„œ (T, 1281) ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜ \n",
    "\n",
    "def prepare_transformer_input(video_path, num_frames=12, target_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Prepares (T, 1281) input sequence combining CNN features + optical flow\n",
    "    for a given video.\n",
    "    \"\"\"\n",
    "    # === Step 1: Extract frames ===\n",
    "    frames = extract_keyframes(video_path, num_frames=num_frames, target_size=target_size)\n",
    "    if frames.shape[0] == 0:\n",
    "        print(f\"Skipping video {video_path} (no frames)\")\n",
    "        return None  # or np.zeros((num_frames, 1281)) as fallback\n",
    "\n",
    "    # === Step 2: Extract CNN (spatial) features per frame ===\n",
    "    frames_float = preprocess_input(frames.astype('float32'))  # preprocess for InceptionV3\n",
    "    spatial_features = base_model.predict(frames_float, batch_size=32, verbose=0)  # (T, 1280)\n",
    "\n",
    "    # === Step 3: Compute optical flow sequence ===\n",
    "    optical_flow_sequence = compute_optical_flow_sequence(frames)  # (T, 1)\n",
    "\n",
    "    # === Step 4: Combine both ===\n",
    "    combined_features = np.concatenate([spatial_features, optical_flow_sequence], axis=1)  # (T, 1281)\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76158f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:21.176161Z",
     "iopub.status.busy": "2025-05-03T06:21:21.175719Z",
     "iopub.status.idle": "2025-05-03T08:21:05.946330Z",
     "shell.execute_reply": "2025-05-03T08:21:05.943906Z"
    },
    "papermill": {
     "duration": 7184.780783,
     "end_time": "2025-05-03T08:21:05.948133",
     "exception": false,
     "start_time": "2025-05-03T06:21:21.167350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–         | 50/1500 [03:57<1:42:41,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 sequences â†’ /kaggle/working/all_sequences_partial_50.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 100/1500 [07:57<2:05:28,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 sequences â†’ /kaggle/working/all_sequences_partial_100.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 150/1500 [11:51<1:42:24,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 150 sequences â†’ /kaggle/working/all_sequences_partial_150.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–        | 200/1500 [15:51<1:48:44,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 sequences â†’ /kaggle/working/all_sequences_partial_200.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 250/1500 [19:44<1:45:17,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 250 sequences â†’ /kaggle/working/all_sequences_partial_250.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 300/1500 [23:40<1:37:48,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 300 sequences â†’ /kaggle/working/all_sequences_partial_300.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–       | 350/1500 [27:39<1:20:30,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 350 sequences â†’ /kaggle/working/all_sequences_partial_350.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 400/1500 [31:42<1:29:44,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 400 sequences â†’ /kaggle/working/all_sequences_partial_400.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 450/1500 [35:48<1:28:02,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 450 sequences â†’ /kaggle/working/all_sequences_partial_450.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 500/1500 [39:56<1:19:01,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 sequences â†’ /kaggle/working/all_sequences_partial_500.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 550/1500 [43:52<1:21:18,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 550 sequences â†’ /kaggle/working/all_sequences_partial_550.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 600/1500 [47:55<1:16:37,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 600 sequences â†’ /kaggle/working/all_sequences_partial_600.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 650/1500 [51:52<1:15:37,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 650 sequences â†’ /kaggle/working/all_sequences_partial_650.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 700/1500 [55:32<1:00:59,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 700 sequences â†’ /kaggle/working/all_sequences_partial_700.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 750/1500 [59:28<52:49,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 750 sequences â†’ /kaggle/working/all_sequences_partial_750.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 800/1500 [1:03:41<55:48,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 800 sequences â†’ /kaggle/working/all_sequences_partial_800.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 850/1500 [1:07:35<57:34,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 850 sequences â†’ /kaggle/working/all_sequences_partial_850.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 900/1500 [1:11:49<44:24,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 900 sequences â†’ /kaggle/working/all_sequences_partial_900.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 950/1500 [1:15:53<38:06,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 950 sequences â†’ /kaggle/working/all_sequences_partial_950.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1000/1500 [1:19:46<41:36,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 sequences â†’ /kaggle/working/all_sequences_partial_1000.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1050/1500 [1:23:39<29:18,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1050 sequences â†’ /kaggle/working/all_sequences_partial_1050.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1100/1500 [1:27:48<33:54,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1100 sequences â†’ /kaggle/working/all_sequences_partial_1100.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1150/1500 [1:31:38<29:59,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1150 sequences â†’ /kaggle/working/all_sequences_partial_1150.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1200/1500 [1:35:27<22:51,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1200 sequences â†’ /kaggle/working/all_sequences_partial_1200.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1250/1500 [1:39:15<22:36,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1250 sequences â†’ /kaggle/working/all_sequences_partial_1250.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1300/1500 [1:43:18<16:06,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1300 sequences â†’ /kaggle/working/all_sequences_partial_1300.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1350/1500 [1:47:30<12:05,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1350 sequences â†’ /kaggle/working/all_sequences_partial_1350.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1400/1500 [1:51:33<07:57,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1400 sequences â†’ /kaggle/working/all_sequences_partial_1400.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1450/1500 [1:55:39<04:12,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1450 sequences â†’ /kaggle/working/all_sequences_partial_1450.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [1:59:44<00:00,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1500 sequences â†’ /kaggle/working/all_sequences_partial_1500.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final saved â†’ /kaggle/working/all_sequences_final.npy\n"
     ]
    }
   ],
   "source": [
    "# ì¤‘ê°„ ì €ì¥ í¬í•¨ ì½”ë“œ\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1ï¸âƒ£ transform ì¤€ë¹„\n",
    "transforms = get_video_transforms()\n",
    "train_transform = transforms['train']\n",
    "\n",
    "# 2ï¸âƒ£ ì €ì¥í•  í´ë” ì„¤ì •\n",
    "output_dir = '/kaggle/working/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 3ï¸âƒ£ feature ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "all_sequences = []\n",
    "\n",
    "# 4ï¸âƒ£ ì¤‘ê°„ ì €ì¥ ì£¼ê¸°\n",
    "save_every = 50\n",
    "\n",
    "# 5ï¸âƒ£ ë°˜ë³µ\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    video_id = row['id']\n",
    "    video_path = os.path.join(train_video_dir, f\"{int(video_id):05d}.mp4\")\n",
    "\n",
    "    sequence = prepare_transformer_input(video_path, num_frames=12)\n",
    "\n",
    "    if sequence is None:\n",
    "        print(f\"Skipping video {video_id} (no valid sequence)\")\n",
    "        continue\n",
    "\n",
    "    all_sequences.append(sequence)\n",
    "\n",
    "    # ğŸ”¥ Nê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥\n",
    "    if (idx + 1) % save_every == 0:\n",
    "        partial_path = os.path.join(output_dir, f'all_sequences_partial_{idx+1}.npy')\n",
    "        np.save(partial_path, np.array(all_sequences))\n",
    "        print(f\"Saved {idx + 1} sequences â†’ {partial_path}\")\n",
    "\n",
    "# 6ï¸âƒ£ ìµœì¢… ì €ì¥\n",
    "final_path = os.path.join(output_dir, 'all_sequences_final.npy')\n",
    "np.save(final_path, np.array(all_sequences))\n",
    "print(f\"\\nFinal saved â†’ {final_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d058165a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:06.117186Z",
     "iopub.status.busy": "2025-05-03T08:21:06.116445Z",
     "iopub.status.idle": "2025-05-03T08:21:06.643427Z",
     "shell.execute_reply": "2025-05-03T08:21:06.641779Z"
    },
    "papermill": {
     "duration": 0.614892,
     "end_time": "2025-05-03T08:21:06.646416",
     "exception": false,
     "start_time": "2025-05-03T08:21:06.031524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final array shape: (1500, 12, 2049)\n",
      "Combined array saved to: /kaggle/working/all_sequences_combined.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "output_dir = '/kaggle/working/'\n",
    "final_file = os.path.join(output_dir, 'all_sequences_final.npy')\n",
    "combined_save_path = os.path.join(output_dir, 'all_sequences_combined.npy')\n",
    "\n",
    "# âœ… ìµœì¢… ì €ì¥ëœ íŒŒì¼ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "final_array = np.load(final_file)\n",
    "print(f\"Final array shape: {final_array.shape}\")\n",
    "\n",
    "# âœ… combined íŒŒì¼ë¡œ ë”°ë¡œ ì €ì¥ (ë§Œì•½ í•„ìš”í•  ë•Œ ëŒ€ë¹„)\n",
    "np.save(combined_save_path, final_array)\n",
    "print(f\"Combined array saved to: {combined_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2440aa9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:06.818134Z",
     "iopub.status.busy": "2025-05-03T08:21:06.817252Z",
     "iopub.status.idle": "2025-05-03T08:21:06.983839Z",
     "shell.execute_reply": "2025-05-03T08:21:06.982817Z"
    },
    "papermill": {
     "duration": 0.256255,
     "end_time": "2025-05-03T08:21:06.985531",
     "exception": false,
     "start_time": "2025-05-03T08:21:06.729276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 12, 2049)\n"
     ]
    }
   ],
   "source": [
    "# (1) combined feature ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "import numpy as np\n",
    "all_sequences = np.load('/kaggle/working/all_sequences_combined.npy')\n",
    "print(all_sequences.shape)  # â†’ (n_videos, 12, 2049) ê°™ì€ ì¶œë ¥ í™•ì¸\n",
    "\n",
    "# (2) train.csv ë‹¤ì‹œ ë¡œë“œ\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('/kaggle/input/nexar-collision-prediction/train.csv')\n",
    "\n",
    "# (3) label ì¶”ì¶œ\n",
    "labels = train_df['target'].values  # shape (n_videos,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6fdf8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:07.155779Z",
     "iopub.status.busy": "2025-05-03T08:21:07.154979Z",
     "iopub.status.idle": "2025-05-03T08:21:07.596435Z",
     "shell.execute_reply": "2025-05-03T08:21:07.594902Z"
    },
    "papermill": {
     "duration": 0.52934,
     "end_time": "2025-05-03T08:21:07.598262",
     "exception": false,
     "start_time": "2025-05-03T08:21:07.068922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all_sequences shape: (1500, 12, 2049)\n",
      "Labels shape: (1500,)\n",
      "Dataset length: 1500\n",
      "Train size: 1200, Val size: 300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# 1ï¸âƒ£ ì €ì¥ëœ feature ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "all_sequences = np.load('all_sequences_combined.npy')\n",
    "print(f\"Loaded all_sequences shape: {all_sequences.shape}\")  # (1500, 12, 2049)\n",
    "\n",
    "# 2ï¸âƒ£ train_dfì—ì„œ label ë¶ˆëŸ¬ì˜¤ê¸° (ì£¼ì˜: feature ê°œìˆ˜ì— ë§ê²Œ ì˜ë¼ì£¼ê¸°!)\n",
    "labels = train_df['target'].values[:all_sequences.shape[0]]  # shape (1500,)\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# 3ï¸âƒ£ Dataset í´ë˜ìŠ¤ ì •ì˜\n",
    "class VideoSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences (numpy.ndarray): shape (n_samples, T, feature_dim)\n",
    "            labels (numpy.ndarray or list): shape (n_samples,)\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.sequences[idx]  # (T, feature_dim)\n",
    "        y = self.labels[idx]     # scalar or class\n",
    "        return x, y\n",
    "\n",
    "# 4ï¸âƒ£ Dataset ê°ì²´ ìƒì„±\n",
    "# Dataset ê°ì²´ ìƒì„±\n",
    "dataset = VideoSequenceDataset(all_sequences, labels)\n",
    "\n",
    "# ì •í™•í•œ ê¸¸ì´ ì²´í¬\n",
    "print(f\"Dataset length: {len(dataset)}\")  # ê¼­ ì°ì–´ë´!\n",
    "\n",
    "# Train/Val split\n",
    "train_size = int(0.8 * len(dataset))  # 80% split â†’ 1200 if 1500 total\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "print(f\"Train size: {train_size}, Val size: {val_size}\")\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb987d",
   "metadata": {
    "papermill": {
     "duration": 0.085668,
     "end_time": "2025-05-03T08:21:07.772836",
     "exception": false,
     "start_time": "2025-05-03T08:21:07.687168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ce8859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:07.937036Z",
     "iopub.status.busy": "2025-05-03T08:21:07.936680Z",
     "iopub.status.idle": "2025-05-03T08:21:07.940984Z",
     "shell.execute_reply": "2025-05-03T08:21:07.940163Z"
    },
    "papermill": {
     "duration": 0.086936,
     "end_time": "2025-05-03T08:21:07.942419",
     "exception": false,
     "start_time": "2025-05-03T08:21:07.855483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # DataLoader ìƒì„±\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# dataset = VideoSequenceDataset(all_sequences, labels)\n",
    "\n",
    "# # Train/Val ë¶„í• \n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "990bfcd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.117158Z",
     "iopub.status.busy": "2025-05-03T08:21:08.116354Z",
     "iopub.status.idle": "2025-05-03T08:21:08.125320Z",
     "shell.execute_reply": "2025-05-03T08:21:08.124463Z"
    },
    "papermill": {
     "duration": 0.097303,
     "end_time": "2025-05-03T08:21:08.126840",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.029537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Temporal Transformer ëª¨ë¸ ì„¤ê³„\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=1281, embed_dim=256, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TemporalTransformerModel, self).__init__()\n",
    "\n",
    "        # 1. Input â†’ embedding layer\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "        # 2. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 3. Classification head (binary classification)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Binary output (0~1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, T, input_dim)\n",
    "        \"\"\"\n",
    "        # Step 1: Project input features\n",
    "        x = self.input_proj(x)  # â†’ (batch_size, T, embed_dim)\n",
    "\n",
    "        # Step 2: Apply Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # â†’ (batch_size, T, embed_dim)\n",
    "\n",
    "        # Step 3: Aggregate (mean pooling over time)\n",
    "        x = x.mean(dim=1)  # â†’ (batch_size, embed_dim)\n",
    "\n",
    "        # Step 4: Final classification\n",
    "        out = self.classifier(x)  # â†’ (batch_size, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6007fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.308212Z",
     "iopub.status.busy": "2025-05-03T08:21:08.307810Z",
     "iopub.status.idle": "2025-05-03T08:21:08.315636Z",
     "shell.execute_reply": "2025-05-03T08:21:08.314725Z"
    },
    "papermill": {
     "duration": 0.097699,
     "end_time": "2025-05-03T08:21:08.317255",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.219556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spatial Transformer ëª¨ë¸ ì„¤ê³„\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=1280, hidden_dim=512, num_heads=8, num_layers=2, dropout=0.1):\n",
    "        super(SpatialTransformer, self).__init__()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # ë§ˆì§€ë§‰ summaryë¥¼ ìœ„í•œ pooling ë˜ëŠ” projection\n",
    "        self.output_layer = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (batch_size, T, input_dim) â†’ per-frame spatial features\n",
    "        \n",
    "        Returns:\n",
    "            out: shape (batch_size, input_dim) â†’ aggregated spatial feature\n",
    "        \"\"\"\n",
    "        # transformer expects (batch_size, T, input_dim)\n",
    "        x_transformed = self.transformer(x)  # (batch_size, T, input_dim)\n",
    "\n",
    "        # Pooling over time (mean pooling)\n",
    "        x_pooled = x_transformed.mean(dim=1)  # (batch_size, input_dim)\n",
    "\n",
    "        out = self.output_layer(x_pooled)  # (batch_size, input_dim)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a39e628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.493736Z",
     "iopub.status.busy": "2025-05-03T08:21:08.493396Z",
     "iopub.status.idle": "2025-05-03T08:21:08.501292Z",
     "shell.execute_reply": "2025-05-03T08:21:08.500334Z"
    },
    "papermill": {
     "duration": 0.096939,
     "end_time": "2025-05-03T08:21:08.502792",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.405853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CollisionPredictionModel(nn.Module):\n",
    "    def __init__(self, temporal_input_dim=2049, spatial_input_dim=1280, embed_dim=256, dropout=0.1):\n",
    "        super(CollisionPredictionModel, self).__init__()\n",
    "\n",
    "        self.temporal_transformer = TemporalTransformerModel(\n",
    "            input_dim=temporal_input_dim, embed_dim=embed_dim\n",
    "        )\n",
    "        self.spatial_transformer = SpatialTransformer(\n",
    "            input_dim=spatial_input_dim\n",
    "        )\n",
    "\n",
    "        fused_dim = embed_dim + spatial_input_dim  # Temporal + Spatial ì¶œë ¥ ì—°ê²°\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, temporal_input, spatial_input):\n",
    "        \"\"\"\n",
    "        temporal_input: (batch, T, 2049)\n",
    "        spatial_input: (batch, T, 1280)\n",
    "        \"\"\"\n",
    "        temporal_out = self.temporal_transformer(temporal_input)  # (batch, embed_dim)\n",
    "        spatial_out = self.spatial_transformer(spatial_input)      # (batch, spatial_input_dim)\n",
    "\n",
    "        # Fusion: concatenate\n",
    "        fused = torch.cat([temporal_out, spatial_out], dim=1)  # (batch, fused_dim)\n",
    "\n",
    "        out = self.classifier(fused)  # (batch, 1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbc94d60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.674569Z",
     "iopub.status.busy": "2025-05-03T08:21:08.674267Z",
     "iopub.status.idle": "2025-05-03T08:21:08.687000Z",
     "shell.execute_reply": "2025-05-03T08:21:08.686192Z"
    },
    "papermill": {
     "duration": 0.100024,
     "end_time": "2025-05-03T08:21:08.689223",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.589199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=256, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TemporalTransformerModel, self).__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)            # (batch, T, embed_dim)\n",
    "        x = self.transformer_encoder(x)   # (batch, T, embed_dim)\n",
    "        x = x.mean(dim=1)                 # (batch, embed_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=1280, hidden_dim=512, num_heads=8, num_layers=2, dropout=0.1):\n",
    "        super(SpatialTransformer, self).__init__()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.transformer(x)  # (batch, T, input_dim)\n",
    "        x_pooled = x_transformed.mean(dim=1) # (batch, input_dim)\n",
    "        out = self.output_layer(x_pooled)    # (batch, input_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, temporal_input_dim=2049, spatial_input_dim=1280,\n",
    "                 temporal_embed_dim=256, combined_dim=256, dropout=0.1):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        self.temporal_transformer = TemporalTransformerModel(\n",
    "            input_dim=temporal_input_dim, embed_dim=temporal_embed_dim\n",
    "        )\n",
    "        self.spatial_transformer = SpatialTransformer(\n",
    "            input_dim=spatial_input_dim\n",
    "        )\n",
    "\n",
    "        # temporal (256) + spatial (1280) = 1536\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1536, combined_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(combined_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, temporal_input, spatial_input):\n",
    "        temporal_out = self.temporal_transformer(temporal_input)  # (batch, 256)\n",
    "        spatial_out = self.spatial_transformer(spatial_input)     # (batch, 1280)\n",
    "\n",
    "        combined = torch.cat([temporal_out, spatial_out], dim=1) # (batch, 1536)\n",
    "        out = self.classifier(combined)                          # (batch, 1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01eddef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.859138Z",
     "iopub.status.busy": "2025-05-03T08:21:08.858804Z",
     "iopub.status.idle": "2025-05-03T08:28:41.904331Z",
     "shell.execute_reply": "2025-05-03T08:28:41.903175Z"
    },
    "papermill": {
     "duration": 453.13316,
     "end_time": "2025-05-03T08:28:41.906003",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.772843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6834\n",
      "Validation Accuracy: 0.5667\n",
      "Epoch [2/20], Loss: 0.6250\n",
      "Validation Accuracy: 0.6667\n",
      "Epoch [3/20], Loss: 0.5442\n",
      "Validation Accuracy: 0.6433\n",
      "Epoch [4/20], Loss: 0.4935\n",
      "Validation Accuracy: 0.6467\n",
      "Epoch [5/20], Loss: 0.3240\n",
      "Validation Accuracy: 0.6667\n",
      "Epoch [6/20], Loss: 0.2347\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch [7/20], Loss: 0.1393\n",
      "Validation Accuracy: 0.6067\n",
      "Epoch [8/20], Loss: 0.0583\n",
      "Validation Accuracy: 0.5967\n",
      "Epoch [9/20], Loss: 0.0777\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch [10/20], Loss: 0.0611\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch [11/20], Loss: 0.0505\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch [12/20], Loss: 0.0149\n",
      "Validation Accuracy: 0.6633\n",
      "Epoch [13/20], Loss: 0.0724\n",
      "Validation Accuracy: 0.6033\n",
      "Epoch [14/20], Loss: 0.0141\n",
      "Validation Accuracy: 0.6533\n",
      "Epoch [15/20], Loss: 0.0007\n",
      "Validation Accuracy: 0.6600\n",
      "Epoch [16/20], Loss: 0.0003\n",
      "Validation Accuracy: 0.6467\n",
      "Epoch [17/20], Loss: 0.0001\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch [18/20], Loss: 0.0001\n",
      "Validation Accuracy: 0.6467\n",
      "Epoch [19/20], Loss: 0.0001\n",
      "Validation Accuracy: 0.6433\n",
      "Epoch [20/20], Loss: 0.0001\n",
      "Validation Accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ëª¨ë¸ ì¤€ë¹„\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CombinedModel(\n",
    "    temporal_input_dim=2049, spatial_input_dim=1280,\n",
    "    temporal_embed_dim=256, combined_dim=256\n",
    ").to(device)\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)  # (batch, T, 2049)\n",
    "        labels = labels.to(device).unsqueeze(1)  # (batch, 1)\n",
    "\n",
    "        # temporal_input = spatial(1280) + flow(1) + ì¶”ê°€ optical flowë“¤ â†’ (2049)\n",
    "        temporal_input = inputs[:, :, :2049]\n",
    "\n",
    "        # spatial_input = spatial part only â†’ (1280)\n",
    "        spatial_input = inputs[:, :, :1280]\n",
    "\n",
    "        # forward\n",
    "        outputs = model(temporal_input, spatial_input)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "    \n",
    "            temporal_input = inputs[:, :, :2049]\n",
    "            spatial_input = inputs[:, :, :1280]\n",
    "    \n",
    "            outputs = model(temporal_input, spatial_input)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "    \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f0784a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:28:42.083745Z",
     "iopub.status.busy": "2025-05-03T08:28:42.083330Z",
     "iopub.status.idle": "2025-05-03T08:28:42.341344Z",
     "shell.execute_reply": "2025-05-03T08:28:42.340025Z"
    },
    "papermill": {
     "duration": 0.347859,
     "end_time": "2025-05-03T08:28:42.343129",
     "exception": false,
     "start_time": "2025-05-03T08:28:41.995270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥ \n",
    "torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2dda785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:28:42.527951Z",
     "iopub.status.busy": "2025-05-03T08:28:42.527576Z",
     "iopub.status.idle": "2025-05-03T08:28:42.955955Z",
     "shell.execute_reply": "2025-05-03T08:28:42.954877Z"
    },
    "papermill": {
     "duration": 0.519809,
     "end_time": "2025-05-03T08:28:42.957970",
     "exception": false,
     "start_time": "2025-05-03T08:28:42.438161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (temporal_transformer): TemporalTransformerModel(\n",
       "    (input_proj): Linear(in_features=2049, out_features=256, bias=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (spatial_transformer): SpatialTransformer(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=1280, bias=True)\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê°™ì€ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¤€ë¹„ (ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "model = CombinedModel(\n",
    "    temporal_input_dim=2049,  # ì£¼ì˜: í•™ìŠµí•  ë•Œì™€ ë™ì¼í•´ì•¼ í•œë‹¤\n",
    "    spatial_input_dim=1280,\n",
    "    temporal_embed_dim=256,\n",
    "    combined_dim=256\n",
    ")\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12dbab2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:28:43.241271Z",
     "iopub.status.busy": "2025-05-03T08:28:43.240902Z",
     "iopub.status.idle": "2025-05-03T10:08:12.586986Z",
     "shell.execute_reply": "2025-05-03T10:08:12.585934Z"
    },
    "papermill": {
     "duration": 5969.59319,
     "end_time": "2025-05-03T10:08:12.744555",
     "exception": false,
     "start_time": "2025-05-03T08:28:43.151365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1344/1344 [1:39:29<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "test_sequences = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Test Videos\"):\n",
    "    video_path = f\"{test_video_dir}/{int(float(row['id'])):05d}.mp4\"\n",
    "    sequence = prepare_transformer_input(video_path, num_frames=12)\n",
    "    if sequence is not None:\n",
    "        test_sequences.append(sequence)\n",
    "\n",
    "test_sequences = np.array(test_sequences)  # shape: (n_test, 12, 2049)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edf2d266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:08:13.070524Z",
     "iopub.status.busy": "2025-05-03T10:08:13.069968Z",
     "iopub.status.idle": "2025-05-03T10:08:13.245406Z",
     "shell.execute_reply": "2025-05-03T10:08:13.244462Z"
    },
    "papermill": {
     "duration": 0.342694,
     "end_time": "2025-05-03T10:08:13.247182",
     "exception": false,
     "start_time": "2025-05-03T10:08:12.904488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ìš© Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš©: dummy labels (ì˜ˆì¸¡ìš©ì´ë¼ ì‹¤ì œ labelì€ í•„ìš” ì—†ìŒ)\n",
    "dummy_labels = np.zeros(len(test_sequences))\n",
    "\n",
    "# Dataset\n",
    "test_dataset = VideoSequenceDataset(test_sequences, dummy_labels)\n",
    "\n",
    "# DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de3fffcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:08:13.550312Z",
     "iopub.status.busy": "2025-05-03T10:08:13.549922Z",
     "iopub.status.idle": "2025-05-03T10:08:20.363539Z",
     "shell.execute_reply": "2025-05-03T10:08:20.362448Z"
    },
    "papermill": {
     "duration": 6.964494,
     "end_time": "2025-05-03T10:08:20.365049",
     "exception": false,
     "start_time": "2025-05-03T10:08:13.400555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (1344,)\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹¤í–‰\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        temporal_input = inputs[:, :, :2049]\n",
    "        spatial_input = inputs[:, :, :1280]\n",
    "        outputs = model(temporal_input, spatial_input)\n",
    "        all_predictions.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "print(f\"Predictions shape: {all_predictions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6991a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:08:20.681180Z",
     "iopub.status.busy": "2025-05-03T10:08:20.680709Z",
     "iopub.status.idle": "2025-05-03T10:08:20.731369Z",
     "shell.execute_reply": "2025-05-03T10:08:20.729985Z"
    },
    "papermill": {
     "duration": 0.212031,
     "end_time": "2025-05-03T10:08:20.733175",
     "exception": false,
     "start_time": "2025-05-03T10:08:20.521144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv!\n",
      "                id         score\n",
      "count  1344.000000  1.344000e+03\n",
      "mean   1906.876488  4.461332e-01\n",
      "std     847.105655  4.724515e-01\n",
      "min       1.000000  4.918239e-07\n",
      "25%    1206.750000  7.033291e-05\n",
      "50%    2243.500000  1.134222e-01\n",
      "75%    2579.250000  9.994114e-01\n",
      "max    2915.000000  9.999987e-01\n"
     ]
    }
   ],
   "source": [
    "# Kaggle ì œì¶œìš© CSV\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'score': all_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Saved submission.csv!\")\n",
    "\n",
    "# ìš”ì•½ í™•ì¸\n",
    "print(submission.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea997f",
   "metadata": {
    "papermill": {
     "duration": 0.270133,
     "end_time": "2025-05-03T10:08:21.164651",
     "exception": false,
     "start_time": "2025-05-03T10:08:20.894518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11038207,
     "sourceId": 92399,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13668.534485,
   "end_time": "2025-05-03T10:08:24.105889",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-03T06:20:35.571404",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
