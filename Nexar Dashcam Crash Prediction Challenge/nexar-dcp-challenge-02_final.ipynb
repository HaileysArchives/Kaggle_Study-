{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb713e2",
   "metadata": {
    "papermill": {
     "duration": 0.0077,
     "end_time": "2025-05-03T06:20:40.491294",
     "exception": false,
     "start_time": "2025-05-03T06:20:40.483594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **💡Reference Notebook - Fernandosr85 - Dashcam Collision Prediction Project 🚗**\n",
    "#### **https://www.kaggle.com/code/fernandosr85/dashcam-collision-prediction-project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dfa2198",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-03T06:20:40.505193Z",
     "iopub.status.busy": "2025-05-03T06:20:40.504873Z",
     "iopub.status.idle": "2025-05-03T06:20:47.153847Z",
     "shell.execute_reply": "2025-05-03T06:20:47.152858Z"
    },
    "papermill": {
     "duration": 6.657537,
     "end_time": "2025-05-03T06:20:47.155488",
     "exception": false,
     "start_time": "2025-05-03T06:20:40.497951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nexar-collision-prediction/sample_submission.csv\n",
      "/kaggle/input/nexar-collision-prediction/train.csv\n",
      "/kaggle/input/nexar-collision-prediction/test.csv\n",
      "/kaggle/input/nexar-collision-prediction/test/02772.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02807.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02509.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/00350.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02163.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02707.mp4\n",
      "/kaggle/input/nexar-collision-prediction/test/02741.mp4\n",
      "/kaggle/input/nexar-collision-prediction/train/02059.mp4\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "max_files = 10 \n",
    "\n",
    "count = 0\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        count += 1\n",
    "        if count >= max_files:\n",
    "            break\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3ac151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:20:47.170616Z",
     "iopub.status.busy": "2025-05-03T06:20:47.170203Z",
     "iopub.status.idle": "2025-05-03T06:21:01.264460Z",
     "shell.execute_reply": "2025-05-03T06:21:01.263416Z"
    },
    "papermill": {
     "duration": 14.104202,
     "end_time": "2025-05-03T06:21:01.266035",
     "exception": false,
     "start_time": "2025-05-03T06:20:47.161833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9de6400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.280553Z",
     "iopub.status.busy": "2025-05-03T06:21:01.280083Z",
     "iopub.status.idle": "2025-05-03T06:21:01.330794Z",
     "shell.execute_reply": "2025-05-03T06:21:01.329619Z"
    },
    "papermill": {
     "duration": 0.060245,
     "end_time": "2025-05-03T06:21:01.332815",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.272570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.csv:\n",
      "     id  time_of_event  time_of_alert  target\n",
      "0  1924            NaN            NaN       0\n",
      "1   822           19.5         18.633       1\n",
      "2  1429            NaN            NaN       0\n",
      "3   208           19.8         19.233       1\n",
      "4  1904            NaN            NaN       0\n",
      "\n",
      "Test.csv:\n",
      "    id\n",
      "0  204\n",
      "1   30\n",
      "2  146\n",
      "3   20\n",
      "4  511\n",
      "\n",
      "Sample Submission:\n",
      "    id  target\n",
      "0  204       0\n",
      "1   30       0\n",
      "2  146       0\n",
      "3   20       0\n",
      "4  511       0\n",
      "\n",
      "Video Directory Paths:\n",
      "Train videos are located at: /kaggle/input/nexar-collision-prediction/train\n",
      "Test videos are located at: /kaggle/input/nexar-collision-prediction/test\n"
     ]
    }
   ],
   "source": [
    "# Suppress unnecessary formatting warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Paths to the CSV files\n",
    "train_csv_path = '/kaggle/input/nexar-collision-prediction/train.csv'\n",
    "test_csv_path = '/kaggle/input/nexar-collision-prediction/test.csv'\n",
    "submission_csv_path = '/kaggle/input/nexar-collision-prediction/sample_submission.csv'\n",
    "\n",
    "# Paths to the video directories\n",
    "train_video_dir = '/kaggle/input/nexar-collision-prediction/train'\n",
    "test_video_dir = '/kaggle/input/nexar-collision-prediction/test'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "submission_df = pd.read_csv(submission_csv_path)\n",
    "\n",
    "# (추가) id 컬럼을 문자열(str)로 변환해서 .0 문제 없애기\n",
    "train_df['id'] = train_df['id'].astype(str)\n",
    "\n",
    "# Display the first few rows of the DataFrames\n",
    "print(\"Train.csv:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest.csv:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(\"\\nSample Submission:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Optional: handle NaN values if needed, filling with zero or another value\n",
    "train_df['time_of_event'] = train_df['time_of_event'].fillna(0)\n",
    "train_df['time_of_alert'] = train_df['time_of_alert'].fillna(0)\n",
    "\n",
    "# (추가) Check the video directory paths\n",
    "print(\"\\nVideo Directory Paths:\")\n",
    "print(f\"Train videos are located at: {train_video_dir}\")\n",
    "print(f\"Test videos are located at: {test_video_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f1650f",
   "metadata": {
    "papermill": {
     "duration": 0.00631,
     "end_time": "2025-05-03T06:21:01.345825",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.339515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Data Preprocessing and Feature Extraction** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57800e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.360208Z",
     "iopub.status.busy": "2025-05-03T06:21:01.359869Z",
     "iopub.status.idle": "2025-05-03T06:21:01.381245Z",
     "shell.execute_reply": "2025-05-03T06:21:01.379973Z"
    },
    "papermill": {
     "duration": 0.030518,
     "end_time": "2025-05-03T06:21:01.382775",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.352257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 일반적으로 충돌이 발생하는 마지막 부분에 초점을 맞춰 비디오에서 주요 프레임을 추출\n",
    "# 지수 분포를 사용하여 마지막에 가까운 프레임에 더 많은 가중치를 부여\n",
    "\n",
    "def extract_keyframes(video_path, num_frames=12, target_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Extracts key frames from the video, focusing on the final part where collisions typically occur.\n",
    "    Uses exponential distribution to give more weight to frames closer to the end.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path) # 동영상을 불러오기 위해 OpenCV의 videoCapture 객체 생성 \n",
    "\n",
    "    # 파일이 제대로 열리지 않았을 경우 대비한 예외 처리\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open the video: {video_path}\")\n",
    "        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "    # 총 프레임 수와 초당 프레임 수(FPS)를 가져오기 \n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    if total_frames <= 0:\n",
    "        print(f\"Video without frames: {video_path}\")\n",
    "        cap.release()\n",
    "        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    # 영상 길이(초 단위) 계산\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    # 짧은 영상 (10초 미만): 균등한 간격으로 프레임 추출\n",
    "    if duration < 10:\n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "    # 긴 영상 (10초 이상): 후반부에 더 집중해서 추출\n",
    "    else:\n",
    "        # 마지막 3초 동안 프레임의 80% 집중(중요 영역)\n",
    "        end_frames = int(num_frames * 0.8)\n",
    "        start_frames = num_frames - end_frames\n",
    "        \n",
    "        # 지난 3초 동안의 시작 인덱스를 계산\n",
    "        last_seconds = 3\n",
    "        last_frame_count = min(int(fps * last_seconds), total_frames - 1)\n",
    "        start_idx = max(0, total_frames - last_frame_count)\n",
    "        \n",
    "        # 마지막 프레임에 더 많은 가중치를 부여하는 지수 분포\n",
    "        # 이렇게 하면 마지막에 더 밀집된 인덱스가 생성된다 (\"프레임을 뽑는 간격\"자체를 조절 → 끝부분에 더 많이 몰리게 만드는 방식)\n",
    "        end_indices = np.array([\n",
    "            start_idx + int((total_frames - start_idx - 1) * (i/end_frames)**2) \n",
    "            for i in range(1, end_frames + 1)\n",
    "        ])\n",
    "        \n",
    "        # context에 맞게 균일하게 배포된 초기 프레임 (초반부에서 균등하게 추출한 프레임들)\n",
    "        # context란? 사고 직전에 어떤 상황이 펼쳐졌는지에 대한 흐름, 배경, 맥락 \n",
    "        begin_indices = np.linspace(0, start_idx - 1, start_frames, dtype=int) if start_idx > 0 else np.zeros(start_frames, dtype=int)\n",
    "        \n",
    "        # 인덱스 결합\n",
    "        frame_indices = np.concatenate([begin_indices, end_indices])\n",
    "    \n",
    "    # 선택한 프레임 추출 \n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Use higher resolution and better interpolation\n",
    "            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(frames, dtype=np.uint8)\n",
    "\n",
    "# 먼저, 전역 범위에서 변환 클래스를 정의 \n",
    "# 입력된 영상 프레임을 일정 확률로 좌우 반전시켜서, 데이터 다양성을 늘리는 역할\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        if np.random.random() < self.p:\n",
    "            return frames[:, :, ::-1, :].copy()  # horizontally flip each frame\n",
    "        return frames\n",
    "\n",
    "# 영상 프레임의 밝기와 대비를 무작위로 조정해, 다양한 조명 환경을 시뮬레이션하는 증강 클래스\n",
    "class ColorJitter(object):\n",
    "    def __init__(self, brightness=0, contrast=0):\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        # Apply brightness jitter\n",
    "        if self.brightness > 0:\n",
    "            brightness_factor = np.random.uniform(max(0, 1-self.brightness), 1+self.brightness)\n",
    "            frames = frames * brightness_factor\n",
    "            frames = np.clip(frames, 0, 255)\n",
    "        \n",
    "        # Apply contrast jitter\n",
    "        if self.contrast > 0:\n",
    "            contrast_factor = np.random.uniform(max(0, 1-self.contrast), 1+self.contrast)\n",
    "            frames = (frames - 128) * contrast_factor + 128\n",
    "            frames = np.clip(frames, 0, 255)\n",
    "            \n",
    "        return frames\n",
    "\n",
    "# 프레임에 흐릿한 안개 효과를 넣어, 시야가 나쁜 날씨 상황을 시뮬레이션하는 클래스\n",
    "class AddFog(object):\n",
    "    def __call__(self, frames):\n",
    "        fog = np.random.uniform(0.7, 0.9, frames.shape).astype(np.float32)\n",
    "        return frames * 0.8 + fog * 50  # Adjusted for 0-255 scale\n",
    "\n",
    "# 프레임에 흰색 선형 노이즈(빗방울)를 추가해 비 오는 날씨를 시뮬레이션하는 클래스\n",
    "class AddRain(object):\n",
    "    def __call__(self, frames):\n",
    "        h, w = frames.shape[1:3]\n",
    "        rain = np.random.uniform(0, 1, (len(frames), h, w, 1)).astype(np.float32)\n",
    "        rain = (rain > 0.97).astype(np.float32) * 200  # White rain drops\n",
    "        return np.clip(frames * 0.9 + rain, 0, 255)  # Darken a bit and add drops\n",
    "\n",
    "# 지정된 확률에 따라 어떤 변환을 적용할지 말지를 무작위로 결정하는 컨트롤러 클래스(랜덤성 부여)\n",
    "class RandomApply(object):\n",
    "    def __init__(self, transform, p=0.5):\n",
    "        self.transform = transform\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        if np.random.random() < self.p:\n",
    "            return self.transform(frames)\n",
    "        return frames\n",
    "\n",
    "# 여러 개의 변환(Flip, Jitter, Fog 등)을 순서대로 적용하는 데이터 증강 파이프라인 클래스\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        for t in self.transforms:\n",
    "            frames = t(frames)\n",
    "        return frames\n",
    "\n",
    "# 영상 프레임 배열을 PyTorch 텐서로 바꾸고, 픽셀 값을 0~1 범위로 정규화하는 클래스\n",
    "class ToTensor(object):\n",
    "    def __call__(self, frames):\n",
    "        # Convert from (T, H, W, C) to (T, C, H, W)\n",
    "        frames = frames.transpose(0, 3, 1, 2)\n",
    "        # Convert to tensor and normalize to [0, 1]\n",
    "        return torch.from_numpy(frames).float() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cfc1a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.397757Z",
     "iopub.status.busy": "2025-05-03T06:21:01.397460Z",
     "iopub.status.idle": "2025-05-03T06:21:01.407576Z",
     "shell.execute_reply": "2025-05-03T06:21:01.406814Z"
    },
    "papermill": {
     "duration": 0.019064,
     "end_time": "2025-05-03T06:21:01.408931",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.389867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 동영상에서 데이터 증강을 위한 변환을 반환\n",
    "\n",
    "def get_video_transforms():\n",
    "    \"\"\"\n",
    "    Returns transformations for data augmentation in videos.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'train': Compose([\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "            ColorJitter(brightness=0.3, contrast=0.3),\n",
    "            RandomApply(AddFog(), p=0.15),\n",
    "            RandomApply(AddRain(), p=0.15),\n",
    "            RandomApply(RandomNoise(0.05), p=0.2), \n",
    "            RandomApply(RandomOcclusion(), p=0.1),\n",
    "            ToTensor()\n",
    "        ]),\n",
    "        'val': Compose([\n",
    "            ToTensor()  # Only tensor conversion for validation\n",
    "        ])\n",
    "    }\n",
    "\n",
    "# 비디오 프레임에서 무작위 가우시안(정규분포) 노이즈를 추가하여, 실제 촬영 환경에서 \n",
    "# 발생할 수 있는 잡음에 대해 모델이 더 강건해지도록 만드는 클래스\n",
    "class RandomNoise(object):\n",
    "    \"\"\"\n",
    "    Applies random Gaussian noise to video frames for data augmentation.\n",
    "    \n",
    "    This transformation helps the model become more robust to noise\n",
    "    that may be present in real-world video data.\n",
    "    \n",
    "    Args:\n",
    "        std (float): Standard deviation of the Gaussian noise as a fraction\n",
    "                     of the pixel value range (default: 0.05)\n",
    "    \"\"\"\n",
    "    def __init__(self, std=0.05):\n",
    "        self.std = std\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"\n",
    "        Apply random noise to the input frames.\n",
    "        \n",
    "        Args:\n",
    "            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n",
    "                                   where T is number of frames\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Noise-augmented frames, clipped to valid pixel range [0, 255]\n",
    "        \"\"\"\n",
    "        # 지정된 표준 편차를 가진 가우시안 노이즈 생성\n",
    "        noise = np.random.normal(0, self.std * 255, frames.shape).astype(np.float32)\n",
    "        \n",
    "        # 유효한 픽셀 범위에 노이즈 및 클립 추가하기\n",
    "        # 영상은 정수형 데이터여야 하므로 형 변환 (astype)\n",
    "        return np.clip(frames + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "# 영상 프레임에 검은색 사각형을 무작위로 덮어 씌워, 일부 정보가 가려졌을 때도 모델이 견딜 수 있도록 훈련시키는 클래스\n",
    "class RandomOcclusion(object):\n",
    "    \"\"\"\n",
    "    Simulates occlusion in video frames by adding black rectangles.\n",
    "    \n",
    "    This transformation helps the model learn to handle partial occlusions\n",
    "    that may occur in real-world scenarios when objects block the camera view.\n",
    "    \"\"\"\n",
    "    def __call__(self, frames):\n",
    "        \"\"\"\n",
    "        Apply random occlusion to the input frames.\n",
    "        \n",
    "        Args:\n",
    "            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n",
    "                                   where T is number of frames\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Frames with random occlusion applied\n",
    "        \"\"\"\n",
    "        # 프레임 하나의 세로(h), 가로(w) 길이 가져오기\n",
    "        h, w = frames.shape[1:3]\n",
    "        \n",
    "        # 전체 프레임 크기의 10%~25% 사이 크기의 가림 영역 크기 설정\n",
    "        occl_h = np.random.randint(int(h * 0.1), int(h * 0.25))\n",
    "        occl_w = np.random.randint(int(w * 0.1), int(w * 0.25))\n",
    "        \n",
    "        # 이 가림 영역이 들어갈 무작위 위치 좌표 설정 \n",
    "        occl_x = np.random.randint(0, w - occl_w)\n",
    "        occl_y = np.random.randint(0, h - occl_h)\n",
    "        \n",
    "        # 원본 프레임을 수정하지 않도록 복사본 만들기\n",
    "        frames_copy = frames.copy()\n",
    "        \n",
    "        # 픽셀을 0(검정색)으로 설정하여 모든 프레임에 occlusion 적용\n",
    "        for i in range(len(frames)):\n",
    "            frames_copy[i, occl_y:occl_y+occl_h, occl_x:occl_x+occl_w, :] = 0\n",
    "            \n",
    "        return frames_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8e09e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.424052Z",
     "iopub.status.busy": "2025-05-03T06:21:01.423737Z",
     "iopub.status.idle": "2025-05-03T06:21:01.432056Z",
     "shell.execute_reply": "2025-05-03T06:21:01.430955Z"
    },
    "papermill": {
     "duration": 0.017922,
     "end_time": "2025-05-03T06:21:01.433971",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.416049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 비디오 프레임 간 움직임(모션)을 추적하는 'optical_flow'를 계산해, 객체나 배경의 이동 방향과 속도를 벡터 형태로 반환하는 함수\n",
    "# 두 연속된 이미지(또는 프레임) 사이에서, 각 픽셀이 어떻게 이동했는지를 벡터로 표현하는 기술 -> optical_flow\n",
    "# Farneback 방식만 사용\n",
    "# \"모든 픽셀의 방향 + 속도 정보를 다 남김\"\n",
    "def compute_optical_flow_sequence(frames, skip_frames=1):\n",
    "    \"\"\"\n",
    "    Calculates per-frame optical flow magnitudes as a sequence.\n",
    "    \n",
    "    Args:\n",
    "        frames (numpy.ndarray): (T, H, W, C)\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: (T, 1) array of flow magnitudes (first frame is 0)\n",
    "    \"\"\"\n",
    "    T = len(frames)\n",
    "    if T < 2:\n",
    "        return np.zeros((T, 1), dtype=np.float32)\n",
    "    \n",
    "    magnitudes = [0.0]  # 첫 프레임은 optical flow가 없으니 0으로 채움\n",
    "\n",
    "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    for i in range(1, T, skip_frames):\n",
    "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
    "        try:\n",
    "            flow = cv2.calcOpticalFlowFarneback(\n",
    "                prev_gray, curr_gray,\n",
    "                None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
    "            )\n",
    "            flow_magnitude = np.linalg.norm(flow, axis=-1).mean()  # (H, W) → scalar mean\n",
    "            magnitudes.append(flow_magnitude)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating optical flow: {str(e)}\")\n",
    "            magnitudes.append(0.0)\n",
    "        \n",
    "        prev_gray = curr_gray\n",
    "\n",
    "    # 길이가 부족하면 padding\n",
    "    while len(magnitudes) < T:\n",
    "        magnitudes.append(0.0)\n",
    "    \n",
    "    return np.array(magnitudes, dtype=np.float32).reshape(T, 1)  # (T, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01debadb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:01.448744Z",
     "iopub.status.busy": "2025-05-03T06:21:01.448403Z",
     "iopub.status.idle": "2025-05-03T06:21:18.321954Z",
     "shell.execute_reply": "2025-05-03T06:21:18.320944Z"
    },
    "papermill": {
     "duration": 16.88274,
     "end_time": "2025-05-03T06:21:18.323502",
     "exception": false,
     "start_time": "2025-05-03T06:21:01.440762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 06:21:03.293860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746253263.565585      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746253263.640758      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6c61e",
   "metadata": {
    "papermill": {
     "duration": 0.006678,
     "end_time": "2025-05-03T06:21:18.338529",
     "exception": false,
     "start_time": "2025-05-03T06:21:18.331851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Positive이면 alert_event 사이만 뽑고, Negative이면 마지막 3초 구간을 기준으로 추출**\n",
    "**그리고 num_frames만큼 균등하게 뽑고 CNN + Optical Flow 둘 다 계산** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84225b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:18.353835Z",
     "iopub.status.busy": "2025-05-03T06:21:18.353210Z",
     "iopub.status.idle": "2025-05-03T06:21:21.086649Z",
     "shell.execute_reply": "2025-05-03T06:21:21.085739Z"
    },
    "papermill": {
     "duration": 2.743492,
     "end_time": "2025-05-03T06:21:21.088551",
     "exception": false,
     "start_time": "2025-05-03T06:21:18.345059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 06:21:18.364151: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# InceptionV3 모델로 특성 추출\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "cnn_feature_dim = base_model.output_shape[-1]\n",
    "\n",
    "def get_hybrid_feature_sequence(video_path, num_frames=12):\n",
    "    \"\"\"\n",
    "    Extract per-frame hybrid features (CNN + Optical flow) as a sequence.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to video file.\n",
    "        num_frames (int): Number of frames to extract.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: (T, 1281) array of per-frame features.\n",
    "    \"\"\"\n",
    "    # 1. 프레임 추출\n",
    "    frames = extract_keyframes(video_path, num_frames=num_frames, target_size=(160,160))\n",
    "    \n",
    "    if len(frames) == 0:\n",
    "        print(f\"Skipping {video_path}: no frames\")\n",
    "        return np.zeros((num_frames, 1281), dtype=np.float32)\n",
    "\n",
    "    # 2. CNN feature per frame (Inception expects (N, H, W, C))\n",
    "    spatial_features = base_model.predict(\n",
    "        preprocess_input(frames.astype('float32')),\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )  # shape: (T, 1280)\n",
    "\n",
    "    # 3. Optical flow sequence\n",
    "    flow_magnitudes = compute_optical_flow_sequence(frames)  # shape: (T, 1)\n",
    "\n",
    "    # 4. Concatenate per frame\n",
    "    hybrid_features = np.concatenate([spatial_features, flow_magnitudes], axis=1)  # (T, 1281)\n",
    "\n",
    "    return hybrid_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3792cbf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:21.104402Z",
     "iopub.status.busy": "2025-05-03T06:21:21.104054Z",
     "iopub.status.idle": "2025-05-03T06:21:21.110609Z",
     "shell.execute_reply": "2025-05-03T06:21:21.109786Z"
    },
    "papermill": {
     "duration": 0.015986,
     "end_time": "2025-05-03T06:21:21.112025",
     "exception": false,
     "start_time": "2025-05-03T06:21:21.096039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hybrid_feature_sequence_from_frames(frames):\n",
    "    \"\"\"\n",
    "    Extract per-frame hybrid features (CNN + Optical flow) from pre-loaded frames.\n",
    "    \n",
    "    Args:\n",
    "        frames (torch.Tensor): (T, 3, 160, 160) tensor (after transform).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: (T, 1281) array of per-frame features.\n",
    "    \"\"\"\n",
    "    if len(frames) == 0:\n",
    "        print(\"Warning: empty frames input\")\n",
    "        return np.zeros((1, 1281), dtype=np.float32)\n",
    "\n",
    "    # 1️⃣ PyTorch tensor → numpy (T, 160, 160, 3), [0, 255] scale\n",
    "    frames_np = frames.permute(0, 2, 3, 1).numpy() * 255.0  # [0,1] → [0,255]\n",
    "    frames_np = frames_np.astype(np.uint8)\n",
    "\n",
    "    # 2️⃣ CNN Features per frame\n",
    "    spatial_features = base_model.predict(\n",
    "        preprocess_input(frames_np.astype('float32')),\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )  # shape: (T, 1280)\n",
    "\n",
    "    # 3️⃣ Optical Flow per frame\n",
    "    flow_magnitudes = compute_optical_flow_sequence(frames_np)  # shape: (T, 1)\n",
    "\n",
    "    # 4️⃣ Concatenate → (T, 1281)\n",
    "    hybrid_features = np.concatenate([spatial_features, flow_magnitudes], axis=1)\n",
    "\n",
    "    return hybrid_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f1ffa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:21.128016Z",
     "iopub.status.busy": "2025-05-03T06:21:21.127704Z",
     "iopub.status.idle": "2025-05-03T06:21:21.135486Z",
     "shell.execute_reply": "2025-05-03T06:21:21.134517Z"
    },
    "papermill": {
     "duration": 0.017468,
     "end_time": "2025-05-03T06:21:21.137072",
     "exception": false,
     "start_time": "2025-05-03T06:21:21.119604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_optical_flow_sequence(frames, skip_frames=1):\n",
    "    \"\"\"\n",
    "    Computes per-frame optical flow magnitudes.\n",
    "    \n",
    "    Args:\n",
    "        frames (np.ndarray): (T, H, W, 3) numpy array of frames.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: (T, 1) array of per-frame optical flow magnitudes.\n",
    "    \"\"\"\n",
    "    T = len(frames)\n",
    "    if T < 2:\n",
    "        return np.zeros((T, 1), dtype=np.float32)\n",
    "\n",
    "    magnitudes = []\n",
    "\n",
    "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    for i in range(1, T, skip_frames):\n",
    "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
    "        try:\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray,\n",
    "                                                None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            # magnitude = sqrt(u^2 + v^2)\n",
    "            mag = np.linalg.norm(flow, axis=-1)  # shape (H, W)\n",
    "            avg_mag = np.mean(mag)  # scalar\n",
    "            magnitudes.append(avg_mag)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating flow at frame {i}: {str(e)}\")\n",
    "            magnitudes.append(0.0)\n",
    "\n",
    "        prev_gray = curr_gray\n",
    "\n",
    "    # 마지막 길이 맞춤 (T, 1)\n",
    "    if len(magnitudes) < T:\n",
    "        magnitudes.append(0.0)  # 마지막 프레임은 flow가 없음\n",
    "\n",
    "    magnitudes = np.array(magnitudes, dtype=np.float32).reshape(-1, 1)  # (T, 1)\n",
    "\n",
    "    return magnitudes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70a6929",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:21.153202Z",
     "iopub.status.busy": "2025-05-03T06:21:21.152395Z",
     "iopub.status.idle": "2025-05-03T06:21:21.158845Z",
     "shell.execute_reply": "2025-05-03T06:21:21.157954Z"
    },
    "papermill": {
     "duration": 0.015977,
     "end_time": "2025-05-03T06:21:21.160273",
     "exception": false,
     "start_time": "2025-05-03T06:21:21.144296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 아직도 최종적으로 Transformer에 넣은 (T, 1281) 시퀀스는 만들어지지 않음\n",
    "# 1. 프레임별 CNN Feature 추출 (InceptionV3) 추출\n",
    "# 2. optical flow sequence (compute_optical_flow_sequence) 추출\n",
    "# 3. 두 결과물 concat\n",
    "# 4. 이걸 Transformer의 input 시퀀스로 사용\n",
    "\n",
    "# 전체 처리 함수 (이제 둘을 결합하는 함수 생성)\n",
    "# CNN + optical flow 붙여서 (T, 1281) 만들어주는 함수 \n",
    "\n",
    "def prepare_transformer_input(video_path, num_frames=12, target_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Prepares (T, 1281) input sequence combining CNN features + optical flow\n",
    "    for a given video.\n",
    "    \"\"\"\n",
    "    # === Step 1: Extract frames ===\n",
    "    frames = extract_keyframes(video_path, num_frames=num_frames, target_size=target_size)\n",
    "    if frames.shape[0] == 0:\n",
    "        print(f\"Skipping video {video_path} (no frames)\")\n",
    "        return None  # or np.zeros((num_frames, 1281)) as fallback\n",
    "\n",
    "    # === Step 2: Extract CNN (spatial) features per frame ===\n",
    "    frames_float = preprocess_input(frames.astype('float32'))  # preprocess for InceptionV3\n",
    "    spatial_features = base_model.predict(frames_float, batch_size=32, verbose=0)  # (T, 1280)\n",
    "\n",
    "    # === Step 3: Compute optical flow sequence ===\n",
    "    optical_flow_sequence = compute_optical_flow_sequence(frames)  # (T, 1)\n",
    "\n",
    "    # === Step 4: Combine both ===\n",
    "    combined_features = np.concatenate([spatial_features, optical_flow_sequence], axis=1)  # (T, 1281)\n",
    "\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76158f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T06:21:21.176161Z",
     "iopub.status.busy": "2025-05-03T06:21:21.175719Z",
     "iopub.status.idle": "2025-05-03T08:21:05.946330Z",
     "shell.execute_reply": "2025-05-03T08:21:05.943906Z"
    },
    "papermill": {
     "duration": 7184.780783,
     "end_time": "2025-05-03T08:21:05.948133",
     "exception": false,
     "start_time": "2025-05-03T06:21:21.167350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 50/1500 [03:57<1:42:41,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 sequences → /kaggle/working/all_sequences_partial_50.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 100/1500 [07:57<2:05:28,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 sequences → /kaggle/working/all_sequences_partial_100.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 150/1500 [11:51<1:42:24,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 150 sequences → /kaggle/working/all_sequences_partial_150.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 200/1500 [15:51<1:48:44,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 sequences → /kaggle/working/all_sequences_partial_200.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 250/1500 [19:44<1:45:17,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 250 sequences → /kaggle/working/all_sequences_partial_250.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 300/1500 [23:40<1:37:48,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 300 sequences → /kaggle/working/all_sequences_partial_300.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 350/1500 [27:39<1:20:30,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 350 sequences → /kaggle/working/all_sequences_partial_350.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 400/1500 [31:42<1:29:44,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 400 sequences → /kaggle/working/all_sequences_partial_400.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 450/1500 [35:48<1:28:02,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 450 sequences → /kaggle/working/all_sequences_partial_450.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 500/1500 [39:56<1:19:01,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 sequences → /kaggle/working/all_sequences_partial_500.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 550/1500 [43:52<1:21:18,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 550 sequences → /kaggle/working/all_sequences_partial_550.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 600/1500 [47:55<1:16:37,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 600 sequences → /kaggle/working/all_sequences_partial_600.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 650/1500 [51:52<1:15:37,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 650 sequences → /kaggle/working/all_sequences_partial_650.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 700/1500 [55:32<1:00:59,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 700 sequences → /kaggle/working/all_sequences_partial_700.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 750/1500 [59:28<52:49,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 750 sequences → /kaggle/working/all_sequences_partial_750.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 800/1500 [1:03:41<55:48,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 800 sequences → /kaggle/working/all_sequences_partial_800.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 850/1500 [1:07:35<57:34,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 850 sequences → /kaggle/working/all_sequences_partial_850.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 900/1500 [1:11:49<44:24,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 900 sequences → /kaggle/working/all_sequences_partial_900.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 950/1500 [1:15:53<38:06,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 950 sequences → /kaggle/working/all_sequences_partial_950.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1000/1500 [1:19:46<41:36,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 sequences → /kaggle/working/all_sequences_partial_1000.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1050/1500 [1:23:39<29:18,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1050 sequences → /kaggle/working/all_sequences_partial_1050.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1100/1500 [1:27:48<33:54,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1100 sequences → /kaggle/working/all_sequences_partial_1100.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1150/1500 [1:31:38<29:59,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1150 sequences → /kaggle/working/all_sequences_partial_1150.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1200/1500 [1:35:27<22:51,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1200 sequences → /kaggle/working/all_sequences_partial_1200.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1250/1500 [1:39:15<22:36,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1250 sequences → /kaggle/working/all_sequences_partial_1250.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1300/1500 [1:43:18<16:06,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1300 sequences → /kaggle/working/all_sequences_partial_1300.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1350/1500 [1:47:30<12:05,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1350 sequences → /kaggle/working/all_sequences_partial_1350.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1400/1500 [1:51:33<07:57,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1400 sequences → /kaggle/working/all_sequences_partial_1400.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1450/1500 [1:55:39<04:12,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1450 sequences → /kaggle/working/all_sequences_partial_1450.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [1:59:44<00:00,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1500 sequences → /kaggle/working/all_sequences_partial_1500.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final saved → /kaggle/working/all_sequences_final.npy\n"
     ]
    }
   ],
   "source": [
    "# 중간 저장 포함 코드\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1️⃣ transform 준비\n",
    "transforms = get_video_transforms()\n",
    "train_transform = transforms['train']\n",
    "\n",
    "# 2️⃣ 저장할 폴더 설정\n",
    "output_dir = '/kaggle/working/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 3️⃣ feature 저장용 리스트\n",
    "all_sequences = []\n",
    "\n",
    "# 4️⃣ 중간 저장 주기\n",
    "save_every = 50\n",
    "\n",
    "# 5️⃣ 반복\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
    "    video_id = row['id']\n",
    "    video_path = os.path.join(train_video_dir, f\"{int(video_id):05d}.mp4\")\n",
    "\n",
    "    sequence = prepare_transformer_input(video_path, num_frames=12)\n",
    "\n",
    "    if sequence is None:\n",
    "        print(f\"Skipping video {video_id} (no valid sequence)\")\n",
    "        continue\n",
    "\n",
    "    all_sequences.append(sequence)\n",
    "\n",
    "    # 🔥 N개마다 중간 저장\n",
    "    if (idx + 1) % save_every == 0:\n",
    "        partial_path = os.path.join(output_dir, f'all_sequences_partial_{idx+1}.npy')\n",
    "        np.save(partial_path, np.array(all_sequences))\n",
    "        print(f\"Saved {idx + 1} sequences → {partial_path}\")\n",
    "\n",
    "# 6️⃣ 최종 저장\n",
    "final_path = os.path.join(output_dir, 'all_sequences_final.npy')\n",
    "np.save(final_path, np.array(all_sequences))\n",
    "print(f\"\\nFinal saved → {final_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d058165a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:06.117186Z",
     "iopub.status.busy": "2025-05-03T08:21:06.116445Z",
     "iopub.status.idle": "2025-05-03T08:21:06.643427Z",
     "shell.execute_reply": "2025-05-03T08:21:06.641779Z"
    },
    "papermill": {
     "duration": 0.614892,
     "end_time": "2025-05-03T08:21:06.646416",
     "exception": false,
     "start_time": "2025-05-03T08:21:06.031524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final array shape: (1500, 12, 2049)\n",
      "Combined array saved to: /kaggle/working/all_sequences_combined.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 경로 설정\n",
    "output_dir = '/kaggle/working/'\n",
    "final_file = os.path.join(output_dir, 'all_sequences_final.npy')\n",
    "combined_save_path = os.path.join(output_dir, 'all_sequences_combined.npy')\n",
    "\n",
    "# ✅ 최종 저장된 파일만 불러오기\n",
    "final_array = np.load(final_file)\n",
    "print(f\"Final array shape: {final_array.shape}\")\n",
    "\n",
    "# ✅ combined 파일로 따로 저장 (만약 필요할 때 대비)\n",
    "np.save(combined_save_path, final_array)\n",
    "print(f\"Combined array saved to: {combined_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2440aa9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:06.818134Z",
     "iopub.status.busy": "2025-05-03T08:21:06.817252Z",
     "iopub.status.idle": "2025-05-03T08:21:06.983839Z",
     "shell.execute_reply": "2025-05-03T08:21:06.982817Z"
    },
    "papermill": {
     "duration": 0.256255,
     "end_time": "2025-05-03T08:21:06.985531",
     "exception": false,
     "start_time": "2025-05-03T08:21:06.729276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 12, 2049)\n"
     ]
    }
   ],
   "source": [
    "# (1) combined feature 불러오기\n",
    "import numpy as np\n",
    "all_sequences = np.load('/kaggle/working/all_sequences_combined.npy')\n",
    "print(all_sequences.shape)  # → (n_videos, 12, 2049) 같은 출력 확인\n",
    "\n",
    "# (2) train.csv 다시 로드\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('/kaggle/input/nexar-collision-prediction/train.csv')\n",
    "\n",
    "# (3) label 추출\n",
    "labels = train_df['target'].values  # shape (n_videos,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6fdf8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:07.155779Z",
     "iopub.status.busy": "2025-05-03T08:21:07.154979Z",
     "iopub.status.idle": "2025-05-03T08:21:07.596435Z",
     "shell.execute_reply": "2025-05-03T08:21:07.594902Z"
    },
    "papermill": {
     "duration": 0.52934,
     "end_time": "2025-05-03T08:21:07.598262",
     "exception": false,
     "start_time": "2025-05-03T08:21:07.068922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all_sequences shape: (1500, 12, 2049)\n",
      "Labels shape: (1500,)\n",
      "Dataset length: 1500\n",
      "Train size: 1200, Val size: 300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# 1️⃣ 저장된 feature 불러오기\n",
    "all_sequences = np.load('all_sequences_combined.npy')\n",
    "print(f\"Loaded all_sequences shape: {all_sequences.shape}\")  # (1500, 12, 2049)\n",
    "\n",
    "# 2️⃣ train_df에서 label 불러오기 (주의: feature 개수에 맞게 잘라주기!)\n",
    "labels = train_df['target'].values[:all_sequences.shape[0]]  # shape (1500,)\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# 3️⃣ Dataset 클래스 정의\n",
    "class VideoSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences (numpy.ndarray): shape (n_samples, T, feature_dim)\n",
    "            labels (numpy.ndarray or list): shape (n_samples,)\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.sequences[idx]  # (T, feature_dim)\n",
    "        y = self.labels[idx]     # scalar or class\n",
    "        return x, y\n",
    "\n",
    "# 4️⃣ Dataset 객체 생성\n",
    "# Dataset 객체 생성\n",
    "dataset = VideoSequenceDataset(all_sequences, labels)\n",
    "\n",
    "# 정확한 길이 체크\n",
    "print(f\"Dataset length: {len(dataset)}\")  # 꼭 찍어봐!\n",
    "\n",
    "# Train/Val split\n",
    "train_size = int(0.8 * len(dataset))  # 80% split → 1200 if 1500 total\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "print(f\"Train size: {train_size}, Val size: {val_size}\")\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb987d",
   "metadata": {
    "papermill": {
     "duration": 0.085668,
     "end_time": "2025-05-03T08:21:07.772836",
     "exception": false,
     "start_time": "2025-05-03T08:21:07.687168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ce8859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:07.937036Z",
     "iopub.status.busy": "2025-05-03T08:21:07.936680Z",
     "iopub.status.idle": "2025-05-03T08:21:07.940984Z",
     "shell.execute_reply": "2025-05-03T08:21:07.940163Z"
    },
    "papermill": {
     "duration": 0.086936,
     "end_time": "2025-05-03T08:21:07.942419",
     "exception": false,
     "start_time": "2025-05-03T08:21:07.855483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # DataLoader 생성\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# dataset = VideoSequenceDataset(all_sequences, labels)\n",
    "\n",
    "# # Train/Val 분할\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "990bfcd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.117158Z",
     "iopub.status.busy": "2025-05-03T08:21:08.116354Z",
     "iopub.status.idle": "2025-05-03T08:21:08.125320Z",
     "shell.execute_reply": "2025-05-03T08:21:08.124463Z"
    },
    "papermill": {
     "duration": 0.097303,
     "end_time": "2025-05-03T08:21:08.126840",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.029537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Temporal Transformer 모델 설계\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=1281, embed_dim=256, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TemporalTransformerModel, self).__init__()\n",
    "\n",
    "        # 1. Input → embedding layer\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "        # 2. Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 3. Classification head (binary classification)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Binary output (0~1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, T, input_dim)\n",
    "        \"\"\"\n",
    "        # Step 1: Project input features\n",
    "        x = self.input_proj(x)  # → (batch_size, T, embed_dim)\n",
    "\n",
    "        # Step 2: Apply Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # → (batch_size, T, embed_dim)\n",
    "\n",
    "        # Step 3: Aggregate (mean pooling over time)\n",
    "        x = x.mean(dim=1)  # → (batch_size, embed_dim)\n",
    "\n",
    "        # Step 4: Final classification\n",
    "        out = self.classifier(x)  # → (batch_size, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6007fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.308212Z",
     "iopub.status.busy": "2025-05-03T08:21:08.307810Z",
     "iopub.status.idle": "2025-05-03T08:21:08.315636Z",
     "shell.execute_reply": "2025-05-03T08:21:08.314725Z"
    },
    "papermill": {
     "duration": 0.097699,
     "end_time": "2025-05-03T08:21:08.317255",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.219556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spatial Transformer 모델 설계\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=1280, hidden_dim=512, num_heads=8, num_layers=2, dropout=0.1):\n",
    "        super(SpatialTransformer, self).__init__()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 마지막 summary를 위한 pooling 또는 projection\n",
    "        self.output_layer = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (batch_size, T, input_dim) → per-frame spatial features\n",
    "        \n",
    "        Returns:\n",
    "            out: shape (batch_size, input_dim) → aggregated spatial feature\n",
    "        \"\"\"\n",
    "        # transformer expects (batch_size, T, input_dim)\n",
    "        x_transformed = self.transformer(x)  # (batch_size, T, input_dim)\n",
    "\n",
    "        # Pooling over time (mean pooling)\n",
    "        x_pooled = x_transformed.mean(dim=1)  # (batch_size, input_dim)\n",
    "\n",
    "        out = self.output_layer(x_pooled)  # (batch_size, input_dim)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a39e628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.493736Z",
     "iopub.status.busy": "2025-05-03T08:21:08.493396Z",
     "iopub.status.idle": "2025-05-03T08:21:08.501292Z",
     "shell.execute_reply": "2025-05-03T08:21:08.500334Z"
    },
    "papermill": {
     "duration": 0.096939,
     "end_time": "2025-05-03T08:21:08.502792",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.405853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CollisionPredictionModel(nn.Module):\n",
    "    def __init__(self, temporal_input_dim=2049, spatial_input_dim=1280, embed_dim=256, dropout=0.1):\n",
    "        super(CollisionPredictionModel, self).__init__()\n",
    "\n",
    "        self.temporal_transformer = TemporalTransformerModel(\n",
    "            input_dim=temporal_input_dim, embed_dim=embed_dim\n",
    "        )\n",
    "        self.spatial_transformer = SpatialTransformer(\n",
    "            input_dim=spatial_input_dim\n",
    "        )\n",
    "\n",
    "        fused_dim = embed_dim + spatial_input_dim  # Temporal + Spatial 출력 연결\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, temporal_input, spatial_input):\n",
    "        \"\"\"\n",
    "        temporal_input: (batch, T, 2049)\n",
    "        spatial_input: (batch, T, 1280)\n",
    "        \"\"\"\n",
    "        temporal_out = self.temporal_transformer(temporal_input)  # (batch, embed_dim)\n",
    "        spatial_out = self.spatial_transformer(spatial_input)      # (batch, spatial_input_dim)\n",
    "\n",
    "        # Fusion: concatenate\n",
    "        fused = torch.cat([temporal_out, spatial_out], dim=1)  # (batch, fused_dim)\n",
    "\n",
    "        out = self.classifier(fused)  # (batch, 1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbc94d60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.674569Z",
     "iopub.status.busy": "2025-05-03T08:21:08.674267Z",
     "iopub.status.idle": "2025-05-03T08:21:08.687000Z",
     "shell.execute_reply": "2025-05-03T08:21:08.686192Z"
    },
    "papermill": {
     "duration": 0.100024,
     "end_time": "2025-05-03T08:21:08.689223",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.589199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=256, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TemporalTransformerModel, self).__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)            # (batch, T, embed_dim)\n",
    "        x = self.transformer_encoder(x)   # (batch, T, embed_dim)\n",
    "        x = x.mean(dim=1)                 # (batch, embed_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=1280, hidden_dim=512, num_heads=8, num_layers=2, dropout=0.1):\n",
    "        super(SpatialTransformer, self).__init__()\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_transformed = self.transformer(x)  # (batch, T, input_dim)\n",
    "        x_pooled = x_transformed.mean(dim=1) # (batch, input_dim)\n",
    "        out = self.output_layer(x_pooled)    # (batch, input_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, temporal_input_dim=2049, spatial_input_dim=1280,\n",
    "                 temporal_embed_dim=256, combined_dim=256, dropout=0.1):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        self.temporal_transformer = TemporalTransformerModel(\n",
    "            input_dim=temporal_input_dim, embed_dim=temporal_embed_dim\n",
    "        )\n",
    "        self.spatial_transformer = SpatialTransformer(\n",
    "            input_dim=spatial_input_dim\n",
    "        )\n",
    "\n",
    "        # temporal (256) + spatial (1280) = 1536\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1536, combined_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(combined_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, temporal_input, spatial_input):\n",
    "        temporal_out = self.temporal_transformer(temporal_input)  # (batch, 256)\n",
    "        spatial_out = self.spatial_transformer(spatial_input)     # (batch, 1280)\n",
    "\n",
    "        combined = torch.cat([temporal_out, spatial_out], dim=1) # (batch, 1536)\n",
    "        out = self.classifier(combined)                          # (batch, 1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01eddef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:21:08.859138Z",
     "iopub.status.busy": "2025-05-03T08:21:08.858804Z",
     "iopub.status.idle": "2025-05-03T08:28:41.904331Z",
     "shell.execute_reply": "2025-05-03T08:28:41.903175Z"
    },
    "papermill": {
     "duration": 453.13316,
     "end_time": "2025-05-03T08:28:41.906003",
     "exception": false,
     "start_time": "2025-05-03T08:21:08.772843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6834\n",
      "Validation Accuracy: 0.5667\n",
      "Epoch [2/20], Loss: 0.6250\n",
      "Validation Accuracy: 0.6667\n",
      "Epoch [3/20], Loss: 0.5442\n",
      "Validation Accuracy: 0.6433\n",
      "Epoch [4/20], Loss: 0.4935\n",
      "Validation Accuracy: 0.6467\n",
      "Epoch [5/20], Loss: 0.3240\n",
      "Validation Accuracy: 0.6667\n",
      "Epoch [6/20], Loss: 0.2347\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch [7/20], Loss: 0.1393\n",
      "Validation Accuracy: 0.6067\n",
      "Epoch [8/20], Loss: 0.0583\n",
      "Validation Accuracy: 0.5967\n",
      "Epoch [9/20], Loss: 0.0777\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch [10/20], Loss: 0.0611\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch [11/20], Loss: 0.0505\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch [12/20], Loss: 0.0149\n",
      "Validation Accuracy: 0.6633\n",
      "Epoch [13/20], Loss: 0.0724\n",
      "Validation Accuracy: 0.6033\n",
      "Epoch [14/20], Loss: 0.0141\n",
      "Validation Accuracy: 0.6533\n",
      "Epoch [15/20], Loss: 0.0007\n",
      "Validation Accuracy: 0.6600\n",
      "Epoch [16/20], Loss: 0.0003\n",
      "Validation Accuracy: 0.6467\n",
      "Epoch [17/20], Loss: 0.0001\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch [18/20], Loss: 0.0001\n",
      "Validation Accuracy: 0.6467\n",
      "Epoch [19/20], Loss: 0.0001\n",
      "Validation Accuracy: 0.6433\n",
      "Epoch [20/20], Loss: 0.0001\n",
      "Validation Accuracy: 0.6500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 모델 준비\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CombinedModel(\n",
    "    temporal_input_dim=2049, spatial_input_dim=1280,\n",
    "    temporal_embed_dim=256, combined_dim=256\n",
    ").to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)  # (batch, T, 2049)\n",
    "        labels = labels.to(device).unsqueeze(1)  # (batch, 1)\n",
    "\n",
    "        # temporal_input = spatial(1280) + flow(1) + 추가 optical flow들 → (2049)\n",
    "        temporal_input = inputs[:, :, :2049]\n",
    "\n",
    "        # spatial_input = spatial part only → (1280)\n",
    "        spatial_input = inputs[:, :, :1280]\n",
    "\n",
    "        # forward\n",
    "        outputs = model(temporal_input, spatial_input)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "    \n",
    "            temporal_input = inputs[:, :, :2049]\n",
    "            spatial_input = inputs[:, :, :1280]\n",
    "    \n",
    "            outputs = model(temporal_input, spatial_input)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "    \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92f0784a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:28:42.083745Z",
     "iopub.status.busy": "2025-05-03T08:28:42.083330Z",
     "iopub.status.idle": "2025-05-03T08:28:42.341344Z",
     "shell.execute_reply": "2025-05-03T08:28:42.340025Z"
    },
    "papermill": {
     "duration": 0.347859,
     "end_time": "2025-05-03T08:28:42.343129",
     "exception": false,
     "start_time": "2025-05-03T08:28:41.995270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델 저장 \n",
    "torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2dda785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:28:42.527951Z",
     "iopub.status.busy": "2025-05-03T08:28:42.527576Z",
     "iopub.status.idle": "2025-05-03T08:28:42.955955Z",
     "shell.execute_reply": "2025-05-03T08:28:42.954877Z"
    },
    "papermill": {
     "duration": 0.519809,
     "end_time": "2025-05-03T08:28:42.957970",
     "exception": false,
     "start_time": "2025-05-03T08:28:42.438161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (temporal_transformer): TemporalTransformerModel(\n",
       "    (input_proj): Linear(in_features=2049, out_features=256, bias=True)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (spatial_transformer): SpatialTransformer(\n",
       "    (transformer): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=1280, bias=True)\n",
       "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 같은 모델 아키텍처 준비 (모델 불러오기)\n",
    "model = CombinedModel(\n",
    "    temporal_input_dim=2049,  # 주의: 학습할 때와 동일해야 한다\n",
    "    spatial_input_dim=1280,\n",
    "    temporal_embed_dim=256,\n",
    "    combined_dim=256\n",
    ")\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12dbab2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T08:28:43.241271Z",
     "iopub.status.busy": "2025-05-03T08:28:43.240902Z",
     "iopub.status.idle": "2025-05-03T10:08:12.586986Z",
     "shell.execute_reply": "2025-05-03T10:08:12.585934Z"
    },
    "papermill": {
     "duration": 5969.59319,
     "end_time": "2025-05-03T10:08:12.744555",
     "exception": false,
     "start_time": "2025-05-03T08:28:43.151365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Videos: 100%|██████████| 1344/1344 [1:39:29<00:00,  4.44s/it]\n"
     ]
    }
   ],
   "source": [
    "test_sequences = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Test Videos\"):\n",
    "    video_path = f\"{test_video_dir}/{int(float(row['id'])):05d}.mp4\"\n",
    "    sequence = prepare_transformer_input(video_path, num_frames=12)\n",
    "    if sequence is not None:\n",
    "        test_sequences.append(sequence)\n",
    "\n",
    "test_sequences = np.array(test_sequences)  # shape: (n_test, 12, 2049)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edf2d266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:08:13.070524Z",
     "iopub.status.busy": "2025-05-03T10:08:13.069968Z",
     "iopub.status.idle": "2025-05-03T10:08:13.245406Z",
     "shell.execute_reply": "2025-05-03T10:08:13.244462Z"
    },
    "papermill": {
     "duration": 0.342694,
     "end_time": "2025-05-03T10:08:13.247182",
     "exception": false,
     "start_time": "2025-05-03T10:08:12.904488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 테스트용 Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 테스트용: dummy labels (예측용이라 실제 label은 필요 없음)\n",
    "dummy_labels = np.zeros(len(test_sequences))\n",
    "\n",
    "# Dataset\n",
    "test_dataset = VideoSequenceDataset(test_sequences, dummy_labels)\n",
    "\n",
    "# DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de3fffcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:08:13.550312Z",
     "iopub.status.busy": "2025-05-03T10:08:13.549922Z",
     "iopub.status.idle": "2025-05-03T10:08:20.363539Z",
     "shell.execute_reply": "2025-05-03T10:08:20.362448Z"
    },
    "papermill": {
     "duration": 6.964494,
     "end_time": "2025-05-03T10:08:20.365049",
     "exception": false,
     "start_time": "2025-05-03T10:08:13.400555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (1344,)\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 예측 실행\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        temporal_input = inputs[:, :, :2049]\n",
    "        spatial_input = inputs[:, :, :1280]\n",
    "        outputs = model(temporal_input, spatial_input)\n",
    "        all_predictions.extend(outputs.cpu().numpy().flatten())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "print(f\"Predictions shape: {all_predictions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6991a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T10:08:20.681180Z",
     "iopub.status.busy": "2025-05-03T10:08:20.680709Z",
     "iopub.status.idle": "2025-05-03T10:08:20.731369Z",
     "shell.execute_reply": "2025-05-03T10:08:20.729985Z"
    },
    "papermill": {
     "duration": 0.212031,
     "end_time": "2025-05-03T10:08:20.733175",
     "exception": false,
     "start_time": "2025-05-03T10:08:20.521144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv!\n",
      "                id         score\n",
      "count  1344.000000  1.344000e+03\n",
      "mean   1906.876488  4.461332e-01\n",
      "std     847.105655  4.724515e-01\n",
      "min       1.000000  4.918239e-07\n",
      "25%    1206.750000  7.033291e-05\n",
      "50%    2243.500000  1.134222e-01\n",
      "75%    2579.250000  9.994114e-01\n",
      "max    2915.000000  9.999987e-01\n"
     ]
    }
   ],
   "source": [
    "# Kaggle 제출용 CSV\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'score': all_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Saved submission.csv!\")\n",
    "\n",
    "# 요약 확인\n",
    "print(submission.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea997f",
   "metadata": {
    "papermill": {
     "duration": 0.270133,
     "end_time": "2025-05-03T10:08:21.164651",
     "exception": false,
     "start_time": "2025-05-03T10:08:20.894518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11038207,
     "sourceId": 92399,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13668.534485,
   "end_time": "2025-05-03T10:08:24.105889",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-03T06:20:35.571404",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
