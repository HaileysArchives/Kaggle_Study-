{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92399,"databundleVersionId":11038207,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **ğŸ’¡Reference Notebook - Fernandosr85 - Dashcam Collision Prediction Project ğŸš—**\n#### **https://www.kaggle.com/code/fernandosr85/dashcam-collision-prediction-project**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nmax_files = 10 \n\ncount = 0\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        count += 1\n        if count >= max_files:\n            break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:03.558840Z","iopub.execute_input":"2025-04-26T23:59:03.559274Z","iopub.status.idle":"2025-04-26T23:59:06.689471Z","shell.execute_reply.started":"2025-04-26T23:59:03.559243Z","shell.execute_reply":"2025-04-26T23:59:06.688442Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nexar-collision-prediction/sample_submission.csv\n/kaggle/input/nexar-collision-prediction/train.csv\n/kaggle/input/nexar-collision-prediction/test.csv\n/kaggle/input/nexar-collision-prediction/test/02772.mp4\n/kaggle/input/nexar-collision-prediction/test/02807.mp4\n/kaggle/input/nexar-collision-prediction/test/02509.mp4\n/kaggle/input/nexar-collision-prediction/test/00350.mp4\n/kaggle/input/nexar-collision-prediction/test/02163.mp4\n/kaggle/input/nexar-collision-prediction/test/02707.mp4\n/kaggle/input/nexar-collision-prediction/test/02741.mp4\n/kaggle/input/nexar-collision-prediction/train/02059.mp4\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport warnings\nfrom multiprocessing import Pool\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.models\n\nwarnings.filterwarnings(\"ignore\")\n\n# Check GPU availability and set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.691203Z","iopub.execute_input":"2025-04-26T23:59:06.692290Z","iopub.status.idle":"2025-04-26T23:59:06.700452Z","shell.execute_reply.started":"2025-04-26T23:59:06.692262Z","shell.execute_reply":"2025-04-26T23:59:06.699388Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Suppress unnecessary formatting warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Paths to the CSV files\ntrain_csv_path = '/kaggle/input/nexar-collision-prediction/train.csv'\ntest_csv_path = '/kaggle/input/nexar-collision-prediction/test.csv'\nsubmission_csv_path = '/kaggle/input/nexar-collision-prediction/sample_submission.csv'\n\n# Paths to the video directories\ntrain_video_dir = '/kaggle/input/nexar-collision-prediction/train'\ntest_video_dir = '/kaggle/input/nexar-collision-prediction/test'\n\n# Load the CSV files\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\nsubmission_df = pd.read_csv(submission_csv_path)\n\n# (ì¶”ê°€) id ì»¬ëŸ¼ì„ ë¬¸ìì—´(str)ë¡œ ë³€í™˜í•´ì„œ .0 ë¬¸ì œ ì—†ì• ê¸°\ntrain_df['id'] = train_df['id'].astype(str)\n\n# Display the first few rows of the DataFrames\nprint(\"Train.csv:\")\nprint(train_df.head())\n\nprint(\"\\nTest.csv:\")\nprint(test_df.head())\n\nprint(\"\\nSample Submission:\")\nprint(submission_df.head())\n\n# Optional: handle NaN values if needed, filling with zero or another value\ntrain_df['time_of_event'] = train_df['time_of_event'].fillna(0)\ntrain_df['time_of_alert'] = train_df['time_of_alert'].fillna(0)\n\n# (ì¶”ê°€) Check the video directory paths\nprint(\"\\nVideo Directory Paths:\")\nprint(f\"Train videos are located at: {train_video_dir}\")\nprint(f\"Test videos are located at: {test_video_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.701882Z","iopub.execute_input":"2025-04-26T23:59:06.702270Z","iopub.status.idle":"2025-04-26T23:59:06.750976Z","shell.execute_reply.started":"2025-04-26T23:59:06.702236Z","shell.execute_reply":"2025-04-26T23:59:06.749918Z"}},"outputs":[{"name":"stdout","text":"Train.csv:\n     id  time_of_event  time_of_alert  target\n0  1924            NaN            NaN       0\n1   822           19.5         18.633       1\n2  1429            NaN            NaN       0\n3   208           19.8         19.233       1\n4  1904            NaN            NaN       0\n\nTest.csv:\n    id\n0  204\n1   30\n2  146\n3   20\n4  511\n\nSample Submission:\n    id  target\n0  204       0\n1   30       0\n2  146       0\n3   20       0\n4  511       0\n\nVideo Directory Paths:\nTrain videos are located at: /kaggle/input/nexar-collision-prediction/train\nTest videos are located at: /kaggle/input/nexar-collision-prediction/test\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## **Data Preprocessing and Feature Extraction** ","metadata":{}},{"cell_type":"code","source":"# ì¼ë°˜ì ìœ¼ë¡œ ì¶©ëŒì´ ë°œìƒí•˜ëŠ” ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì´ˆì ì„ ë§ì¶° ë¹„ë””ì˜¤ì—ì„œ ì£¼ìš” í”„ë ˆì„ì„ ì¶”ì¶œ\n# ì§€ìˆ˜ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ì— ê°€ê¹Œìš´ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬\n\ndef extract_keyframes(video_path, num_frames=12, target_size=(160, 160)):\n    \"\"\"\n    Extracts key frames from the video, focusing on the final part where collisions typically occur.\n    Uses exponential distribution to give more weight to frames closer to the end.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path) # ë™ì˜ìƒì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ OpenCVì˜ videoCapture ê°ì²´ ìƒì„± \n\n    # íŒŒì¼ì´ ì œëŒ€ë¡œ ì—´ë¦¬ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬\n    if not cap.isOpened():\n        print(f\"Could not open the video: {video_path}\")\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n\n    # ì´ í”„ë ˆì„ ìˆ˜ì™€ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜(FPS)ë¥¼ ê°€ì ¸ì˜¤ê¸° \n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    if total_frames <= 0:\n        print(f\"Video without frames: {video_path}\")\n        cap.release()\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n    \n    # ì˜ìƒ ê¸¸ì´(ì´ˆ ë‹¨ìœ„) ê³„ì‚°\n    duration = total_frames / fps if fps > 0 else 0\n    \n    # ì§§ì€ ì˜ìƒ (10ì´ˆ ë¯¸ë§Œ): ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ\n    if duration < 10:\n        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n\n    # ê¸´ ì˜ìƒ (10ì´ˆ ì´ìƒ): í›„ë°˜ë¶€ì— ë” ì§‘ì¤‘í•´ì„œ ì¶”ì¶œ\n    else:\n        # ë§ˆì§€ë§‰ 3ì´ˆ ë™ì•ˆ í”„ë ˆì„ì˜ 80% ì§‘ì¤‘(ì¤‘ìš” ì˜ì—­)\n        end_frames = int(num_frames * 0.8)\n        start_frames = num_frames - end_frames\n        \n        # ì§€ë‚œ 3ì´ˆ ë™ì•ˆì˜ ì‹œì‘ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°\n        last_seconds = 3\n        last_frame_count = min(int(fps * last_seconds), total_frames - 1)\n        start_idx = max(0, total_frames - last_frame_count)\n        \n        # ë§ˆì§€ë§‰ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì§€ìˆ˜ ë¶„í¬\n        # ì´ë ‡ê²Œ í•˜ë©´ ë§ˆì§€ë§‰ì— ë” ë°€ì§‘ëœ ì¸ë±ìŠ¤ê°€ ìƒì„±ëœë‹¤ (\"í”„ë ˆì„ì„ ë½‘ëŠ” ê°„ê²©\"ìì²´ë¥¼ ì¡°ì ˆ â†’ ëë¶€ë¶„ì— ë” ë§ì´ ëª°ë¦¬ê²Œ ë§Œë“œëŠ” ë°©ì‹)\n        end_indices = np.array([\n            start_idx + int((total_frames - start_idx - 1) * (i/end_frames)**2) \n            for i in range(1, end_frames + 1)\n        ])\n        \n        # contextì— ë§ê²Œ ê· ì¼í•˜ê²Œ ë°°í¬ëœ ì´ˆê¸° í”„ë ˆì„ (ì´ˆë°˜ë¶€ì—ì„œ ê· ë“±í•˜ê²Œ ì¶”ì¶œí•œ í”„ë ˆì„ë“¤)\n        # contextë€? ì‚¬ê³  ì§ì „ì— ì–´ë–¤ ìƒí™©ì´ í¼ì³ì¡ŒëŠ”ì§€ì— ëŒ€í•œ íë¦„, ë°°ê²½, ë§¥ë½ \n        begin_indices = np.linspace(0, start_idx - 1, start_frames, dtype=int) if start_idx > 0 else np.zeros(start_frames, dtype=int)\n        \n        # ì¸ë±ìŠ¤ ê²°í•©\n        frame_indices = np.concatenate([begin_indices, end_indices])\n    \n    # ì„ íƒí•œ í”„ë ˆì„ ì¶”ì¶œ \n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            # Use higher resolution and better interpolation\n            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        else:\n            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n    \n    cap.release()\n    return np.array(frames, dtype=np.uint8)\n\n# ë¨¼ì €, ì „ì—­ ë²”ìœ„ì—ì„œ ë³€í™˜ í´ë˜ìŠ¤ë¥¼ ì •ì˜ \n# ì…ë ¥ëœ ì˜ìƒ í”„ë ˆì„ì„ ì¼ì • í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ì‹œì¼œì„œ, ë°ì´í„° ë‹¤ì–‘ì„±ì„ ëŠ˜ë¦¬ëŠ” ì—­í• \nclass RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n        \n    def __call__(self, frames):\n        if np.random.random() < self.p:\n            return frames[:, :, ::-1, :].copy()  # horizontally flip each frame\n        return frames\n\n# ì˜ìƒ í”„ë ˆì„ì˜ ë°ê¸°ì™€ ëŒ€ë¹„ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°ì •í•´, ë‹¤ì–‘í•œ ì¡°ëª… í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ì¦ê°• í´ë˜ìŠ¤\nclass ColorJitter(object):\n    def __init__(self, brightness=0, contrast=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        \n    def __call__(self, frames):\n        # Apply brightness jitter\n        if self.brightness > 0:\n            brightness_factor = np.random.uniform(max(0, 1-self.brightness), 1+self.brightness)\n            frames = frames * brightness_factor\n            frames = np.clip(frames, 0, 255)\n        \n        # Apply contrast jitter\n        if self.contrast > 0:\n            contrast_factor = np.random.uniform(max(0, 1-self.contrast), 1+self.contrast)\n            frames = (frames - 128) * contrast_factor + 128\n            frames = np.clip(frames, 0, 255)\n            \n        return frames\n\n# í”„ë ˆì„ì— íë¦¿í•œ ì•ˆê°œ íš¨ê³¼ë¥¼ ë„£ì–´, ì‹œì•¼ê°€ ë‚˜ìœ ë‚ ì”¨ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\nclass AddFog(object):\n    def __call__(self, frames):\n        fog = np.random.uniform(0.7, 0.9, frames.shape).astype(np.float32)\n        return frames * 0.8 + fog * 50  # Adjusted for 0-255 scale\n\n# í”„ë ˆì„ì— í°ìƒ‰ ì„ í˜• ë…¸ì´ì¦ˆ(ë¹—ë°©ìš¸)ë¥¼ ì¶”ê°€í•´ ë¹„ ì˜¤ëŠ” ë‚ ì”¨ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\nclass AddRain(object):\n    def __call__(self, frames):\n        h, w = frames.shape[1:3]\n        rain = np.random.uniform(0, 1, (len(frames), h, w, 1)).astype(np.float32)\n        rain = (rain > 0.97).astype(np.float32) * 200  # White rain drops\n        return np.clip(frames * 0.9 + rain, 0, 255)  # Darken a bit and add drops\n\n# ì§€ì •ëœ í™•ë¥ ì— ë”°ë¼ ì–´ë–¤ ë³€í™˜ì„ ì ìš©í• ì§€ ë§ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê²°ì •í•˜ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ í´ë˜ìŠ¤(ëœë¤ì„± ë¶€ì—¬)\nclass RandomApply(object):\n    def __init__(self, transform, p=0.5):\n        self.transform = transform\n        self.p = p\n        \n    def __call__(self, frames):\n        if np.random.random() < self.p:\n            return self.transform(frames)\n        return frames\n\n# ì—¬ëŸ¬ ê°œì˜ ë³€í™˜(Flip, Jitter, Fog ë“±)ì„ ìˆœì„œëŒ€ë¡œ ì ìš©í•˜ëŠ” ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def __call__(self, frames):\n        for t in self.transforms:\n            frames = t(frames)\n        return frames\n\n# ì˜ìƒ í”„ë ˆì„ ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë°”ê¾¸ê³ , í”½ì…€ ê°’ì„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ëŠ” í´ë˜ìŠ¤\nclass ToTensor(object):\n    def __call__(self, frames):\n        # Convert from (T, H, W, C) to (T, C, H, W)\n        frames = frames.transpose(0, 3, 1, 2)\n        # Convert to tensor and normalize to [0, 1]\n        return torch.from_numpy(frames).float() / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.753189Z","iopub.execute_input":"2025-04-26T23:59:06.753507Z","iopub.status.idle":"2025-04-26T23:59:06.776208Z","shell.execute_reply.started":"2025-04-26T23:59:06.753484Z","shell.execute_reply":"2025-04-26T23:59:06.774882Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# ë™ì˜ìƒì—ì„œ ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ ë³€í™˜ì„ ë°˜í™˜\n\ndef get_video_transforms():\n    \"\"\"\n    Returns transformations for data augmentation in videos.\n    \"\"\"\n    return {\n        'train': Compose([\n            RandomHorizontalFlip(p=0.5),\n            ColorJitter(brightness=0.3, contrast=0.3),\n            RandomApply(AddFog(), p=0.15),\n            RandomApply(AddRain(), p=0.15),\n            RandomApply(RandomNoise(0.05), p=0.2), \n            RandomApply(RandomOcclusion(), p=0.1),\n            ToTensor()\n        ]),\n        'val': Compose([\n            ToTensor()  # Only tensor conversion for validation\n        ])\n    }\n\n# ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ë¬´ì‘ìœ„ ê°€ìš°ì‹œì•ˆ(ì •ê·œë¶„í¬) ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬, ì‹¤ì œ ì´¬ì˜ í™˜ê²½ì—ì„œ \n# ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¡ìŒì— ëŒ€í•´ ëª¨ë¸ì´ ë” ê°•ê±´í•´ì§€ë„ë¡ ë§Œë“œëŠ” í´ë˜ìŠ¤\nclass RandomNoise(object):\n    \"\"\"\n    Applies random Gaussian noise to video frames for data augmentation.\n    \n    This transformation helps the model become more robust to noise\n    that may be present in real-world video data.\n    \n    Args:\n        std (float): Standard deviation of the Gaussian noise as a fraction\n                     of the pixel value range (default: 0.05)\n    \"\"\"\n    def __init__(self, std=0.05):\n        self.std = std\n        \n    def __call__(self, frames):\n        \"\"\"\n        Apply random noise to the input frames.\n        \n        Args:\n            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n                                   where T is number of frames\n        \n        Returns:\n            numpy.ndarray: Noise-augmented frames, clipped to valid pixel range [0, 255]\n        \"\"\"\n        # ì§€ì •ëœ í‘œì¤€ í¸ì°¨ë¥¼ ê°€ì§„ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±\n        noise = np.random.normal(0, self.std * 255, frames.shape).astype(np.float32)\n        \n        # ìœ íš¨í•œ í”½ì…€ ë²”ìœ„ì— ë…¸ì´ì¦ˆ ë° í´ë¦½ ì¶”ê°€í•˜ê¸°\n        # ì˜ìƒì€ ì •ìˆ˜í˜• ë°ì´í„°ì—¬ì•¼ í•˜ë¯€ë¡œ í˜• ë³€í™˜ (astype)\n        return np.clip(frames + noise, 0, 255).astype(np.uint8)\n\n# ì˜ìƒ í”„ë ˆì„ì— ê²€ì€ìƒ‰ ì‚¬ê°í˜•ì„ ë¬´ì‘ìœ„ë¡œ ë®ì–´ ì”Œì›Œ, ì¼ë¶€ ì •ë³´ê°€ ê°€ë ¤ì¡Œì„ ë•Œë„ ëª¨ë¸ì´ ê²¬ë”œ ìˆ˜ ìˆë„ë¡ í›ˆë ¨ì‹œí‚¤ëŠ” í´ë˜ìŠ¤\nclass RandomOcclusion(object):\n    \"\"\"\n    Simulates occlusion in video frames by adding black rectangles.\n    \n    This transformation helps the model learn to handle partial occlusions\n    that may occur in real-world scenarios when objects block the camera view.\n    \"\"\"\n    def __call__(self, frames):\n        \"\"\"\n        Apply random occlusion to the input frames.\n        \n        Args:\n            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n                                   where T is number of frames\n        \n        Returns:\n            numpy.ndarray: Frames with random occlusion applied\n        \"\"\"\n        # í”„ë ˆì„ í•˜ë‚˜ì˜ ì„¸ë¡œ(h), ê°€ë¡œ(w) ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°\n        h, w = frames.shape[1:3]\n        \n        # ì „ì²´ í”„ë ˆì„ í¬ê¸°ì˜ 10%~25% ì‚¬ì´ í¬ê¸°ì˜ ê°€ë¦¼ ì˜ì—­ í¬ê¸° ì„¤ì •\n        occl_h = np.random.randint(int(h * 0.1), int(h * 0.25))\n        occl_w = np.random.randint(int(w * 0.1), int(w * 0.25))\n        \n        # ì´ ê°€ë¦¼ ì˜ì—­ì´ ë“¤ì–´ê°ˆ ë¬´ì‘ìœ„ ìœ„ì¹˜ ì¢Œí‘œ ì„¤ì • \n        occl_x = np.random.randint(0, w - occl_w)\n        occl_y = np.random.randint(0, h - occl_h)\n        \n        # ì›ë³¸ í”„ë ˆì„ì„ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ ë³µì‚¬ë³¸ ë§Œë“¤ê¸°\n        frames_copy = frames.copy()\n        \n        # í”½ì…€ì„ 0(ê²€ì •ìƒ‰)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  í”„ë ˆì„ì— occlusion ì ìš©\n        for i in range(len(frames)):\n            frames_copy[i, occl_y:occl_y+occl_h, occl_x:occl_x+occl_w, :] = 0\n            \n        return frames_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.777877Z","iopub.execute_input":"2025-04-26T23:59:06.778273Z","iopub.status.idle":"2025-04-26T23:59:06.802257Z","shell.execute_reply.started":"2025-04-26T23:59:06.778238Z","shell.execute_reply":"2025-04-26T23:59:06.801126Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# ë¹„ë””ì˜¤ í”„ë ˆì„ ê°„ ì›€ì§ì„(ëª¨ì…˜)ì„ ì¶”ì í•˜ëŠ” 'optical_flow'ë¥¼ ê³„ì‚°í•´, ê°ì²´ë‚˜ ë°°ê²½ì˜ ì´ë™ ë°©í–¥ê³¼ ì†ë„ë¥¼ ë²¡í„° í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n# ë‘ ì—°ì†ëœ ì´ë¯¸ì§€(ë˜ëŠ” í”„ë ˆì„) ì‚¬ì´ì—ì„œ, ê° í”½ì…€ì´ ì–´ë–»ê²Œ ì´ë™í–ˆëŠ”ì§€ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ  -> optical_flow\n# Farneback ë°©ì‹ë§Œ ì‚¬ìš©\n# \"ëª¨ë“  í”½ì…€ì˜ ë°©í–¥ + ì†ë„ ì •ë³´ë¥¼ ë‹¤ ë‚¨ê¹€\"\n\ndef compute_optical_flow(frames, skip_frames=1):\n    \"\"\"Calculates optical flow skipping some frames to reduce processing.\"\"\"\n    if len(frames) < 2:\n        return np.zeros((1, frames.shape[1], frames.shape[2], 2), dtype=np.float32)\n    \n    flows = []\n\n    # ì²« í”„ë ˆì„ì„ ê·¸ë ˆì´ìŠ¤ì¼€ì¼(í‘ë°± ì´ë¯¸ì§€)ë¡œ ë³€í™˜\n    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n\n    # ì—°ì†ëœ í”„ë ˆì„ ìŒë§ˆë‹¤ optical_flow ê³„ì‚° \n    for i in range(1, len(frames), skip_frames):\n        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n        try:\n            # ë” ë¹ ë¥¸ ê³„ì‚°ì„ ìœ„í•´ ë§¤ê°œë³€ìˆ˜ ì¤„ì´ê¸°\n            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray,\n                                               None, 0.5, 3, 15, 3, 5, 1.2, 0)\n            flows.append(flow)\n        # ì˜ˆì™¸ ì²˜ë¦¬ (ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ 0ìœ¼ë¡œ ëŒ€ì²´)\n        except Exception as e:\n            print(f\"Error calculating optical flow: {str(e)}\")\n            flows.append(np.zeros((frames.shape[1], frames.shape[2], 2), dtype=np.float32))\n            \n        prev_gray = curr_gray\n    \n    if not flows:\n        return np.zeros((1, frames.shape[1], frames.shape[2], 2), dtype=np.float32)\n        \n    return np.array(flows, dtype=np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.803397Z","iopub.execute_input":"2025-04-26T23:59:06.803819Z","iopub.status.idle":"2025-04-26T23:59:06.829143Z","shell.execute_reply.started":"2025-04-26T23:59:06.803794Z","shell.execute_reply":"2025-04-26T23:59:06.827896Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.830380Z","iopub.execute_input":"2025-04-26T23:59:06.831903Z","iopub.status.idle":"2025-04-26T23:59:06.855377Z","shell.execute_reply.started":"2025-04-26T23:59:06.831862Z","shell.execute_reply":"2025-04-26T23:59:06.854125Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Positiveì´ë©´ alert_event ì‚¬ì´ë§Œ ë½‘ê³ , Negativeì´ë©´ ë§ˆì§€ë§‰ 3ì´ˆ êµ¬ê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œ\n# ê·¸ë¦¬ê³  num_framesë§Œí¼ ê· ë“±í•˜ê²Œ ë½‘ê³  CNN + Optical Flow ë‘˜ ë‹¤ ê³„ì‚° ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.856164Z","iopub.execute_input":"2025-04-26T23:59:06.856497Z","iopub.status.idle":"2025-04-26T23:59:06.882057Z","shell.execute_reply.started":"2025-04-26T23:59:06.856474Z","shell.execute_reply":"2025-04-26T23:59:06.880806Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# InceptionV3 ëª¨ë¸ë¡œ íŠ¹ì„± ì¶”ì¶œ\nbase_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\ncnn_feature_dim = base_model.output_shape[-1]\n\ndef get_hybrid_features(video_path, alert_time=None, event_time=None, num_frames=8):\n    \"\"\"\n    Extract hybrid features (CNN features + Optical flow feature) from a video.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n\n    if not cap.isOpened():\n        print(f\"Could not open video: {video_path}\")\n        return np.zeros(1280 + 1)\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30\n    video_duration = total_frames / fps\n\n    # ì´ë²¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° (Positive)\n    if alert_time is not None and event_time is not None and event_time > alert_time:\n        # alert_time ~ event_time ì‚¬ì´ êµ¬ê°„ë§Œ ì¶”ì¶œ\n        start_frame = int(alert_time * fps)\n        end_frame = int(event_time * fps)\n        start_frame = max(0, min(start_frame, total_frames-1))\n        end_frame = max(0, min(end_frame, total_frames-1))\n\n    else:\n        # ì´ë²¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° (Negative)\n        # ë§ˆì§€ë§‰ 3ì´ˆ ê¸°ì¤€\n        last_seconds = 3\n        start_frame = max(0, total_frames - int(last_seconds * fps))\n        end_frame = total_frames - 1\n\n    if end_frame <= start_frame:\n        frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n    else:\n        frame_indices = np.linspace(start_frame, end_frame, num_frames, dtype=int)\n\n    # í”„ë ˆì„ ì½ê¸°\n    frames = []\n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (160, 160), interpolation=cv2.INTER_LANCZOS4)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n    cap.release()\n\n    frames = np.array(frames)\n\n    if len(frames) == 0:\n        return np.zeros(1280 + 1)\n\n    # CNN Features\n    spatial_features = base_model.predict(\n        preprocess_input(frames.astype('float32')),\n        batch_size=32,\n        verbose=0\n    )\n\n    # Optical Flow Feature\n    flow_feature_farneback = 0.0\n    if len(frames) > 1:\n        flow_farneback = compute_optical_flow(frames, skip_frames=1)\n        flow_magnitude = np.linalg.norm(flow_farneback, axis=-1)  # (T-1, H, W)\n        flow_feature_farneback = np.mean(flow_magnitude)\n\n    # Feature ê²°í•©\n    return np.concatenate([\n        np.mean(spatial_features, axis=0),  # (1280,)\n        [flow_feature_farneback]  # (1,)\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.883445Z","iopub.execute_input":"2025-04-26T23:59:06.883788Z","iopub.status.idle":"2025-04-26T23:59:09.209473Z","shell.execute_reply.started":"2025-04-26T23:59:06.883765Z","shell.execute_reply":"2025-04-26T23:59:09.208314Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def get_hybrid_features_from_frames(frames):\n    \"\"\"\n    Extract hybrid features (CNN features + Optical flow feature) from pre-extracted frames.\n    Input frames must be (T, 3, 160, 160) PyTorch tensor.\n    \"\"\"\n    if len(frames) == 0:\n        return np.zeros(1280 + 1)\n\n    # CNN Features\n    # frames: (T, 3, 160, 160) â†’ (T, 160, 160, 3)ë¡œ ë³€í™˜\n    frames_np = frames.permute(0, 2, 3, 1).numpy() * 255.0  # ë‹¤ì‹œ [0,255] ë²”ìœ„ë¡œ ë˜ëŒë¦¬ê¸°\n    frames_np = frames_np.astype(np.uint8)\n\n    spatial_features = base_model.predict(\n        preprocess_input(frames_np.astype('float32')),\n        batch_size=32,\n        verbose=0\n    )\n\n    # Optical Flow Feature\n    flow_feature_farneback = 0.0\n    if len(frames_np) > 1:\n        flow_farneback = compute_optical_flow(frames_np, skip_frames=1)\n        flow_magnitude = np.linalg.norm(flow_farneback, axis=-1)  # (T-1, H, W)\n        flow_feature_farneback = np.mean(flow_magnitude)\n\n    # Feature ê²°í•©\n    return np.concatenate([\n        np.mean(spatial_features, axis=0),  # (1280,)\n        [flow_feature_farneback]            # (1,)\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:09.212886Z","iopub.execute_input":"2025-04-26T23:59:09.213238Z","iopub.status.idle":"2025-04-26T23:59:09.221487Z","shell.execute_reply.started":"2025-04-26T23:59:09.213191Z","shell.execute_reply":"2025-04-26T23:59:09.220307Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"from tqdm import tqdm  # ê¼­ import ì¶”ê°€!\n\n# 1. transform ì¤€ë¹„\ntransforms = get_video_transforms()\ntrain_transform = transforms['train']\n\n# 2. featureë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\nall_features = []\n\n# 3. train_df ì „ì²´ ë°˜ë³µ (ì—¬ê¸° tqdm ì ìš©!)\nfor idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n    video_id = row['id']  # 'id' ì‚¬ìš©!\n    video_path = os.path.join(train_video_dir, f\"{int(video_id):05d}.mp4\")  # 5ìë¦¬ zero-padding\n\n    # ë¹„ë””ì˜¤ ì—´ê¸°\n    frames = extract_keyframes(video_path, num_frames=12, target_size=(160,160))\n\n    if frames.shape[0] == 0:\n        print(f\"Skipping video {video_id} (no frames)\")\n        continue\n\n    # transform ì ìš©\n    frames_augmented = train_transform(frames)\n\n    # feature ë½‘ê¸°\n    feature = get_hybrid_features_from_frames(frames_augmented)\n\n    all_features.append(feature)\n\n# 4. ê²°ê³¼ë¥¼ numpy arrayë¡œ ë³€í™˜\nall_features = np.array(all_features)\nprint(f\"\\nAll features shape: {all_features.shape}\")\n# ì˜ˆì‹œ ì¶œë ¥: (n_samples, 1281)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:23:09.195705Z","iopub.execute_input":"2025-04-27T00:23:09.196179Z","iopub.status.idle":"2025-04-27T02:35:43.350532Z","shell.execute_reply.started":"2025-04-27T00:23:09.196149Z","shell.execute_reply":"2025-04-27T02:35:43.347715Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [2:12:34<00:00,  5.30s/it] ","output_type":"stream"},{"name":"stdout","text":"\nAll features shape: (1500, 2049)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# # 1. transform ì¤€ë¹„\n# transforms = get_video_transforms()\n# train_transform = transforms['train']\n\n# # 2. feature ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„\n# all_features = []\n\n# # 3. train_df ì¤‘ 50ê°œë§Œ ì„ íƒí•´ì„œ ì§„í–‰\n# for idx, row in tqdm(train_df.iloc[:50].iterrows(), total=20):\n#     video_id = row['id']\n#     video_path = os.path.join(train_video_dir, f\"{video_id}.mp4\")\n    \n#     frames = extract_keyframes(video_path, num_frames=12, target_size=(160,160))\n    \n#     if frames.shape[0] == 0:\n#         print(f\"Skipping video {video_id} (no frames)\")\n#         continue\n    \n#     frames_augmented = train_transform(frames)\n#     feature = get_hybrid_features_from_frames(frames_augmented)\n    \n#     all_features.append(feature)\n\n# # 4. ê²°ê³¼ë¥¼ numpy arrayë¡œ ë³€í™˜\n# all_features = np.array(all_features)\n# print(f\"\\nExtracted {all_features.shape[0]} features, each of shape {all_features.shape[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:08:57.837478Z","iopub.execute_input":"2025-04-27T00:08:57.837816Z","iopub.status.idle":"2025-04-27T00:09:31.400115Z","shell.execute_reply.started":"2025-04-27T00:08:57.837792Z","shell.execute_reply":"2025-04-27T00:09:31.398908Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/50 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1924.mp4\n","output_type":"stream"},{"name":"stderr","text":"  2%|â–         | 1/50 [00:00<00:43,  1.13it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/822.mp4\n","output_type":"stream"},{"name":"stderr","text":"  4%|â–         | 2/50 [00:01<00:32,  1.46it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1429.mp4\n","output_type":"stream"},{"name":"stderr","text":"  6%|â–Œ         | 3/50 [00:02<00:35,  1.32it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/208.mp4\n","output_type":"stream"},{"name":"stderr","text":"  8%|â–Š         | 4/50 [00:02<00:31,  1.48it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1904.mp4\n","output_type":"stream"},{"name":"stderr","text":" 10%|â–ˆ         | 5/50 [00:03<00:28,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/72.mp4\n","output_type":"stream"},{"name":"stderr","text":" 12%|â–ˆâ–        | 6/50 [00:03<00:26,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1486.mp4\n","output_type":"stream"},{"name":"stderr","text":" 14%|â–ˆâ–        | 7/50 [00:04<00:26,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1171.mp4\n","output_type":"stream"},{"name":"stderr","text":" 16%|â–ˆâ–Œ        | 8/50 [00:05<00:25,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/128.mp4\n","output_type":"stream"},{"name":"stderr","text":" 18%|â–ˆâ–Š        | 9/50 [00:06<00:28,  1.44it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1518.mp4\n","output_type":"stream"},{"name":"stderr","text":" 20%|â–ˆâ–ˆ        | 10/50 [00:06<00:26,  1.53it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/205.mp4\n","output_type":"stream"},{"name":"stderr","text":" 22%|â–ˆâ–ˆâ–       | 11/50 [00:07<00:24,  1.59it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1889.mp4\n","output_type":"stream"},{"name":"stderr","text":" 24%|â–ˆâ–ˆâ–       | 12/50 [00:07<00:23,  1.60it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1286.mp4\n","output_type":"stream"},{"name":"stderr","text":" 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:08<00:22,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/408.mp4\n","output_type":"stream"},{"name":"stderr","text":" 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:08<00:21,  1.71it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/948.mp4\n","output_type":"stream"},{"name":"stderr","text":" 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:09<00:20,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/457.mp4\n","output_type":"stream"},{"name":"stderr","text":" 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:09<00:19,  1.77it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1267.mp4\n","output_type":"stream"},{"name":"stderr","text":" 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:10<00:21,  1.52it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1802.mp4\n","output_type":"stream"},{"name":"stderr","text":" 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:11<00:22,  1.41it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1755.mp4\n","output_type":"stream"},{"name":"stderr","text":" 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:12<00:20,  1.51it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/471.mp4\n","output_type":"stream"},{"name":"stderr","text":" 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:13<00:21,  1.40it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/2134.mp4\n","output_type":"stream"},{"name":"stderr","text":" 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:13<00:19,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/900.mp4\n","output_type":"stream"},{"name":"stderr","text":" 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:14<00:17,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/841.mp4\n","output_type":"stream"},{"name":"stderr","text":" 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:14<00:16,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/333.mp4\n","output_type":"stream"},{"name":"stderr","text":" 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:15<00:17,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/564.mp4\n","output_type":"stream"},{"name":"stderr","text":" 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:16<00:16,  1.54it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/245.mp4\n","output_type":"stream"},{"name":"stderr","text":" 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:16<00:16,  1.42it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/171.mp4\n","output_type":"stream"},{"name":"stderr","text":" 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:17<00:15,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1076.mp4\n","output_type":"stream"},{"name":"stderr","text":" 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:18<00:13,  1.60it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/795.mp4\n","output_type":"stream"},{"name":"stderr","text":" 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:18<00:12,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/372.mp4\n","output_type":"stream"},{"name":"stderr","text":" 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:19<00:13,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/938.mp4\n","output_type":"stream"},{"name":"stderr","text":" 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:20<00:13,  1.37it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1749.mp4\n","output_type":"stream"},{"name":"stderr","text":" 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:20<00:12,  1.45it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/884.mp4\n","output_type":"stream"},{"name":"stderr","text":" 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:21<00:11,  1.52it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/641.mp4\n","output_type":"stream"},{"name":"stderr","text":" 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:22<00:10,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/10.mp4\n","output_type":"stream"},{"name":"stderr","text":" 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:22<00:09,  1.63it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1816.mp4\n","output_type":"stream"},{"name":"stderr","text":" 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:23<00:09,  1.45it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1052.mp4\n","output_type":"stream"},{"name":"stderr","text":" 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [00:24<00:09,  1.33it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1205.mp4\n","output_type":"stream"},{"name":"stderr","text":" 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [00:25<00:09,  1.28it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/927.mp4\n","output_type":"stream"},{"name":"stderr","text":" 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [00:25<00:07,  1.40it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/378.mp4\n","output_type":"stream"},{"name":"stderr","text":" 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [00:26<00:06,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1579.mp4\n","output_type":"stream"},{"name":"stderr","text":" 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [00:27<00:06,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1973.mp4\n","output_type":"stream"},{"name":"stderr","text":" 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [00:28<00:06,  1.32it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/238.mp4\n","output_type":"stream"},{"name":"stderr","text":" 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [00:29<00:05,  1.25it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1050.mp4\n","output_type":"stream"},{"name":"stderr","text":" 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [00:29<00:04,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1204.mp4\n","output_type":"stream"},{"name":"stderr","text":" 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [00:30<00:03,  1.45it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/313.mp4\n","output_type":"stream"},{"name":"stderr","text":" 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [00:31<00:02,  1.35it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/876.mp4\n","output_type":"stream"},{"name":"stderr","text":" 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [00:31<00:02,  1.46it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/98.mp4\n","output_type":"stream"},{"name":"stderr","text":" 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [00:32<00:01,  1.55it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1505.mp4\n","output_type":"stream"},{"name":"stderr","text":" 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [00:32<00:00,  1.43it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1699.mp4\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:33<00:00,  1.49it/s]","output_type":"stream"},{"name":"stdout","text":"\nExtracted 50 features, each of shape 2049\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# ì™¸ë¶€ì—ì„œ object_detectorì™€ driver_attention_modelì„ ì¸ìë¡œ ì£¼ì…ë°›ë„ë¡ ì„¤ê³„\n\n# ë”ë¯¸ ê°ì²´ íƒì§€ê¸° í´ë˜ìŠ¤\nclass ObjectDetector:\n    def detect(self, frame):\n        h, w, _ = frame.shape\n        return [(w//3, h//3, 2*w//3, 2*h//3)]  # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°•ìŠ¤\n\n# ë”ë¯¸ ì‹œì„  ì˜ˆì¸¡ ëª¨ë¸ í´ë˜ìŠ¤\nclass DummyDriverAttention:\n    def predict(self, frame):\n        h, w, _ = frame.shape\n        attention_map = np.zeros((h, w), dtype=np.float32)\n        cx, cy = w // 2, h // 2\n        attention_map[cy-10:cy+10, cx-10:cx+10] = 1.0\n        return cv2.GaussianBlur(attention_map, (25, 25), 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.143440Z","iopub.status.idle":"2025-04-27T00:00:37.143917Z","shell.execute_reply.started":"2025-04-27T00:00:37.143648Z","shell.execute_reply":"2025-04-27T00:00:37.143671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nfrom torchvision import models, transforms\n\nclass AATDAPreprocessor:\n    def __init__(self, object_detector, driver_attention_model, vgg_model=None):\n        self.detector = object_detector  # Cascade R-CNN ë“± ì™¸ë¶€ ê°ì§€ê¸°\n        self.attention_model = driver_attention_model  # Gate-DAP ë“±\n        self.vgg = vgg_model or models.vgg16(pretrained=True).features.eval()\n        self.vgg_fc7 = torch.nn.Sequential(*list(models.vgg16(pretrained=True).classifier.children())[:6]).eval()\n        self.vgg_transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])\n\n    # ê°ì§€ëœ ê°ì²´ ì˜ì—­ì„ ì˜ë¼ì„œ VGG FC7 feature(4096-dim) ì¶”ì¶œ\n    def extract_object_features(self, frame, boxes):\n        features = []\n        for (x1, y1, x2, y2) in boxes:\n            crop = frame[y1:y2, x1:x2]\n            if crop.shape[0] == 0 or crop.shape[1] == 0:\n                features.append(torch.zeros(4096))\n                continue\n            input_tensor = self.vgg_transform(crop).unsqueeze(0)\n            with torch.no_grad():\n                feat = self.vgg(input_tensor)\n                feat = feat.view(feat.size(0), -1)\n                fc7 = self.vgg_fc7(feat).squeeze(0)\n            features.append(fc7)\n        return torch.stack(features)  # Shape: (N, 4096)\n\n    # attention mapê³¼ ê°ì²´ ì˜ì—­ì´ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ í‰ê· ê°’ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚° \n    def compute_attention_weights(self, attention_map, boxes):\n        weights = []\n        for (x1, y1, x2, y2) in boxes:\n            patch = attention_map[y1:y2, x1:x2]\n            weight = np.mean(patch) if patch.size > 0 else 0.0\n            weights.append(weight)\n        return torch.tensor(weights, dtype=torch.float32)  # Shape: (N,)\n\n    # í•œ í”„ë ˆì„ì— ëŒ€í•´ ì•„ë˜ì™€ ê°™ì´ ìˆœì„œëŒ€ë¡œ ì‘ë™\n    def process_frame(self, frame):\n        # 1. ê°ì²´ ê°ì§€\n        boxes = self.detector.detect(frame)  # returns List of [x1, y1, x2, y2]\n\n        # 2. VGG Feature ì¶”ì¶œ\n        object_feats = self.extract_object_features(frame, boxes)  # (N, 4096)\n\n        # 3. ì‹œì„  ë§µ ìƒì„±\n        attention_map = self.attention_model.predict(frame)  # (H, W), numpy float32\n\n        # 4. Attention Weight ê³„ì‚°\n        weights = self.compute_attention_weights(attention_map, boxes)  # (N,)\n\n        # 5. Feature ê°•í™”\n        weighted_feats = object_feats * weights.unsqueeze(1)  # (N, 4096)\n\n        return weighted_feats, weights, boxes  # Transformer ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.145890Z","iopub.status.idle":"2025-04-27T00:00:37.146287Z","shell.execute_reply.started":"2025-04-27T00:00:37.146097Z","shell.execute_reply":"2025-04-27T00:00:37.146112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ì‹¤í–‰ ì˜ˆì‹œ\nif __name__ == '__main__':\n    detector = ObjectDetector()\n    attention_model = DummyDriverAttention()\n    processor = AATDAPreprocessor(detector, attention_model)\n\n    # í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n    frame = cv2.imread(\"sample_frame.jpg\")  # ì‹¤ì œ ê²½ë¡œë¡œ ì‘ì„±í•˜ê¸°!!!\n    if frame is not None:\n        weighted_feats, weights, boxes = processor.process_frame(frame)\n        print(\"ê°ì§€ëœ ê°ì²´ ìˆ˜:\", len(boxes))\n        print(\"Feature shape:\", weighted_feats.shape)  # (N, 4096)\n    else:\n        print(\"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.147178Z","iopub.status.idle":"2025-04-27T00:00:37.147529Z","shell.execute_reply.started":"2025-04-27T00:00:37.147386Z","shell.execute_reply":"2025-04-27T00:00:37.147400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ë¹„ë””ì˜¤ í•˜ë‚˜ì— í¬í•¨ëœ ì—¬ëŸ¬ í”„ë ˆì„ì„ process_frame()ìœ¼ë¡œ ë°˜ë³µ ì²˜ë¦¬ \n# ê° í”„ë ˆì„ì˜ ê°ì²´ featureë¥¼ (T, N, 4096) êµ¬ì¡°ë¡œ íŒ¨ë”©í•˜ì—¬ Transformer ì…ë ¥ì— ë§ì¶¤\n\nclass AATDADataset(Dataset):\n    def __init__(self, video_data_dict, annotations, processor, num_frames=8):\n        self.video_data = video_data_dict  # {video_id: [frame1, frame2, ...]}\n        self.annotations = annotations  # {video_id: {label: ..., alert_time: ...}}\n        self.processor = processor\n        self.num_frames = num_frames\n        self.video_ids = list(video_data_dict.keys())\n\n    def __len__(self):\n        return len(self.video_ids)\n\n    def __getitem__(self, idx):\n        video_id = self.video_ids[idx]\n        frames = self.video_data[video_id]  # raw RGB frames\n\n        # ê° í”„ë ˆì„ì— ëŒ€í•´ Object Feature + Attention ì²˜ë¦¬\n        sequence_features = []\n        for frame in frames:\n            weighted_feats, _, _ = self.processor.process_frame(frame)\n            sequence_features.append(weighted_feats)\n\n        # ì‹œí€€ìŠ¤ë¥¼ (T, N, 4096) í˜•íƒœì˜ tensorë¡œ ì •ë¦¬ (padding ê°€ëŠ¥)\n        max_objects = max(f.shape[0] for f in sequence_features)\n        padded_seq = []\n        for f in sequence_features:\n            pad_size = max_objects - f.shape[0]\n            if pad_size > 0:\n                padded = torch.cat([f, torch.zeros(pad_size, 4096)], dim=0)\n            else:\n                padded = f\n            padded_seq.append(padded)\n\n        # ìµœì¢… ì‹œí€€ìŠ¤: (T, N, 4096)\n        input_tensor = torch.stack(padded_seq)\n\n        label = torch.tensor(self.annotations[video_id]['label']).float()\n        alert_time = torch.tensor(self.annotations[video_id].get('alert_time', 0)).float()\n\n        return {\n            'video_id': video_id,\n            'input': input_tensor,  # (T, N, 4096)\n            'label': label,\n            'alert_time': alert_time\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.148747Z","iopub.status.idle":"2025-04-27T00:00:37.149133Z","shell.execute_reply.started":"2025-04-27T00:00:37.148964Z","shell.execute_reply":"2025-04-27T00:00:37.148984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **1ë‹¨ê³„: ì¼ë°˜ ì „ì²˜ë¦¬** \n#### **(1) í”„ë ˆì„ ì¶”ì¶œ - extract_keyframes()** \n#### **(2) Optical Flow ê³„ì‚° - compute_optical_flow()** \n#### **(3) ê¸°ë³¸ ì¦ê°• - ColorJitter, AddRain, AddFog, ToTensor() ë“±**\n#### **(4) ë°ì´í„° êµ¬ì„± - DashcamDataset ë˜ëŠ” PreprocessDashcamDatasetìœ¼ë¡œ êµ¬ì„±**","metadata":{}},{"cell_type":"markdown","source":"### **2ë‹¨ê³„: AAT-DA ì „ìš© ì „ì²˜ë¦¬ (ê¸°ì¡´ ë°ì´í„°ì—ì„œ Transformerìš© êµ¬ì¡° ë³€í™˜)**\n##### AAT-DAëŠ” ë‹¨ìˆœí•œ ì˜ìƒ í”„ë ˆì„ì´ ì•„ë‹ˆë¼, \"ê°ì²´ ì¤‘ì‹¬ì˜ ì‹œê³µê°„ Attention ì…ë ¥ êµ¬ì¡°\"ë¥¼ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ê°€ í•„ìš”\n#### **(1) ê°ì²´ ê°ì§€ - í”„ë ˆì„ì—ì„œ ê°ì²´ ê°ì§€ (Cascade R-CNN ë“±)** \n#### **(2) ê°ì²´ íŠ¹ì§• ì¶”ì¶œ - ê°ì§€ëœ ë°•ìŠ¤ë§ˆë‹¤ VGG16 FC7 feature ì¶”ì¶œ (4096-dim)** \n#### **(3) ì‹œì„  ë§µ ì˜ˆì¸¡ - Gate-DAP ë“±ìœ¼ë¡œ driver attention heatmap ìƒì„±**\n#### **(4) ì£¼ì˜ ê°€ì¤‘ì¹˜ ê³„ì‚° - ì‹œì„  ë§µ + ê°ì²´ ìœ„ì¹˜ â†’ ê°ì²´ë³„ attention weight Î±áµ¢ ê³„ì‚°**\n#### **(5) Feature ê°€ì¤‘ - ê°ì²´ feature Î±áµ¢ â†’ ê°•ì¡°ëœ ê°ì²´ feature**\n#### **(6) ì‹œí€€ìŠ¤ êµ¬ì„± - ëª¨ë“  í”„ë ˆì„ì˜ ê²°ê³¼ë¥¼ (T, N, 4096) ì‹œí€€ìŠ¤ë¡œ íŒ¨ë”© ì •ë¦¬**","metadata":{}},{"cell_type":"code","source":"# AAT-DA ëª¨ë¸ êµ¬ì„±\n# ì…ë ¥: (B, T, N, 4096)\n# êµ¬ì„±: Spatial Transformer + Temporal Transformer + Classifier\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ê° í”„ë ˆì„ ë‚´ ê°ì²´ë“¤ ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ (Object Self-Attention)\nclass SpatialTransformer(nn.Module):\n    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n\n    def forward(self, x):  # x: (B, T, N, 4096)\n        B, T, N, C = x.shape\n        x = x.view(B * T, N, C)  # (B*T, N, C)\n        x = self.encoder(x)  # (B*T, N, C)\n        x = x.mean(dim=1)  # (B*T, C)\n        x = x.view(B, T, C)  # (B, T, C)\n        return x\n\n# ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ì˜ë¯¸ íë¦„ ëª¨ë¸ë§ (Temporal Attention)\nclass TemporalTransformer(nn.Module):\n    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n\n    def forward(self, x):  # x: (B, T, C)\n        x = x.transpose(0, 1)  # (T, B, C)\n        x = self.encoder(x)  # (T, B, C)\n        x = x.transpose(0, 1)  # (B, T, C)\n        return x\n\nclass AATDA(nn.Module):\n    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048, dropout=0.1):\n        super().__init__()\n        self.spatial_transformer = SpatialTransformer(d_model, nhead, dim_feedforward)\n        self.temporal_transformer = TemporalTransformer(d_model, nhead, dim_feedforward)\n        self.classifier = nn.Sequential(\n            nn.Linear(d_model, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, x):  # x: (B, T, N, 4096)\n        x = self.spatial_transformer(x)     # (B, T, 4096)\n        x = self.temporal_transformer(x)    # (B, T, 4096)\n        x = x.mean(dim=1)                   # (B, 4096) - í‰ê·  í’€ë§\n        out = self.classifier(x)            # (B, 1)\n        return out.squeeze(-1)              # (B,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.151308Z","iopub.status.idle":"2025-04-27T00:00:37.151697Z","shell.execute_reply.started":"2025-04-27T00:00:37.151536Z","shell.execute_reply":"2025-04-27T00:00:37.151551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}