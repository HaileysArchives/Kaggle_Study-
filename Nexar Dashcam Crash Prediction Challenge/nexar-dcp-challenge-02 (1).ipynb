{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92399,"databundleVersionId":11038207,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **💡Reference Notebook - Fernandosr85 - Dashcam Collision Prediction Project 🚗**\n#### **https://www.kaggle.com/code/fernandosr85/dashcam-collision-prediction-project**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nmax_files = 10 \n\ncount = 0\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        count += 1\n        if count >= max_files:\n            break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:03.558840Z","iopub.execute_input":"2025-04-26T23:59:03.559274Z","iopub.status.idle":"2025-04-26T23:59:06.689471Z","shell.execute_reply.started":"2025-04-26T23:59:03.559243Z","shell.execute_reply":"2025-04-26T23:59:06.688442Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nexar-collision-prediction/sample_submission.csv\n/kaggle/input/nexar-collision-prediction/train.csv\n/kaggle/input/nexar-collision-prediction/test.csv\n/kaggle/input/nexar-collision-prediction/test/02772.mp4\n/kaggle/input/nexar-collision-prediction/test/02807.mp4\n/kaggle/input/nexar-collision-prediction/test/02509.mp4\n/kaggle/input/nexar-collision-prediction/test/00350.mp4\n/kaggle/input/nexar-collision-prediction/test/02163.mp4\n/kaggle/input/nexar-collision-prediction/test/02707.mp4\n/kaggle/input/nexar-collision-prediction/test/02741.mp4\n/kaggle/input/nexar-collision-prediction/train/02059.mp4\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import os\nimport gc\nimport time\nimport warnings\nfrom multiprocessing import Pool\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.models\n\nwarnings.filterwarnings(\"ignore\")\n\n# Check GPU availability and set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.691203Z","iopub.execute_input":"2025-04-26T23:59:06.692290Z","iopub.status.idle":"2025-04-26T23:59:06.700452Z","shell.execute_reply.started":"2025-04-26T23:59:06.692262Z","shell.execute_reply":"2025-04-26T23:59:06.699388Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Suppress unnecessary formatting warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Paths to the CSV files\ntrain_csv_path = '/kaggle/input/nexar-collision-prediction/train.csv'\ntest_csv_path = '/kaggle/input/nexar-collision-prediction/test.csv'\nsubmission_csv_path = '/kaggle/input/nexar-collision-prediction/sample_submission.csv'\n\n# Paths to the video directories\ntrain_video_dir = '/kaggle/input/nexar-collision-prediction/train'\ntest_video_dir = '/kaggle/input/nexar-collision-prediction/test'\n\n# Load the CSV files\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\nsubmission_df = pd.read_csv(submission_csv_path)\n\n# (추가) id 컬럼을 문자열(str)로 변환해서 .0 문제 없애기\ntrain_df['id'] = train_df['id'].astype(str)\n\n# Display the first few rows of the DataFrames\nprint(\"Train.csv:\")\nprint(train_df.head())\n\nprint(\"\\nTest.csv:\")\nprint(test_df.head())\n\nprint(\"\\nSample Submission:\")\nprint(submission_df.head())\n\n# Optional: handle NaN values if needed, filling with zero or another value\ntrain_df['time_of_event'] = train_df['time_of_event'].fillna(0)\ntrain_df['time_of_alert'] = train_df['time_of_alert'].fillna(0)\n\n# (추가) Check the video directory paths\nprint(\"\\nVideo Directory Paths:\")\nprint(f\"Train videos are located at: {train_video_dir}\")\nprint(f\"Test videos are located at: {test_video_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.701882Z","iopub.execute_input":"2025-04-26T23:59:06.702270Z","iopub.status.idle":"2025-04-26T23:59:06.750976Z","shell.execute_reply.started":"2025-04-26T23:59:06.702236Z","shell.execute_reply":"2025-04-26T23:59:06.749918Z"}},"outputs":[{"name":"stdout","text":"Train.csv:\n     id  time_of_event  time_of_alert  target\n0  1924            NaN            NaN       0\n1   822           19.5         18.633       1\n2  1429            NaN            NaN       0\n3   208           19.8         19.233       1\n4  1904            NaN            NaN       0\n\nTest.csv:\n    id\n0  204\n1   30\n2  146\n3   20\n4  511\n\nSample Submission:\n    id  target\n0  204       0\n1   30       0\n2  146       0\n3   20       0\n4  511       0\n\nVideo Directory Paths:\nTrain videos are located at: /kaggle/input/nexar-collision-prediction/train\nTest videos are located at: /kaggle/input/nexar-collision-prediction/test\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## **Data Preprocessing and Feature Extraction** ","metadata":{}},{"cell_type":"code","source":"# 일반적으로 충돌이 발생하는 마지막 부분에 초점을 맞춰 비디오에서 주요 프레임을 추출\n# 지수 분포를 사용하여 마지막에 가까운 프레임에 더 많은 가중치를 부여\n\ndef extract_keyframes(video_path, num_frames=12, target_size=(160, 160)):\n    \"\"\"\n    Extracts key frames from the video, focusing on the final part where collisions typically occur.\n    Uses exponential distribution to give more weight to frames closer to the end.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path) # 동영상을 불러오기 위해 OpenCV의 videoCapture 객체 생성 \n\n    # 파일이 제대로 열리지 않았을 경우 대비한 예외 처리\n    if not cap.isOpened():\n        print(f\"Could not open the video: {video_path}\")\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n\n    # 총 프레임 수와 초당 프레임 수(FPS)를 가져오기 \n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    if total_frames <= 0:\n        print(f\"Video without frames: {video_path}\")\n        cap.release()\n        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n    \n    # 영상 길이(초 단위) 계산\n    duration = total_frames / fps if fps > 0 else 0\n    \n    # 짧은 영상 (10초 미만): 균등한 간격으로 프레임 추출\n    if duration < 10:\n        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n\n    # 긴 영상 (10초 이상): 후반부에 더 집중해서 추출\n    else:\n        # 마지막 3초 동안 프레임의 80% 집중(중요 영역)\n        end_frames = int(num_frames * 0.8)\n        start_frames = num_frames - end_frames\n        \n        # 지난 3초 동안의 시작 인덱스를 계산\n        last_seconds = 3\n        last_frame_count = min(int(fps * last_seconds), total_frames - 1)\n        start_idx = max(0, total_frames - last_frame_count)\n        \n        # 마지막 프레임에 더 많은 가중치를 부여하는 지수 분포\n        # 이렇게 하면 마지막에 더 밀집된 인덱스가 생성된다 (\"프레임을 뽑는 간격\"자체를 조절 → 끝부분에 더 많이 몰리게 만드는 방식)\n        end_indices = np.array([\n            start_idx + int((total_frames - start_idx - 1) * (i/end_frames)**2) \n            for i in range(1, end_frames + 1)\n        ])\n        \n        # context에 맞게 균일하게 배포된 초기 프레임 (초반부에서 균등하게 추출한 프레임들)\n        # context란? 사고 직전에 어떤 상황이 펼쳐졌는지에 대한 흐름, 배경, 맥락 \n        begin_indices = np.linspace(0, start_idx - 1, start_frames, dtype=int) if start_idx > 0 else np.zeros(start_frames, dtype=int)\n        \n        # 인덱스 결합\n        frame_indices = np.concatenate([begin_indices, end_indices])\n    \n    # 선택한 프레임 추출 \n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            # Use higher resolution and better interpolation\n            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        else:\n            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n    \n    cap.release()\n    return np.array(frames, dtype=np.uint8)\n\n# 먼저, 전역 범위에서 변환 클래스를 정의 \n# 입력된 영상 프레임을 일정 확률로 좌우 반전시켜서, 데이터 다양성을 늘리는 역할\nclass RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n        \n    def __call__(self, frames):\n        if np.random.random() < self.p:\n            return frames[:, :, ::-1, :].copy()  # horizontally flip each frame\n        return frames\n\n# 영상 프레임의 밝기와 대비를 무작위로 조정해, 다양한 조명 환경을 시뮬레이션하는 증강 클래스\nclass ColorJitter(object):\n    def __init__(self, brightness=0, contrast=0):\n        self.brightness = brightness\n        self.contrast = contrast\n        \n    def __call__(self, frames):\n        # Apply brightness jitter\n        if self.brightness > 0:\n            brightness_factor = np.random.uniform(max(0, 1-self.brightness), 1+self.brightness)\n            frames = frames * brightness_factor\n            frames = np.clip(frames, 0, 255)\n        \n        # Apply contrast jitter\n        if self.contrast > 0:\n            contrast_factor = np.random.uniform(max(0, 1-self.contrast), 1+self.contrast)\n            frames = (frames - 128) * contrast_factor + 128\n            frames = np.clip(frames, 0, 255)\n            \n        return frames\n\n# 프레임에 흐릿한 안개 효과를 넣어, 시야가 나쁜 날씨 상황을 시뮬레이션하는 클래스\nclass AddFog(object):\n    def __call__(self, frames):\n        fog = np.random.uniform(0.7, 0.9, frames.shape).astype(np.float32)\n        return frames * 0.8 + fog * 50  # Adjusted for 0-255 scale\n\n# 프레임에 흰색 선형 노이즈(빗방울)를 추가해 비 오는 날씨를 시뮬레이션하는 클래스\nclass AddRain(object):\n    def __call__(self, frames):\n        h, w = frames.shape[1:3]\n        rain = np.random.uniform(0, 1, (len(frames), h, w, 1)).astype(np.float32)\n        rain = (rain > 0.97).astype(np.float32) * 200  # White rain drops\n        return np.clip(frames * 0.9 + rain, 0, 255)  # Darken a bit and add drops\n\n# 지정된 확률에 따라 어떤 변환을 적용할지 말지를 무작위로 결정하는 컨트롤러 클래스(랜덤성 부여)\nclass RandomApply(object):\n    def __init__(self, transform, p=0.5):\n        self.transform = transform\n        self.p = p\n        \n    def __call__(self, frames):\n        if np.random.random() < self.p:\n            return self.transform(frames)\n        return frames\n\n# 여러 개의 변환(Flip, Jitter, Fog 등)을 순서대로 적용하는 데이터 증강 파이프라인 클래스\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def __call__(self, frames):\n        for t in self.transforms:\n            frames = t(frames)\n        return frames\n\n# 영상 프레임 배열을 PyTorch 텐서로 바꾸고, 픽셀 값을 0~1 범위로 정규화하는 클래스\nclass ToTensor(object):\n    def __call__(self, frames):\n        # Convert from (T, H, W, C) to (T, C, H, W)\n        frames = frames.transpose(0, 3, 1, 2)\n        # Convert to tensor and normalize to [0, 1]\n        return torch.from_numpy(frames).float() / 255.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.753189Z","iopub.execute_input":"2025-04-26T23:59:06.753507Z","iopub.status.idle":"2025-04-26T23:59:06.776208Z","shell.execute_reply.started":"2025-04-26T23:59:06.753484Z","shell.execute_reply":"2025-04-26T23:59:06.774882Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# 동영상에서 데이터 증강을 위한 변환을 반환\n\ndef get_video_transforms():\n    \"\"\"\n    Returns transformations for data augmentation in videos.\n    \"\"\"\n    return {\n        'train': Compose([\n            RandomHorizontalFlip(p=0.5),\n            ColorJitter(brightness=0.3, contrast=0.3),\n            RandomApply(AddFog(), p=0.15),\n            RandomApply(AddRain(), p=0.15),\n            RandomApply(RandomNoise(0.05), p=0.2), \n            RandomApply(RandomOcclusion(), p=0.1),\n            ToTensor()\n        ]),\n        'val': Compose([\n            ToTensor()  # Only tensor conversion for validation\n        ])\n    }\n\n# 비디오 프레임에서 무작위 가우시안(정규분포) 노이즈를 추가하여, 실제 촬영 환경에서 \n# 발생할 수 있는 잡음에 대해 모델이 더 강건해지도록 만드는 클래스\nclass RandomNoise(object):\n    \"\"\"\n    Applies random Gaussian noise to video frames for data augmentation.\n    \n    This transformation helps the model become more robust to noise\n    that may be present in real-world video data.\n    \n    Args:\n        std (float): Standard deviation of the Gaussian noise as a fraction\n                     of the pixel value range (default: 0.05)\n    \"\"\"\n    def __init__(self, std=0.05):\n        self.std = std\n        \n    def __call__(self, frames):\n        \"\"\"\n        Apply random noise to the input frames.\n        \n        Args:\n            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n                                   where T is number of frames\n        \n        Returns:\n            numpy.ndarray: Noise-augmented frames, clipped to valid pixel range [0, 255]\n        \"\"\"\n        # 지정된 표준 편차를 가진 가우시안 노이즈 생성\n        noise = np.random.normal(0, self.std * 255, frames.shape).astype(np.float32)\n        \n        # 유효한 픽셀 범위에 노이즈 및 클립 추가하기\n        # 영상은 정수형 데이터여야 하므로 형 변환 (astype)\n        return np.clip(frames + noise, 0, 255).astype(np.uint8)\n\n# 영상 프레임에 검은색 사각형을 무작위로 덮어 씌워, 일부 정보가 가려졌을 때도 모델이 견딜 수 있도록 훈련시키는 클래스\nclass RandomOcclusion(object):\n    \"\"\"\n    Simulates occlusion in video frames by adding black rectangles.\n    \n    This transformation helps the model learn to handle partial occlusions\n    that may occur in real-world scenarios when objects block the camera view.\n    \"\"\"\n    def __call__(self, frames):\n        \"\"\"\n        Apply random occlusion to the input frames.\n        \n        Args:\n            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n                                   where T is number of frames\n        \n        Returns:\n            numpy.ndarray: Frames with random occlusion applied\n        \"\"\"\n        # 프레임 하나의 세로(h), 가로(w) 길이 가져오기\n        h, w = frames.shape[1:3]\n        \n        # 전체 프레임 크기의 10%~25% 사이 크기의 가림 영역 크기 설정\n        occl_h = np.random.randint(int(h * 0.1), int(h * 0.25))\n        occl_w = np.random.randint(int(w * 0.1), int(w * 0.25))\n        \n        # 이 가림 영역이 들어갈 무작위 위치 좌표 설정 \n        occl_x = np.random.randint(0, w - occl_w)\n        occl_y = np.random.randint(0, h - occl_h)\n        \n        # 원본 프레임을 수정하지 않도록 복사본 만들기\n        frames_copy = frames.copy()\n        \n        # 픽셀을 0(검정색)으로 설정하여 모든 프레임에 occlusion 적용\n        for i in range(len(frames)):\n            frames_copy[i, occl_y:occl_y+occl_h, occl_x:occl_x+occl_w, :] = 0\n            \n        return frames_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.777877Z","iopub.execute_input":"2025-04-26T23:59:06.778273Z","iopub.status.idle":"2025-04-26T23:59:06.802257Z","shell.execute_reply.started":"2025-04-26T23:59:06.778238Z","shell.execute_reply":"2025-04-26T23:59:06.801126Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# 비디오 프레임 간 움직임(모션)을 추적하는 'optical_flow'를 계산해, 객체나 배경의 이동 방향과 속도를 벡터 형태로 반환하는 함수\n# 두 연속된 이미지(또는 프레임) 사이에서, 각 픽셀이 어떻게 이동했는지를 벡터로 표현하는 기술 -> optical_flow\n# Farneback 방식만 사용\n# \"모든 픽셀의 방향 + 속도 정보를 다 남김\"\n\ndef compute_optical_flow(frames, skip_frames=1):\n    \"\"\"Calculates optical flow skipping some frames to reduce processing.\"\"\"\n    if len(frames) < 2:\n        return np.zeros((1, frames.shape[1], frames.shape[2], 2), dtype=np.float32)\n    \n    flows = []\n\n    # 첫 프레임을 그레이스케일(흑백 이미지)로 변환\n    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n\n    # 연속된 프레임 쌍마다 optical_flow 계산 \n    for i in range(1, len(frames), skip_frames):\n        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n        try:\n            # 더 빠른 계산을 위해 매개변수 줄이기\n            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray,\n                                               None, 0.5, 3, 15, 3, 5, 1.2, 0)\n            flows.append(flow)\n        # 예외 처리 (계산 중 오류 발생 시 안전하게 0으로 대체)\n        except Exception as e:\n            print(f\"Error calculating optical flow: {str(e)}\")\n            flows.append(np.zeros((frames.shape[1], frames.shape[2], 2), dtype=np.float32))\n            \n        prev_gray = curr_gray\n    \n    if not flows:\n        return np.zeros((1, frames.shape[1], frames.shape[2], 2), dtype=np.float32)\n        \n    return np.array(flows, dtype=np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.803397Z","iopub.execute_input":"2025-04-26T23:59:06.803819Z","iopub.status.idle":"2025-04-26T23:59:06.829143Z","shell.execute_reply.started":"2025-04-26T23:59:06.803794Z","shell.execute_reply":"2025-04-26T23:59:06.827896Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.830380Z","iopub.execute_input":"2025-04-26T23:59:06.831903Z","iopub.status.idle":"2025-04-26T23:59:06.855377Z","shell.execute_reply.started":"2025-04-26T23:59:06.831862Z","shell.execute_reply":"2025-04-26T23:59:06.854125Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# Positive이면 alert_event 사이만 뽑고, Negative이면 마지막 3초 구간을 기준으로 추출\n# 그리고 num_frames만큼 균등하게 뽑고 CNN + Optical Flow 둘 다 계산 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.856164Z","iopub.execute_input":"2025-04-26T23:59:06.856497Z","iopub.status.idle":"2025-04-26T23:59:06.882057Z","shell.execute_reply.started":"2025-04-26T23:59:06.856474Z","shell.execute_reply":"2025-04-26T23:59:06.880806Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# InceptionV3 모델로 특성 추출\nbase_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\ncnn_feature_dim = base_model.output_shape[-1]\n\ndef get_hybrid_features(video_path, alert_time=None, event_time=None, num_frames=8):\n    \"\"\"\n    Extract hybrid features (CNN features + Optical flow feature) from a video.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n\n    if not cap.isOpened():\n        print(f\"Could not open video: {video_path}\")\n        return np.zeros(1280 + 1)\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = cap.get(cv2.CAP_PROP_FPS) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30\n    video_duration = total_frames / fps\n\n    # 이벤트가 있는 경우 (Positive)\n    if alert_time is not None and event_time is not None and event_time > alert_time:\n        # alert_time ~ event_time 사이 구간만 추출\n        start_frame = int(alert_time * fps)\n        end_frame = int(event_time * fps)\n        start_frame = max(0, min(start_frame, total_frames-1))\n        end_frame = max(0, min(end_frame, total_frames-1))\n\n    else:\n        # 이벤트가 없는 경우 (Negative)\n        # 마지막 3초 기준\n        last_seconds = 3\n        start_frame = max(0, total_frames - int(last_seconds * fps))\n        end_frame = total_frames - 1\n\n    if end_frame <= start_frame:\n        frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n    else:\n        frame_indices = np.linspace(start_frame, end_frame, num_frames, dtype=int)\n\n    # 프레임 읽기\n    frames = []\n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        if ret:\n            frame = cv2.resize(frame, (160, 160), interpolation=cv2.INTER_LANCZOS4)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n    cap.release()\n\n    frames = np.array(frames)\n\n    if len(frames) == 0:\n        return np.zeros(1280 + 1)\n\n    # CNN Features\n    spatial_features = base_model.predict(\n        preprocess_input(frames.astype('float32')),\n        batch_size=32,\n        verbose=0\n    )\n\n    # Optical Flow Feature\n    flow_feature_farneback = 0.0\n    if len(frames) > 1:\n        flow_farneback = compute_optical_flow(frames, skip_frames=1)\n        flow_magnitude = np.linalg.norm(flow_farneback, axis=-1)  # (T-1, H, W)\n        flow_feature_farneback = np.mean(flow_magnitude)\n\n    # Feature 결합\n    return np.concatenate([\n        np.mean(spatial_features, axis=0),  # (1280,)\n        [flow_feature_farneback]  # (1,)\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:06.883445Z","iopub.execute_input":"2025-04-26T23:59:06.883788Z","iopub.status.idle":"2025-04-26T23:59:09.209473Z","shell.execute_reply.started":"2025-04-26T23:59:06.883765Z","shell.execute_reply":"2025-04-26T23:59:09.208314Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def get_hybrid_features_from_frames(frames):\n    \"\"\"\n    Extract hybrid features (CNN features + Optical flow feature) from pre-extracted frames.\n    Input frames must be (T, 3, 160, 160) PyTorch tensor.\n    \"\"\"\n    if len(frames) == 0:\n        return np.zeros(1280 + 1)\n\n    # CNN Features\n    # frames: (T, 3, 160, 160) → (T, 160, 160, 3)로 변환\n    frames_np = frames.permute(0, 2, 3, 1).numpy() * 255.0  # 다시 [0,255] 범위로 되돌리기\n    frames_np = frames_np.astype(np.uint8)\n\n    spatial_features = base_model.predict(\n        preprocess_input(frames_np.astype('float32')),\n        batch_size=32,\n        verbose=0\n    )\n\n    # Optical Flow Feature\n    flow_feature_farneback = 0.0\n    if len(frames_np) > 1:\n        flow_farneback = compute_optical_flow(frames_np, skip_frames=1)\n        flow_magnitude = np.linalg.norm(flow_farneback, axis=-1)  # (T-1, H, W)\n        flow_feature_farneback = np.mean(flow_magnitude)\n\n    # Feature 결합\n    return np.concatenate([\n        np.mean(spatial_features, axis=0),  # (1280,)\n        [flow_feature_farneback]            # (1,)\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T23:59:09.212886Z","iopub.execute_input":"2025-04-26T23:59:09.213238Z","iopub.status.idle":"2025-04-26T23:59:09.221487Z","shell.execute_reply.started":"2025-04-26T23:59:09.213191Z","shell.execute_reply":"2025-04-26T23:59:09.220307Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"from tqdm import tqdm  # 꼭 import 추가!\n\n# 1. transform 준비\ntransforms = get_video_transforms()\ntrain_transform = transforms['train']\n\n# 2. feature를 저장할 리스트 준비\nall_features = []\n\n# 3. train_df 전체 반복 (여기 tqdm 적용!)\nfor idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n    video_id = row['id']  # 'id' 사용!\n    video_path = os.path.join(train_video_dir, f\"{int(video_id):05d}.mp4\")  # 5자리 zero-padding\n\n    # 비디오 열기\n    frames = extract_keyframes(video_path, num_frames=12, target_size=(160,160))\n\n    if frames.shape[0] == 0:\n        print(f\"Skipping video {video_id} (no frames)\")\n        continue\n\n    # transform 적용\n    frames_augmented = train_transform(frames)\n\n    # feature 뽑기\n    feature = get_hybrid_features_from_frames(frames_augmented)\n\n    all_features.append(feature)\n\n# 4. 결과를 numpy array로 변환\nall_features = np.array(all_features)\nprint(f\"\\nAll features shape: {all_features.shape}\")\n# 예시 출력: (n_samples, 1281)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:23:09.195705Z","iopub.execute_input":"2025-04-27T00:23:09.196179Z","iopub.status.idle":"2025-04-27T02:35:43.350532Z","shell.execute_reply.started":"2025-04-27T00:23:09.196149Z","shell.execute_reply":"2025-04-27T02:35:43.347715Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1500/1500 [2:12:34<00:00,  5.30s/it] ","output_type":"stream"},{"name":"stdout","text":"\nAll features shape: (1500, 2049)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# from tqdm import tqdm\n\n# # 1. transform 준비\n# transforms = get_video_transforms()\n# train_transform = transforms['train']\n\n# # 2. feature 저장할 리스트 준비\n# all_features = []\n\n# # 3. train_df 중 50개만 선택해서 진행\n# for idx, row in tqdm(train_df.iloc[:50].iterrows(), total=20):\n#     video_id = row['id']\n#     video_path = os.path.join(train_video_dir, f\"{video_id}.mp4\")\n    \n#     frames = extract_keyframes(video_path, num_frames=12, target_size=(160,160))\n    \n#     if frames.shape[0] == 0:\n#         print(f\"Skipping video {video_id} (no frames)\")\n#         continue\n    \n#     frames_augmented = train_transform(frames)\n#     feature = get_hybrid_features_from_frames(frames_augmented)\n    \n#     all_features.append(feature)\n\n# # 4. 결과를 numpy array로 변환\n# all_features = np.array(all_features)\n# print(f\"\\nExtracted {all_features.shape[0]} features, each of shape {all_features.shape[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:08:57.837478Z","iopub.execute_input":"2025-04-27T00:08:57.837816Z","iopub.status.idle":"2025-04-27T00:09:31.400115Z","shell.execute_reply.started":"2025-04-27T00:08:57.837792Z","shell.execute_reply":"2025-04-27T00:09:31.398908Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/50 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1924.mp4\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 1/50 [00:00<00:43,  1.13it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/822.mp4\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 2/50 [00:01<00:32,  1.46it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1429.mp4\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 3/50 [00:02<00:35,  1.32it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/208.mp4\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 4/50 [00:02<00:31,  1.48it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1904.mp4\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 5/50 [00:03<00:28,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/72.mp4\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 6/50 [00:03<00:26,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1486.mp4\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 7/50 [00:04<00:26,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1171.mp4\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 8/50 [00:05<00:25,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/128.mp4\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 9/50 [00:06<00:28,  1.44it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1518.mp4\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 10/50 [00:06<00:26,  1.53it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/205.mp4\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 11/50 [00:07<00:24,  1.59it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1889.mp4\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 12/50 [00:07<00:23,  1.60it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1286.mp4\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 13/50 [00:08<00:22,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/408.mp4\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 14/50 [00:08<00:21,  1.71it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/948.mp4\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 15/50 [00:09<00:20,  1.74it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/457.mp4\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 16/50 [00:09<00:19,  1.77it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1267.mp4\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 17/50 [00:10<00:21,  1.52it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1802.mp4\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 18/50 [00:11<00:22,  1.41it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1755.mp4\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 19/50 [00:12<00:20,  1.51it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/471.mp4\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 20/50 [00:13<00:21,  1.40it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/2134.mp4\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 21/50 [00:13<00:19,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/900.mp4\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 22/50 [00:14<00:17,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/841.mp4\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 23/50 [00:14<00:16,  1.64it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/333.mp4\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 24/50 [00:15<00:17,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/564.mp4\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 25/50 [00:16<00:16,  1.54it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/245.mp4\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 26/50 [00:16<00:16,  1.42it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/171.mp4\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 27/50 [00:17<00:15,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1076.mp4\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▌    | 28/50 [00:18<00:13,  1.60it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/795.mp4\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 29/50 [00:18<00:12,  1.65it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/372.mp4\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 30/50 [00:19<00:13,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/938.mp4\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 31/50 [00:20<00:13,  1.37it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1749.mp4\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 32/50 [00:20<00:12,  1.45it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/884.mp4\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 33/50 [00:21<00:11,  1.52it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/641.mp4\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 34/50 [00:22<00:10,  1.58it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/10.mp4\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 35/50 [00:22<00:09,  1.63it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1816.mp4\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 36/50 [00:23<00:09,  1.45it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1052.mp4\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 37/50 [00:24<00:09,  1.33it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1205.mp4\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 38/50 [00:25<00:09,  1.28it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/927.mp4\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 39/50 [00:25<00:07,  1.40it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/378.mp4\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 40/50 [00:26<00:06,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1579.mp4\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 41/50 [00:27<00:06,  1.38it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1973.mp4\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 42/50 [00:28<00:06,  1.32it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/238.mp4\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 43/50 [00:29<00:05,  1.25it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1050.mp4\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 44/50 [00:29<00:04,  1.39it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1204.mp4\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 45/50 [00:30<00:03,  1.45it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/313.mp4\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 46/50 [00:31<00:02,  1.35it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/876.mp4\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 47/50 [00:31<00:02,  1.46it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/98.mp4\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 48/50 [00:32<00:01,  1.55it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1505.mp4\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 49/50 [00:32<00:00,  1.43it/s]","output_type":"stream"},{"name":"stdout","text":"Could not open the video: /kaggle/input/nexar-collision-prediction/train/1699.mp4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50/50 [00:33<00:00,  1.49it/s]","output_type":"stream"},{"name":"stdout","text":"\nExtracted 50 features, each of shape 2049\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 외부에서 object_detector와 driver_attention_model을 인자로 주입받도록 설계\n\n# 더미 객체 탐지기 클래스\nclass ObjectDetector:\n    def detect(self, frame):\n        h, w, _ = frame.shape\n        return [(w//3, h//3, 2*w//3, 2*h//3)]  # 테스트용 더미 박스\n\n# 더미 시선 예측 모델 클래스\nclass DummyDriverAttention:\n    def predict(self, frame):\n        h, w, _ = frame.shape\n        attention_map = np.zeros((h, w), dtype=np.float32)\n        cx, cy = w // 2, h // 2\n        attention_map[cy-10:cy+10, cx-10:cx+10] = 1.0\n        return cv2.GaussianBlur(attention_map, (25, 25), 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.143440Z","iopub.status.idle":"2025-04-27T00:00:37.143917Z","shell.execute_reply.started":"2025-04-27T00:00:37.143648Z","shell.execute_reply":"2025-04-27T00:00:37.143671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nfrom torchvision import models, transforms\n\nclass AATDAPreprocessor:\n    def __init__(self, object_detector, driver_attention_model, vgg_model=None):\n        self.detector = object_detector  # Cascade R-CNN 등 외부 감지기\n        self.attention_model = driver_attention_model  # Gate-DAP 등\n        self.vgg = vgg_model or models.vgg16(pretrained=True).features.eval()\n        self.vgg_fc7 = torch.nn.Sequential(*list(models.vgg16(pretrained=True).classifier.children())[:6]).eval()\n        self.vgg_transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])\n\n    # 감지된 객체 영역을 잘라서 VGG FC7 feature(4096-dim) 추출\n    def extract_object_features(self, frame, boxes):\n        features = []\n        for (x1, y1, x2, y2) in boxes:\n            crop = frame[y1:y2, x1:x2]\n            if crop.shape[0] == 0 or crop.shape[1] == 0:\n                features.append(torch.zeros(4096))\n                continue\n            input_tensor = self.vgg_transform(crop).unsqueeze(0)\n            with torch.no_grad():\n                feat = self.vgg(input_tensor)\n                feat = feat.view(feat.size(0), -1)\n                fc7 = self.vgg_fc7(feat).squeeze(0)\n            features.append(fc7)\n        return torch.stack(features)  # Shape: (N, 4096)\n\n    # attention map과 객체 영역이 얼마나 겹치는지 평균값으로 가중치 계산 \n    def compute_attention_weights(self, attention_map, boxes):\n        weights = []\n        for (x1, y1, x2, y2) in boxes:\n            patch = attention_map[y1:y2, x1:x2]\n            weight = np.mean(patch) if patch.size > 0 else 0.0\n            weights.append(weight)\n        return torch.tensor(weights, dtype=torch.float32)  # Shape: (N,)\n\n    # 한 프레임에 대해 아래와 같이 순서대로 작동\n    def process_frame(self, frame):\n        # 1. 객체 감지\n        boxes = self.detector.detect(frame)  # returns List of [x1, y1, x2, y2]\n\n        # 2. VGG Feature 추출\n        object_feats = self.extract_object_features(frame, boxes)  # (N, 4096)\n\n        # 3. 시선 맵 생성\n        attention_map = self.attention_model.predict(frame)  # (H, W), numpy float32\n\n        # 4. Attention Weight 계산\n        weights = self.compute_attention_weights(attention_map, boxes)  # (N,)\n\n        # 5. Feature 강화\n        weighted_feats = object_feats * weights.unsqueeze(1)  # (N, 4096)\n\n        return weighted_feats, weights, boxes  # Transformer 입력으로 사용할 수 있음\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.145890Z","iopub.status.idle":"2025-04-27T00:00:37.146287Z","shell.execute_reply.started":"2025-04-27T00:00:37.146097Z","shell.execute_reply":"2025-04-27T00:00:37.146112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 실행 예시\nif __name__ == '__main__':\n    detector = ObjectDetector()\n    attention_model = DummyDriverAttention()\n    processor = AATDAPreprocessor(detector, attention_model)\n\n    # 테스트용 이미지 불러오기\n    frame = cv2.imread(\"sample_frame.jpg\")  # 실제 경로로 작성하기!!!\n    if frame is not None:\n        weighted_feats, weights, boxes = processor.process_frame(frame)\n        print(\"감지된 객체 수:\", len(boxes))\n        print(\"Feature shape:\", weighted_feats.shape)  # (N, 4096)\n    else:\n        print(\"이미지를 불러올 수 없습니다.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.147178Z","iopub.status.idle":"2025-04-27T00:00:37.147529Z","shell.execute_reply.started":"2025-04-27T00:00:37.147386Z","shell.execute_reply":"2025-04-27T00:00:37.147400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 비디오 하나에 포함된 여러 프레임을 process_frame()으로 반복 처리 \n# 각 프레임의 객체 feature를 (T, N, 4096) 구조로 패딩하여 Transformer 입력에 맞춤\n\nclass AATDADataset(Dataset):\n    def __init__(self, video_data_dict, annotations, processor, num_frames=8):\n        self.video_data = video_data_dict  # {video_id: [frame1, frame2, ...]}\n        self.annotations = annotations  # {video_id: {label: ..., alert_time: ...}}\n        self.processor = processor\n        self.num_frames = num_frames\n        self.video_ids = list(video_data_dict.keys())\n\n    def __len__(self):\n        return len(self.video_ids)\n\n    def __getitem__(self, idx):\n        video_id = self.video_ids[idx]\n        frames = self.video_data[video_id]  # raw RGB frames\n\n        # 각 프레임에 대해 Object Feature + Attention 처리\n        sequence_features = []\n        for frame in frames:\n            weighted_feats, _, _ = self.processor.process_frame(frame)\n            sequence_features.append(weighted_feats)\n\n        # 시퀀스를 (T, N, 4096) 형태의 tensor로 정리 (padding 가능)\n        max_objects = max(f.shape[0] for f in sequence_features)\n        padded_seq = []\n        for f in sequence_features:\n            pad_size = max_objects - f.shape[0]\n            if pad_size > 0:\n                padded = torch.cat([f, torch.zeros(pad_size, 4096)], dim=0)\n            else:\n                padded = f\n            padded_seq.append(padded)\n\n        # 최종 시퀀스: (T, N, 4096)\n        input_tensor = torch.stack(padded_seq)\n\n        label = torch.tensor(self.annotations[video_id]['label']).float()\n        alert_time = torch.tensor(self.annotations[video_id].get('alert_time', 0)).float()\n\n        return {\n            'video_id': video_id,\n            'input': input_tensor,  # (T, N, 4096)\n            'label': label,\n            'alert_time': alert_time\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.148747Z","iopub.status.idle":"2025-04-27T00:00:37.149133Z","shell.execute_reply.started":"2025-04-27T00:00:37.148964Z","shell.execute_reply":"2025-04-27T00:00:37.148984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **1단계: 일반 전처리** \n#### **(1) 프레임 추출 - extract_keyframes()** \n#### **(2) Optical Flow 계산 - compute_optical_flow()** \n#### **(3) 기본 증강 - ColorJitter, AddRain, AddFog, ToTensor() 등**\n#### **(4) 데이터 구성 - DashcamDataset 또는 PreprocessDashcamDataset으로 구성**","metadata":{}},{"cell_type":"markdown","source":"### **2단계: AAT-DA 전용 전처리 (기존 데이터에서 Transformer용 구조 변환)**\n##### AAT-DA는 단순한 영상 프레임이 아니라, \"객체 중심의 시공간 Attention 입력 구조\"를 요구하기 때문에 기존 전처리된 데이터를 바탕으로 추가적인 전처리가 필요\n#### **(1) 객체 감지 - 프레임에서 객체 감지 (Cascade R-CNN 등)** \n#### **(2) 객체 특징 추출 - 감지된 박스마다 VGG16 FC7 feature 추출 (4096-dim)** \n#### **(3) 시선 맵 예측 - Gate-DAP 등으로 driver attention heatmap 생성**\n#### **(4) 주의 가중치 계산 - 시선 맵 + 객체 위치 → 객체별 attention weight αᵢ 계산**\n#### **(5) Feature 가중 - 객체 feature αᵢ → 강조된 객체 feature**\n#### **(6) 시퀀스 구성 - 모든 프레임의 결과를 (T, N, 4096) 시퀀스로 패딩 정리**","metadata":{}},{"cell_type":"code","source":"# AAT-DA 모델 구성\n# 입력: (B, T, N, 4096)\n# 구성: Spatial Transformer + Temporal Transformer + Classifier\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# 각 프레임 내 객체들 간 상호작용 학습 (Object Self-Attention)\nclass SpatialTransformer(nn.Module):\n    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n\n    def forward(self, x):  # x: (B, T, N, 4096)\n        B, T, N, C = x.shape\n        x = x.view(B * T, N, C)  # (B*T, N, C)\n        x = self.encoder(x)  # (B*T, N, C)\n        x = x.mean(dim=1)  # (B*T, C)\n        x = x.view(B, T, C)  # (B, T, C)\n        return x\n\n# 시간 순서에 따른 의미 흐름 모델링 (Temporal Attention)\nclass TemporalTransformer(nn.Module):\n    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n\n    def forward(self, x):  # x: (B, T, C)\n        x = x.transpose(0, 1)  # (T, B, C)\n        x = self.encoder(x)  # (T, B, C)\n        x = x.transpose(0, 1)  # (B, T, C)\n        return x\n\nclass AATDA(nn.Module):\n    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048, dropout=0.1):\n        super().__init__()\n        self.spatial_transformer = SpatialTransformer(d_model, nhead, dim_feedforward)\n        self.temporal_transformer = TemporalTransformer(d_model, nhead, dim_feedforward)\n        self.classifier = nn.Sequential(\n            nn.Linear(d_model, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, x):  # x: (B, T, N, 4096)\n        x = self.spatial_transformer(x)     # (B, T, 4096)\n        x = self.temporal_transformer(x)    # (B, T, 4096)\n        x = x.mean(dim=1)                   # (B, 4096) - 평균 풀링\n        out = self.classifier(x)            # (B, 1)\n        return out.squeeze(-1)              # (B,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T00:00:37.151308Z","iopub.status.idle":"2025-04-27T00:00:37.151697Z","shell.execute_reply.started":"2025-04-27T00:00:37.151536Z","shell.execute_reply":"2025-04-27T00:00:37.151551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}