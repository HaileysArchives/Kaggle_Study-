# Final Notebook README

## ğŸš— Nexar DCP Challenge ì „ì²´ íŒŒì´í”„ë¼ì¸ & ì½”ë“œ ë¦¬ë·°

---

### ğŸ“ **ì „ì²´ ëª©í‘œ**

- ë¹„ë””ì˜¤ ë°ì´í„° (mp4)ì—ì„œ í”„ë ˆì„ ê¸°ë°˜ featureë¥¼ ì¶”ì¶œ
- Temporal + Spatial Transformerë¡œ ì¶©ëŒ ì˜ˆì¸¡ (binary classification)
- ìµœì¢… Kaggle submission ì œì¶œ

---

### ğŸ— **1ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**

âœ… **ì£¼ìš” ë‚´ìš©**:

- ì›ë³¸: `train_df` (ë¹„ë””ì˜¤ ID + ë¼ë²¨)
- ê° ë¹„ë””ì˜¤ì—ì„œ 12í”„ë ˆì„ ì¶”ì¶œ
- í”„ë ˆì„ë‹¹ feature:
    - spatial feature (1280 dim)
    - optical flow feature (1 dim)
    - **ì´ 2049 dim** (ì£¼ì˜! ì´ ê°’ì´ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì¤‘ìš”)

âœ… **í•µì‹¬ ì½”ë“œ**:

```python

for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):
    video_id = row['id']
    video_path = os.path.join(train_video_dir, f"{int(video_id):05d}.mp4")
    sequence = prepare_transformer_input(video_path, num_frames=12)

    if sequence is not None:
        all_sequences.append(sequence)
```

âœ… **ê²°ê³¼**:

- `all_sequences`: shape `(n_samples, 12, 2049)`
- `.npy` íŒŒì¼ë¡œ ì €ì¥ â†’ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°

âœ… **ë¦¬ë·°**:

- ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ë¼ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¼
- ì¤‘ê°„ì— `.npy` ì €ì¥ ê°•ë ¥ ì¶”ì²œ!!! (ë‹¤ì‹œ ì²˜ë¦¬í•˜ëŠ” ê±¸ ë°©ì§€)

---

### ğŸ‹ï¸â€â™‚ï¸ **2ï¸âƒ£ Dataset & DataLoader êµ¬ì„±**

âœ… **Dataset í´ë˜ìŠ¤**:

```python

class VideoSequenceDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = torch.tensor(sequences, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]
```

âœ… **DataLoader**:

- 80/20 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ê¸° (train/validation split)
- `batch_size=16`

âœ… **ë¦¬ë·°**:

- DataLoader shuffle: train=True, val=False
- large `.npy` ë©”ëª¨ë¦¬ ì ì¬ ì£¼ì˜ (RAM ì œí•œ í™•ì¸)

---

### ğŸ§  **3ï¸âƒ£ ëª¨ë¸ ì„¤ê³„ (Transformer ê¸°ë°˜)**

### âœ… **Temporal Transformer**:

> â€œê° í”„ë ˆì„ì—ì„œ **ê·¸ë¦¼ + ì›€ì§ì„ + ë§¥ë½**ê¹Œì§€ ë‹¤ ë³´ê³  í•™ìŠµâ€
> 
- **ì…ë ¥: (batch, 12, 2049)**
    - ë”°ë¼ì„œ **backbone + optical flow + ì¶”ê°€ feature**ê¹Œì§€ ë‹¤ í•©ì¹œ 2049 ì°¨ì›
- Linear projection â†’ Transformer â†’ mean pooling â†’ (batch, 256)

### âœ… **Spatial Transformer**:

> CNN ê°™ì€ Backboneì—ì„œ ì¶”ì¶œí•œ per-frame(í•œ í”„ë ˆì„ ë‹¨ìœ„ë‹¹) featureëŠ” ë³´í†µ 1280 ì°¨ì›ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤
> 
- **ì…ë ¥: (batch, 12, 1280)**
- Transformer â†’ mean pooling â†’ (batch, 1280)

<aside>

https://mytomato.tistory.com/66

</aside>

### âœ… **Combined Model**:

```python

combined = torch.cat([temporal_out, spatial_out], dim=1)  # (batch, 1536)
out = self.classifier(combined)                           # (batch, 1)
```

âœ… **ë¦¬ë·°**:

- `temporal_input_dim` â†’ ë°˜ë“œì‹œ 2049 (ì•„ë‹ˆë©´ shape mismatch ë°œìƒ!)
- ìµœì¢… classifier input dim â†’ 256 + 1280 = 1536

---

### ğŸ”¥ **4ï¸âƒ£ í•™ìŠµ ë£¨í”„**

âœ… **í•µì‹¬ ë£¨í”„**:

```python

for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        temporal_input = inputs[:, :, :2049]
        spatial_input = inputs[:, :, :1280]

        outputs = model(temporal_input, spatial_input)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

âœ… **ë¦¬ë·°**:

- ë°˜ë“œì‹œ temporal/spatial slicing ì •í™•íˆ ë§ì¶”ê¸°
- optimizer: Adam (lr=1e-4), loss: BCELoss
- validation accuracy ëª¨ë‹ˆí„°ë§ í•„ìˆ˜

---

### ğŸ’¾ **5ï¸âƒ£ ì €ì¥ & Submission**

âœ… **ì¤‘ê°„ ì €ì¥**:

- `.npy` or `.pt`ë¡œ feature/ëª¨ë¸ ì €ì¥ â†’ ì¬í›ˆë ¨ ë°©ì§€

âœ… **ìµœì¢… ì œì¶œ**:

- Kaggle: Save & Run All â†’ submission ìƒì„±

---

### ğŸ“Œ **ìµœì¢… ë©”ëª¨**

- íŒŒì´í”„ë¼ì¸ ì†ë„ ê°œì„ :
    - frame ìˆ˜ ì¤„ì´ê¸°
    - feature dim ì¶•ì†Œ
    - transformer layer/head ìˆ˜ ìµœì í™”
- ë©”ëª¨ë¦¬ ê´€ë¦¬:
    - `.npy` ë¶„ë¦¬ ì €ì¥
    - batch size ì¡°ì •
- ì œì¶œ ì „: ë¡œì»¬ì—ì„œ ì „ì²´ inference ëŒë ¤ì„œ shape mismatch ì—†ëŠ”ì§€ í™•ì¸

![kaggle_image.png](kaggle_image.png)