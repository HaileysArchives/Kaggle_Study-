# Machine Learning Study (Concepts)

## **💡 1. CNN (Convolutional Neural Networ**k)

<aside>

합성곱 신경망(CNN)은 이미지 및 비디오 처리를 위해 설계되었다. 컨볼루션 레이어(Convolution Layer)를 사용하여 입력 데이터에서 특징을 추출하고 풀링 레이어(Pooling Layer)를 사용하여 출력의 차원을 줄인다. CNN은 물체 인식 및 분할과 같이 공간적 이해가 필요한 작업에 적합하다. 

</aside>

### 1️⃣ 개념

- 이미지 처리에 특화된 신경망
- 입력 이미지의 공간적 패턴(위치, 형태)을 추출하는 데 탁월하다
- 필터(커널)를 이미지 위에 슬라이딩하면서 특징을 추출한다

![image.png](image.png)

### 2️⃣ 작동 원리

- **합성곱층 (Convolution Layer)**
    - 여러 필터(Kernal)을 사용해 합성곱을 수행하고 이미지에서 특징(예: 윤관, 모서리, 텍스처)을 추출해 특징 맵을 생성
- **풀링층 (Pooling Layer)**
    - 계산량과 과대적합을 방지하기 위해 특징 요약 (예: Max Pooling)
- **완전연결층 (Fully Connected Layer)**
    - 추출된 특징을 바탕으로 최종 예측을 담당하는 부분

![image.png](image%201.png)

### 3️⃣ 예시

- 고양이 사진 → 고양이 귀, 눈, 수염 등의 패턴을 인식한 후 ‘고양이’로 분류

---

## 🔁 2. RNN (Recurrent Neural Network)

<aside>

순환 신경망(RNN)은 시계열이나 자연어와 같은 순차적인 데이터를 처리하도록 설계되었다. 시간이 지나도 정보가 지속될 수 있도록 피드백 루프(Feedback Loop)가 있어 메모리가 필요한 작업에 적합하다. 

</aside>

*피드백 루프란? 네트워크의 출력이 다시 입력으로 연결되는 구조를 말한다. 

### 1️⃣ 개념

- 순차적 데이터(시계열 데이터)를 처리하기 위한 모델
- 이전의 정보를 기억하면서 다음 데이터를 처리
- 자연어 처리(NLP), 음성인식, 주가/기온 등 시계열 데이터 등에 주로 활용된다

### 2️⃣ 작동 원리

1. 시점 t의 입력이 들어오면, 
2. 이전 상태(hidden state) 정보를 함께 이용해 현재 출력을 계산한다
3. 이 출력이 다음 시점의 입력에 영향을 준다

![image.png](image%202.png)

### 3️⃣ 한계점

**1. 장기 의존성 문제 (Long-Term Dependency)**

- 오래된 정보가 점점 희미해짐 (Vanishing Gradient 문제)

<aside>

### 💥 문제는 ‘학습 과정’에서 발생한다

1. 신경망은 출력에 따라 오차(loss)를 계산한다 
2. 이 오차를 뒤로 전달하면서 각 가중치를 업데이트한다 
3. 이를 반복함으로써 모델이 점점 똑똑해진다 
</aside>

> **RNN은 매 시간의 오차가 이전 시간의 가중치에 영향을 주기 위해, 곱하고 또 곱하고 또 곱하는 과정을 반복한다**
> 

### 📌 그리고 곱을 계속하게 되면?

1. 행렬이 작으면 → 점점 0에 가까워짐 (Vanishing Gradient)
2. 행렬이 크면 → 폭주함 (Exploding Gradient) 

**2. 병렬처리 어려움** 

- 입력을 순차적으로 처리해야 하므로, 속도가 느림 (GPU 성능 활용이 어렵다)

---

## ✨ 3. Transformer

<aside>

트랜스포머(Transformer)는 **인코더(Encoder)**와 **디코더(Decoder)**라는 두 가지 주요 블록으로 구성된 **Seq2Seq(Sequence to Sequence) 모델**이다. 

가장 큰 특징은 **어텐션 메커니즘(Attention Mechanism)**을 사용하여 각 입력 요소가 다른 모든 요소와의 상관관계를 학습한다는 것이다. 이를 통해 모델은 입력 시퀀스 내에서 중요한 정보에 더 집중할 수 있다.

</aside>

![image.png](image%203.png)

### 1️⃣ 개념

- RNN 없이도 순차 정보를 병렬적으로 처리할 수 있는 구조
- **Attention 메커니즘**을 기반으로 동작
    
    ### 💡 Attention 메커니즘
    
    - “어떤 단어(혹은 객체)가 다른 단어와 얼마나 관련이 있는가?”를 수치화
    - 예: “The cat sat on the mat” → “sat”는 “cat”과 강하게 연결됨

### 2️⃣ 작동 원리

1. 각 입력 토큰(단어나 이미지 패치)을 벡터로 변환
2. 모든 토큰이 서로를 동시에 주목한다 
3. **Self-Attention**을 통해 관계를 계산하여, 중요한 정보를 강조한다
4. 이 과정을 **병렬적**으로 처리한다 

### 3️⃣ 장점 ****

- **병렬 처리:** 모든 시퀀스 요소를 동시에 처리할 수 있어 훈련과 추론 속도가 빠름
- **긴 문맥 정보 학습:** Self-Attention을 통해 긴 문맥 의존성을 효과적으로 학습함

### 4️⃣ 단점

- **기하급수적 메모리 요구**: 시퀀스 길이가 길어질수록 메모리 요구량이 기하급수적으로 증가함
- **위치 정보 손실**: 위치 인코딩으로 시퀀스 정보를 보존하지만, 순환 신경망(RNN)처럼 자연스러운 순차 흐름을 표현하는 데는 한계가 있다

![image.png](image%204.png)

---

# 🚗 AM-NET(Attention-guided Multistream Feature Fusion Network) 모델

## 1️⃣ Attention과 Transformer의 차이가 뭘까?

> **Attention은 “기술”이고, Tranformer는 그 기술을 기반으로 만든 모델 구조**
> 

### 📌 Attention이란?

<aside>

입력 안에 서로 어떤 정보가 더 중요하냐를 판단해서 중요한 것에 더 많은 가중.치를 주는 연산 방식을 말한다

</aside>

**대표식**

<aside>

`Attention(Q, K, V) = softmax(QKᵀ / √d_k) · V`

</aside>

- Q (Query): 지금 내가 보고 싶은 정보
- K (Key): 각각의 정보가 어떤 의미를 갖는지
- V (Value): 실제 정보

---

## 📌 Transformer 란?

<aside>

Attention을 활용한 **완전한 인코더-디코더 구조의 모델**이다. **Self-Attention을 여러 개의 층으로 쌓고**, 중간에 LayerNorm, Feedforward, Residual Connection을 더해 만든 아키텍처 

</aside>

- **Self-Attention**
    
    입력 시퀀스 내의 토큰(단어, 형태소 등) 간의 **관계를 파악하고 각 토큰의 중요도를 가중치**로 나타내는 메커니즘이다. 즉, **입력 시퀀스 내부의 요소들끼리 서로에게 얼마나 집중해야 하는지**를 계산한다.
    

### 🔑 Transformer Encoder의 구성요소

<aside>

**`[Input]
↓
[Multi-Head Self Attention]
↓
[Feedforward Layer]
↓
[LayerNorm + Residual] x L layers
↓
[Output Embedding]`**

</aside>

## 2️⃣ Attention-Guided Fusion 은 모델일까, 구조일까?

> 모델이 아니라, **여러 정보를 통합(fusion)할 때 attention을 이용해 결합하는 방식**을 말한다
> 

| 용어 | 의미 |
| --- | --- |
| Fusion (융합) | 서로 다른 두 입력 (예: RGB와 Optical Flow)을 하나로 합치는 것 |
| Attention-Guided | 그 융합을 단순 평균이 아닌 **중요도 기반 가중치**로 합치는 것 |

### 🎥 예: Dashcam 영상 분석

- `A`: RGB 이미지의 객체 feature
- `B`: Optical Flow (움직임 정보)
- `α`: attention map (예: 시선 정보, 거리, 속도 등으로 계산된 중요도)

> 👉 `Attention-Guided Fusion`을 적용하면:
> 

<aside>

`fused = α * A + (1 - α) * B`

</aside>

⇒ A가 더 중요하면 A에 더 큰 비중을 주고, B가 더 중요하면 B에 더 큰 비중을 주는 방식이다

---

## 3️⃣ optical flow에 대해 자세한 설명

<aside>

두 프레임 사이에서 픽셀들이 어디로 얼마나 이동했는지를 나타내는 움직임 벡터

</aside>

### 🎬 상상해보세요:

1. 자동차가 오른쪽으로 움직이는 영상을 본다고 가정해요.
2. **프레임1**: 자동차가 왼쪽에 있음
3. **프레임2**: 자동차가 오른쪽으로 조금 이동함

⇒ 이때 Optical Flow는,각 픽셀이 **프레임1 → 프레임2 사이에서 얼마나 이동했는지**

를 벡터 형태로 나타낸다.

<aside>

`(픽셀 위치) → (움직임 방향 + 크기)
예: (50, 100) → (3픽셀 오른쪽, 0픽셀 아래)`

</aside>

## 🧠 수식으로 보면

가장 대표적인 방법은 **Farneback Optical Flow** 또는 **Lucas-Kanade** 기법이고,

핵심 아이디어는 아래와 같다. 

> "물체는 짧은 시간 안에는 갑자기 모양이나 색이 크게 변하지 않는다.
> 
> 
> 그렇다면 픽셀의 밝기(intensity)는 움직여도 거의 비슷해야 하지 않을까?"
> 

## 📊 시각화 예시

Optical Flow는 종종 **컬러 화살표 맵**으로 시각화돼요:

- **방향**: 색상(Hue)
- **속도 크기**: 밝기(Value)

> optical flow의 **방향**은 “색상”으로 **속도의 크기**는 “밝기”로 표현하면 사람 눈이 빠르게 이해할 수 있기 때문
> 

---

# 📌 Transformer는 왜 과거/현재/미래를 모두 보고 사고 여부를 판단하는 모델인가?

> **입력 시퀀스 전체**를 한꺼번에 처리하기 때문에, 과거, 현재, 미래 모든 시점을 동시에 고려하는 구조이다
> 

🔑 하지만 사고 예측이라는 건 사실 **“실제 미래를 모르고”** 해야 실용적이기 때문에 Transformer는 미래 프레임까지 본 다음에 예측하는 방식이라 **“실제 실시간 사고 예측”**에서는 바로 쓰기 어려움 

<aside>

Transforme를 사고 예측에 맞게 **‘미래를 안 보게’** 수정 가능하다 

</aside>

✅ 구조를 살짝 변경하거나 옵션만 추가하면 가능하다. 그리고 이걸 **"Causal Masking"** 또는 **"Attention Masking"이라 한다** 

---

# 1️⃣ Optical Flow란?

> 영상에서 픽셀이 시간에 따라 **어디로 얼마나 움직였는지**를 계산하는 것
> 
- 밝기나 패턴이 이동한 방향(벡터)을 추정함
    - 단순히 밝기의 차이만 보는 게 아니라, **밝기의 “공간적/시간적 변화율”**을 같이 계산
- 영상 2장, (t, t+1) 사이에서 계산

# 2️⃣ Farneback Optical Flow란?

> **“밀집(Dense) Optical Flow”**를 빠르고 부드럽게 계산하는 고전적인 방법
> 
- 영상 전체에 대해 모든 픽셀의 움직임을 계산
- 특히 부드러운 움직임(smooth motion)을 잘 포착하는 게 특징

# 🧠 Farneback 방법의 핵심 아이디어

<aside>

**로컬 지역을 2차 함수(Quadratic Polynomial)**로 근사한다

</aside>

Farneback은 영상의 아주 작은 부분(패치)을 보면, 그 밝기 패턴은 어느 정도 매끄럽고 부드럽다고 가정한다. 

- 2차 다항식 근사: 각 픽셀 주변의 작은 영역(패치) 내에서 영상의 밝기 분포를 2차 다항식으로 근사
    - **패치(Patch)**란?
        
        영상의 작은 직사각형 영역을 의미한다. 이미지 분석이나 처리 과정에서 특정 작업을 수행하기 위해 이미지 전체가 아닌 일부분을 잘라내어 사용하는 경우가 많으며, 이때 잘라낸 작은 영역을 말한다.
        
- 이동 벡터 추정: 연속된 두 프레임에서 각 픽셀 주변 영역의 다항식 계수 변화를 분석하여 해당 픽셀의 이동 벡터를 추정
- 다단계 피라미드: 큰 움직임을 감지하기 위해 영상의 해상도를 점진적으로 낮춘 피라미드 구조를 사용한다. 낮은 해상도에서 큰 움직임을 먼저 추정한 후, 이를 바탕으로 높은 해상도에서 더 정확한 움직임을 추정

| 단계 | 설명 |
| --- | --- |
| 1. 패치 단위로 | 각 패치 밝기 패턴을 2차 다항식으로 근사 |
| 2. 이동 가정 | 두 프레임 사이에서 이 패치가 약간 이동했다고 가정  |
| 3. 변위 계산 | 2차 함수 간 차이를 분석해서 변위(움직임)을 추정 |
| 4. 전체 결합 | 모든 패치의 결과를 합쳐서 전체 Optical Flow Field를 생성 |