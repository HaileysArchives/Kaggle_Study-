{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGFio6_cblWC"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NCHaeRMblWD"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "nexar_collision_prediction_path = kagglehub.competition_download('nexar-collision-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN6-jqnMblWE"
      },
      "source": [
        "## **💡Reference Notebook - Fernandosr85 - Dashcam Collision Prediction Project 🚗**\n",
        "#### **https://www.kaggle.com/code/fernandosr85/dashcam-collision-prediction-project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-20T04:17:54.006303Z",
          "iopub.status.busy": "2025-04-20T04:17:54.006111Z",
          "iopub.status.idle": "2025-04-20T04:17:59.507284Z",
          "shell.execute_reply": "2025-04-20T04:17:59.506332Z",
          "shell.execute_reply.started": "2025-04-20T04:17:54.006255Z"
        },
        "id": "arqneDNWblWF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "max_files = 10\n",
        "\n",
        "count = 0\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "        count += 1\n",
        "        if count >= max_files:\n",
        "            break\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:17:59.50899Z",
          "iopub.status.busy": "2025-04-20T04:17:59.508721Z",
          "iopub.status.idle": "2025-04-20T04:18:07.367697Z",
          "shell.execute_reply": "2025-04-20T04:18:07.366956Z",
          "shell.execute_reply.started": "2025-04-20T04:17:59.508975Z"
        },
        "id": "YIAQ2Xh0blWG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.models\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Check GPU availability and set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.368815Z",
          "iopub.status.busy": "2025-04-20T04:18:07.36844Z",
          "iopub.status.idle": "2025-04-20T04:18:07.414909Z",
          "shell.execute_reply": "2025-04-20T04:18:07.414325Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.368793Z"
        },
        "id": "SBSjDWaublWG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Suppress unnecessary formatting warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# Paths to the CSV files\n",
        "train_csv_path = '/kaggle/input/nexar-collision-prediction/train.csv'\n",
        "test_csv_path = '/kaggle/input/nexar-collision-prediction/test.csv'\n",
        "submission_csv_path = '/kaggle/input/nexar-collision-prediction/sample_submission.csv'\n",
        "\n",
        "# Load the CSV files\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "submission_df = pd.read_csv(submission_csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrames\n",
        "print(\"Train.csv:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTest.csv:\")\n",
        "print(test_df.head())\n",
        "\n",
        "print(\"\\nSample Submission:\")\n",
        "print(submission_df.head())\n",
        "\n",
        "# Optional: handle NaN values if needed, filling with zero or another value\n",
        "train_df['time_of_event'] = train_df['time_of_event'].fillna(0)\n",
        "train_df['time_of_alert'] = train_df['time_of_alert'].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0cpLiI9blWH"
      },
      "source": [
        "## **Data Preprocessing and Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.416443Z",
          "iopub.status.busy": "2025-04-20T04:18:07.416189Z",
          "iopub.status.idle": "2025-04-20T04:18:07.438567Z",
          "shell.execute_reply": "2025-04-20T04:18:07.43764Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.416421Z"
        },
        "id": "sFS0o5sOblWH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 일반적으로 충돌이 발생하는 마지막 부분에 초점을 맞춰 비디오에서 주요 프레임을 추출\n",
        "# 지수 분포를 사용하여 마지막에 가까운 프레임에 더 많은 가중치를 부여\n",
        "\n",
        "def extract_keyframes(video_path, num_frames=12, target_size=(160, 160)):\n",
        "    \"\"\"\n",
        "    Extracts key frames from the video, focusing on the final part where collisions typically occur.\n",
        "    Uses exponential distribution to give more weight to frames closer to the end.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path) # 동영상을 불러오기 위해 OpenCV의 videoCapture 객체 생성\n",
        "\n",
        "    # 파일이 제대로 열리지 않았을 경우 대비한 예외 처리\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Could not open the video: {video_path}\")\n",
        "        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n",
        "\n",
        "    # 총 프레임 수와 초당 프레임 수(FPS)를 가져오기\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    if total_frames <= 0:\n",
        "        print(f\"Video without frames: {video_path}\")\n",
        "        cap.release()\n",
        "        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n",
        "\n",
        "    # 영상 길이(초 단위) 계산\n",
        "    duration = total_frames / fps if fps > 0 else 0\n",
        "\n",
        "    # 짧은 영상 (10초 미만): 균등한 간격으로 프레임 추출\n",
        "    if duration < 10:\n",
        "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    # 긴 영상 (10초 이상): 후반부에 더 집중해서 추출\n",
        "    else:\n",
        "        # 마지막 3초 동안 프레임의 80% 집중(중요 영역)\n",
        "        end_frames = int(num_frames * 0.8)\n",
        "        start_frames = num_frames - end_frames\n",
        "\n",
        "        # 지난 3초 동안의 시작 인덱스를 계산\n",
        "        last_seconds = 3\n",
        "        last_frame_count = min(int(fps * last_seconds), total_frames - 1)\n",
        "        start_idx = max(0, total_frames - last_frame_count)\n",
        "\n",
        "        # 마지막 프레임에 더 많은 가중치를 부여하는 지수 분포\n",
        "        # 이렇게 하면 마지막에 더 밀집된 인덱스가 생성된다 (\"프레임을 뽑는 간격\"자체를 조절 → 끝부분에 더 많이 몰리게 만드는 방식)\n",
        "        end_indices = np.array([\n",
        "            start_idx + int((total_frames - start_idx - 1) * (i/end_frames)**2)\n",
        "            for i in range(1, end_frames + 1)\n",
        "        ])\n",
        "\n",
        "        # context에 맞게 균일하게 배포된 초기 프레임 (초반부에서 균등하게 추출한 프레임들)\n",
        "        # context란? 사고 직전에 어떤 상황이 펼쳐졌는지에 대한 흐름, 배경, 맥락\n",
        "        begin_indices = np.linspace(0, start_idx - 1, start_frames, dtype=int) if start_idx > 0 else np.zeros(start_frames, dtype=int)\n",
        "\n",
        "        # 인덱스 결합\n",
        "        frame_indices = np.concatenate([begin_indices, end_indices])\n",
        "\n",
        "    # 선택한 프레임 추출\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            # Use higher resolution and better interpolation\n",
        "            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        else:\n",
        "            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames, dtype=np.uint8)\n",
        "\n",
        "# 먼저, 전역 범위에서 변환 클래스를 정의\n",
        "# 입력된 영상 프레임을 일정 확률로 좌우 반전시켜서, 데이터 다양성을 늘리는 역할\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        if np.random.random() < self.p:\n",
        "            return frames[:, :, ::-1, :].copy()  # horizontally flip each frame\n",
        "        return frames\n",
        "\n",
        "# 영상 프레임의 밝기와 대비를 무작위로 조정해, 다양한 조명 환경을 시뮬레이션하는 증강 클래스\n",
        "class ColorJitter(object):\n",
        "    def __init__(self, brightness=0, contrast=0):\n",
        "        self.brightness = brightness\n",
        "        self.contrast = contrast\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        # Apply brightness jitter\n",
        "        if self.brightness > 0:\n",
        "            brightness_factor = np.random.uniform(max(0, 1-self.brightness), 1+self.brightness)\n",
        "            frames = frames * brightness_factor\n",
        "            frames = np.clip(frames, 0, 255)\n",
        "\n",
        "        # Apply contrast jitter\n",
        "        if self.contrast > 0:\n",
        "            contrast_factor = np.random.uniform(max(0, 1-self.contrast), 1+self.contrast)\n",
        "            frames = (frames - 128) * contrast_factor + 128\n",
        "            frames = np.clip(frames, 0, 255)\n",
        "\n",
        "        return frames\n",
        "\n",
        "# 프레임에 흐릿한 안개 효과를 넣어, 시야가 나쁜 날씨 상황을 시뮬레이션하는 클래스\n",
        "class AddFog(object):\n",
        "    def __call__(self, frames):\n",
        "        fog = np.random.uniform(0.7, 0.9, frames.shape).astype(np.float32)\n",
        "        return frames * 0.8 + fog * 50  # Adjusted for 0-255 scale\n",
        "\n",
        "# 프레임에 흰색 선형 노이즈(빗방울)를 추가해 비 오는 날씨를 시뮬레이션하는 클래스\n",
        "class AddRain(object):\n",
        "    def __call__(self, frames):\n",
        "        h, w = frames.shape[1:3]\n",
        "        rain = np.random.uniform(0, 1, (len(frames), h, w, 1)).astype(np.float32)\n",
        "        rain = (rain > 0.97).astype(np.float32) * 200  # White rain drops\n",
        "        return np.clip(frames * 0.9 + rain, 0, 255)  # Darken a bit and add drops\n",
        "\n",
        "# 지정된 확률에 따라 어떤 변환을 적용할지 말지를 무작위로 결정하는 컨트롤러 클래스(랜덤성 부여)\n",
        "class RandomApply(object):\n",
        "    def __init__(self, transform, p=0.5):\n",
        "        self.transform = transform\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        if np.random.random() < self.p:\n",
        "            return self.transform(frames)\n",
        "        return frames\n",
        "\n",
        "# 여러 개의 변환(Flip, Jitter, Fog 등)을 순서대로 적용하는 데이터 증강 파이프라인 클래스\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        for t in self.transforms:\n",
        "            frames = t(frames)\n",
        "        return frames\n",
        "\n",
        "# 영상 프레임 배열을 PyTorch 텐서로 바꾸고, 픽셀 값을 0~1 범위로 정규화하는 클래스\n",
        "class ToTensor(object):\n",
        "    def __call__(self, frames):\n",
        "        # Convert from (T, H, W, C) to (T, C, H, W)\n",
        "        frames = frames.transpose(0, 3, 1, 2)\n",
        "        # Convert to tensor and normalize to [0, 1]\n",
        "        return torch.from_numpy(frames).float() / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.440093Z",
          "iopub.status.busy": "2025-04-20T04:18:07.439866Z",
          "iopub.status.idle": "2025-04-20T04:18:07.455049Z",
          "shell.execute_reply": "2025-04-20T04:18:07.454188Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.440073Z"
        },
        "id": "ykx3ndJRblWI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 동영상에서 데이터 증강을 위한 변환을 반환\n",
        "\n",
        "def get_video_transforms():\n",
        "    \"\"\"\n",
        "    Returns transformations for data augmentation in videos.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'train': Compose([\n",
        "            RandomHorizontalFlip(p=0.5),\n",
        "            ColorJitter(brightness=0.3, contrast=0.3),\n",
        "            RandomApply(AddFog(), p=0.15),\n",
        "            RandomApply(AddRain(), p=0.15),\n",
        "            RandomApply(RandomNoise(0.05), p=0.2),\n",
        "            RandomApply(RandomOcclusion(), p=0.1),\n",
        "            ToTensor()\n",
        "        ]),\n",
        "        'val': Compose([\n",
        "            ToTensor()  # Only tensor conversion for validation\n",
        "        ])\n",
        "    }\n",
        "\n",
        "# 비디오 프레임에서 무작위 가우시안(정규분포) 노이즈를 추가하여, 실제 촬영 환경에서\n",
        "# 발생할 수 있는 잡음에 대해 모델이 더 강건해지도록 만드는 클래스\n",
        "class RandomNoise(object):\n",
        "    \"\"\"\n",
        "    Applies random Gaussian noise to video frames for data augmentation.\n",
        "\n",
        "    This transformation helps the model become more robust to noise\n",
        "    that may be present in real-world video data.\n",
        "\n",
        "    Args:\n",
        "        std (float): Standard deviation of the Gaussian noise as a fraction\n",
        "                     of the pixel value range (default: 0.05)\n",
        "    \"\"\"\n",
        "    def __init__(self, std=0.05):\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        \"\"\"\n",
        "        Apply random noise to the input frames.\n",
        "\n",
        "        Args:\n",
        "            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n",
        "                                   where T is number of frames\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Noise-augmented frames, clipped to valid pixel range [0, 255]\n",
        "        \"\"\"\n",
        "        # 지정된 표준 편차를 가진 가우시안 노이즈 생성\n",
        "        noise = np.random.normal(0, self.std * 255, frames.shape).astype(np.float32)\n",
        "\n",
        "        # 유효한 픽셀 범위에 노이즈 및 클립 추가하기\n",
        "        # 영상은 정수형 데이터여야 하므로 형 변환 (astype)\n",
        "        return np.clip(frames + noise, 0, 255).astype(np.uint8)\n",
        "\n",
        "# 영상 프레임에 검은색 사각형을 무작위로 덮어 씌워, 일부 정보가 가려졌을 때도 모델이 견딜 수 있도록 훈련시키는 클래스\n",
        "class RandomOcclusion(object):\n",
        "    \"\"\"\n",
        "    Simulates occlusion in video frames by adding black rectangles.\n",
        "\n",
        "    This transformation helps the model learn to handle partial occlusions\n",
        "    that may occur in real-world scenarios when objects block the camera view.\n",
        "    \"\"\"\n",
        "    def __call__(self, frames):\n",
        "        \"\"\"\n",
        "        Apply random occlusion to the input frames.\n",
        "\n",
        "        Args:\n",
        "            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n",
        "                                   where T is number of frames\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Frames with random occlusion applied\n",
        "        \"\"\"\n",
        "        # 프레임 하나의 세로(h), 가로(w) 길이 가져오기\n",
        "        h, w = frames.shape[1:3]\n",
        "\n",
        "        # 전체 프레임 크기의 10%~25% 사이 크기의 가림 영역 크기 설정\n",
        "        occl_h = np.random.randint(int(h * 0.1), int(h * 0.25))\n",
        "        occl_w = np.random.randint(int(w * 0.1), int(w * 0.25))\n",
        "\n",
        "        # 이 가림 영역이 들어갈 무작위 위치 좌표 설정\n",
        "        occl_x = np.random.randint(0, w - occl_w)\n",
        "        occl_y = np.random.randint(0, h - occl_h)\n",
        "\n",
        "        # 원본 프레임을 수정하지 않도록 복사본 만들기\n",
        "        frames_copy = frames.copy()\n",
        "\n",
        "        # 픽셀을 0(검정색)으로 설정하여 모든 프레임에 occlusion 적용\n",
        "        for i in range(len(frames)):\n",
        "            frames_copy[i, occl_y:occl_y+occl_h, occl_x:occl_x+occl_w, :] = 0\n",
        "\n",
        "        return frames_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.457304Z",
          "iopub.status.busy": "2025-04-20T04:18:07.457088Z",
          "iopub.status.idle": "2025-04-20T04:18:07.475068Z",
          "shell.execute_reply": "2025-04-20T04:18:07.474327Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.457283Z"
        },
        "id": "eV2-OVDNblWI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 비디오 프레임 간 움직임(모션)을 추적하는 'optical_flow'를 계산해, 객체나 배경의 이동 방향과 속도를 벡터 형태로 반환하는 함수\n",
        "# 두 연속된 이미지(또는 프레임) 사이에서, 각 픽셀이 어떻게 이동했는지를 벡터로 표현하는 기술 -> optical_flow\n",
        "\n",
        "def compute_optical_flow(frames, skip_frames=1):\n",
        "    \"\"\"Calculates optical flow skipping some frames to reduce processing.\"\"\"\n",
        "    if len(frames) < 2:\n",
        "        return np.zeros((1, frames.shape[1], frames.shape[2], 2), dtype=np.float32)\n",
        "\n",
        "    flows = []\n",
        "\n",
        "    # 첫 프레임을 그레이스케일(흑백 이미지)로 변환\n",
        "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # 연속된 프레임 쌍마다 optical_flow 계산\n",
        "    for i in range(1, len(frames), skip_frames):\n",
        "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
        "        try:\n",
        "            # 더 빠른 계산을 위해 매개변수 줄이기\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray,\n",
        "                                               None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            flows.append(flow)\n",
        "        # 예외 처리 (계산 중 오류 발생 시 안전하게 0으로 대체)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating optical flow: {str(e)}\")\n",
        "            flows.append(np.zeros((frames.shape[1], frames.shape[2], 2), dtype=np.float32))\n",
        "\n",
        "        prev_gray = curr_gray\n",
        "\n",
        "    if not flows:\n",
        "        return np.zeros((1, frames.shape[1], frames.shape[2], 2), dtype=np.float32)\n",
        "\n",
        "    return np.array(flows, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.476245Z",
          "iopub.status.busy": "2025-04-20T04:18:07.476046Z",
          "iopub.status.idle": "2025-04-20T04:18:07.496397Z",
          "shell.execute_reply": "2025-04-20T04:18:07.495429Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.476231Z"
        },
        "id": "C5kZLuZmblWI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 하나의 동영상 파일을 받아, 주요 프레임을 추출하고 optical_flow(움직임 정보)를 계산해 구조화된 데이터로 반환하는 전처리 함수\n",
        "\n",
        "def process_video(args):\n",
        "    \"\"\"\n",
        "    Function to process an individual video.\n",
        "    \"\"\"\n",
        "    video_path, video_id, num_frames = args\n",
        "    try:\n",
        "        # 더 높은 해상도로 프레임 추출\n",
        "        frames = extract_keyframes(video_path, num_frames=num_frames, target_size=(160, 160))\n",
        "\n",
        "        # optical flow 계산\n",
        "        optical_flow = compute_optical_flow(frames, skip_frames=1)\n",
        "\n",
        "        # 변환을 적용하는 대신 NumPy 배열을 반환\n",
        "        return video_id, {\n",
        "            'frames': frames,\n",
        "            'optical_flow': optical_flow,\n",
        "        }\n",
        "    # 예외 처리 (None을 리턴)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_id}: {str(e)}\")\n",
        "        return video_id, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.497584Z",
          "iopub.status.busy": "2025-04-20T04:18:07.497243Z",
          "iopub.status.idle": "2025-04-20T04:18:07.514822Z",
          "shell.execute_reply": "2025-04-20T04:18:07.514063Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.497557Z"
        },
        "id": "QzcT6uueblWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 여러 개의 영상을 병렬로 처리해 빠르게 전처리하고, 각 영상의 프레임 및 optical flow 데이터를 하나의 딕셔너리로 정리하는 함수\n",
        "\n",
        "def parallel_preprocess_dataset(video_dir, video_ids, num_frames=8, num_workers=4):\n",
        "    \"\"\"\n",
        "    Pre-processes multiple videos in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    # 비디오 목록 준비 (튜플 형태로 리스트에 추가)\n",
        "    args_list = []\n",
        "    for video_id in video_ids:\n",
        "        video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n",
        "        if os.path.exists(video_path):\n",
        "            args_list.append((video_path, video_id, num_frames))\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(f\"Starting parallel pre-processing of {len(args_list)} videos with {num_workers} workers...\")\n",
        "\n",
        "    processed_data = {}\n",
        "\n",
        "    # 병렬 처리 시작\n",
        "    with Pool(num_workers) as p:\n",
        "        results = p.map(process_video, args_list)\n",
        "        for video_id, data in results:\n",
        "            if data is not None:\n",
        "                processed_data[video_id] = data\n",
        "\n",
        "    print(f\"Pre-processing completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    print(f\"Processed {len(processed_data)} out of {len(args_list)} videos.\")\n",
        "\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.515891Z",
          "iopub.status.busy": "2025-04-20T04:18:07.515672Z",
          "iopub.status.idle": "2025-04-20T04:18:07.702243Z",
          "shell.execute_reply": "2025-04-20T04:18:07.70132Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.515872Z"
        },
        "id": "rD44YqbwblWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# mp4 비디오 파일에서 직접 프레임과 optical flow를 추출하며 학습 데이터를 생성\n",
        "class DashcamDataset(Dataset):\n",
        "   def __init__(self, video_dir, annotations, transform=None, num_frames=8):\n",
        "       self.video_dir = video_dir\n",
        "       self.annotations = annotations\n",
        "       self.transform = transform\n",
        "       self.num_frames = num_frames\n",
        "       self.video_ids = list(annotations.keys())\n",
        "\n",
        "   def __len__(self):\n",
        "       return len(self.video_ids)\n",
        "\n",
        "   def __getitem__(self, idx):\n",
        "       video_id = self.video_ids[idx]\n",
        "       video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "       try:\n",
        "           # Check if the file exists\n",
        "           if not os.path.exists(video_path):\n",
        "               print(f\"Video not found: {video_path}\")\n",
        "               raise FileNotFoundError(f\"File not found: {video_path}\")\n",
        "\n",
        "           # Extract frames with reduced resolution\n",
        "           frames = extract_keyframes(video_path, self.num_frames, target_size=(112, 112))\n",
        "\n",
        "           # Calculate optical flow with skip_frames\n",
        "           optical_flow = compute_optical_flow(frames, skip_frames=1)\n",
        "\n",
        "           # Apply transformations\n",
        "           if self.transform:\n",
        "               frames = self.transform(frames)\n",
        "           else:\n",
        "               # Convert to tensor manually\n",
        "               frames = torch.from_numpy(frames.transpose(0, 3, 1, 2)).float() / 255.0\n",
        "\n",
        "           # Convert optical flow to tensor\n",
        "           optical_flow = torch.from_numpy(optical_flow.transpose(0, 3, 1, 2)).float()\n",
        "\n",
        "           # Load label and alert time\n",
        "           label = self.annotations[video_id]['label']\n",
        "           alert_time = self.annotations[video_id].get('alert_time', 0)\n",
        "\n",
        "           return {\n",
        "               'frames': frames,\n",
        "               'optical_flow': optical_flow,\n",
        "               'label': torch.tensor(label).float(),\n",
        "               'alert_time': torch.tensor(alert_time).float(),\n",
        "               'video_id': video_id\n",
        "           }\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error processing video {video_id}: {str(e)}\")\n",
        "           # Create a placeholder for this video\n",
        "           dummy_frames = torch.zeros((self.num_frames, 3, 112, 112))\n",
        "           dummy_flow = torch.zeros((max(1, self.num_frames-1), 2, 112, 112))\n",
        "           return {\n",
        "               'frames': dummy_frames,\n",
        "               'optical_flow': dummy_flow,\n",
        "               'label': torch.tensor(self.annotations[video_id]['label']).float(),\n",
        "               'alert_time': torch.tensor(self.annotations[video_id].get('alert_time', 0)).float(),\n",
        "               'video_id': video_id\n",
        "           }\n",
        "\n",
        "# 이미 전처리된 결과(NumPy 배열)를 메모리에서 불러와서 빠르게 학습 데이터를 생성\n",
        "class PreprocessedDashcamDataset(Dataset):\n",
        "   def __init__(self, processed_data, annotations, transform=None):\n",
        "       self.processed_data = processed_data\n",
        "       self.annotations = annotations\n",
        "       self.transform = transform\n",
        "       self.video_ids = list(processed_data.keys())\n",
        "\n",
        "   def __len__(self):\n",
        "       return len(self.video_ids)\n",
        "\n",
        "   def __getitem__(self, idx):\n",
        "       video_id = self.video_ids[idx]\n",
        "       data = self.processed_data[video_id]\n",
        "\n",
        "       # Get frames and optical flow\n",
        "       frames = data['frames']\n",
        "       optical_flow = data['optical_flow']\n",
        "\n",
        "       # Apply transformations to frames\n",
        "       if self.transform:\n",
        "           frames_tensor = self.transform(frames)\n",
        "       else:\n",
        "           # Convert to tensor manually\n",
        "           frames_tensor = torch.from_numpy(frames.transpose(0, 3, 1, 2)).float() / 255.0\n",
        "\n",
        "       # Convert optical flow to tensor\n",
        "       optical_flow_tensor = torch.from_numpy(optical_flow.transpose(0, 3, 1, 2)).float()\n",
        "\n",
        "       # Load label and alert time\n",
        "       label = self.annotations[video_id]['label']\n",
        "       alert_time = self.annotations[video_id].get('alert_time', 0)\n",
        "\n",
        "       return {\n",
        "           'frames': frames_tensor,\n",
        "           'optical_flow': optical_flow_tensor,\n",
        "           'label': torch.tensor(label).float(),\n",
        "           'alert_time': torch.tensor(alert_time).float(),\n",
        "           'video_id': video_id\n",
        "       }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKdwMpeoblWJ"
      },
      "source": [
        "## **AAT-DA 모델 입력 생성을 위한 데이터 처리**\n",
        "#### **필수 구성요소: Object Detection, VGG Feature 추출, Driver Attention 맵 계산, Attention Weight 계산**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:45:41.906022Z",
          "iopub.status.busy": "2025-04-20T04:45:41.905755Z",
          "iopub.status.idle": "2025-04-20T04:45:41.9129Z",
          "shell.execute_reply": "2025-04-20T04:45:41.911729Z",
          "shell.execute_reply.started": "2025-04-20T04:45:41.906004Z"
        },
        "id": "jMuWHfTYblWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 외부에서 object_detector와 driver_attention_model을 인자로 주입받도록 설계\n",
        "\n",
        "# 더미 객체 탐지기 클래스\n",
        "class ObjectDetector:\n",
        "    def detect(self, frame):\n",
        "        h, w, _ = frame.shape\n",
        "        return [(w//3, h//3, 2*w//3, 2*h//3)]  # 테스트용 더미 박스\n",
        "\n",
        "# 더미 시선 예측 모델 클래스\n",
        "class DummyDriverAttention:\n",
        "    def predict(self, frame):\n",
        "        h, w, _ = frame.shape\n",
        "        attention_map = np.zeros((h, w), dtype=np.float32)\n",
        "        cx, cy = w // 2, h // 2\n",
        "        attention_map[cy-10:cy+10, cx-10:cx+10] = 1.0\n",
        "        return cv2.GaussianBlur(attention_map, (25, 25), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.703409Z",
          "iopub.status.busy": "2025-04-20T04:18:07.703171Z",
          "iopub.status.idle": "2025-04-20T04:18:07.725727Z",
          "shell.execute_reply": "2025-04-20T04:18:07.724961Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.703391Z"
        },
        "id": "MffQj4R4blWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import models, transforms\n",
        "\n",
        "class AATDAPreprocessor:\n",
        "    def __init__(self, object_detector, driver_attention_model, vgg_model=None):\n",
        "        self.detector = object_detector  # Cascade R-CNN 등 외부 감지기\n",
        "        self.attention_model = driver_attention_model  # Gate-DAP 등\n",
        "        self.vgg = vgg_model or models.vgg16(pretrained=True).features.eval()\n",
        "        self.vgg_fc7 = torch.nn.Sequential(*list(models.vgg16(pretrained=True).classifier.children())[:6]).eval()\n",
        "        self.vgg_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # 감지된 객체 영역을 잘라서 VGG FC7 feature(4096-dim) 추출\n",
        "    def extract_object_features(self, frame, boxes):\n",
        "        features = []\n",
        "        for (x1, y1, x2, y2) in boxes:\n",
        "            crop = frame[y1:y2, x1:x2]\n",
        "            if crop.shape[0] == 0 or crop.shape[1] == 0:\n",
        "                features.append(torch.zeros(4096))\n",
        "                continue\n",
        "            input_tensor = self.vgg_transform(crop).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                feat = self.vgg(input_tensor)\n",
        "                feat = feat.view(feat.size(0), -1)\n",
        "                fc7 = self.vgg_fc7(feat).squeeze(0)\n",
        "            features.append(fc7)\n",
        "        return torch.stack(features)  # Shape: (N, 4096)\n",
        "\n",
        "    # attention map과 객체 영역이 얼마나 겹치는지 평균값으로 가중치 계산\n",
        "    def compute_attention_weights(self, attention_map, boxes):\n",
        "        weights = []\n",
        "        for (x1, y1, x2, y2) in boxes:\n",
        "            patch = attention_map[y1:y2, x1:x2]\n",
        "            weight = np.mean(patch) if patch.size > 0 else 0.0\n",
        "            weights.append(weight)\n",
        "        return torch.tensor(weights, dtype=torch.float32)  # Shape: (N,)\n",
        "\n",
        "    # 한 프레임에 대해 아래와 같이 순서대로 작동\n",
        "    def process_frame(self, frame):\n",
        "        # 1. 객체 감지\n",
        "        boxes = self.detector.detect(frame)  # returns List of [x1, y1, x2, y2]\n",
        "\n",
        "        # 2. VGG Feature 추출\n",
        "        object_feats = self.extract_object_features(frame, boxes)  # (N, 4096)\n",
        "\n",
        "        # 3. 시선 맵 생성\n",
        "        attention_map = self.attention_model.predict(frame)  # (H, W), numpy float32\n",
        "\n",
        "        # 4. Attention Weight 계산\n",
        "        weights = self.compute_attention_weights(attention_map, boxes)  # (N,)\n",
        "\n",
        "        # 5. Feature 강화\n",
        "        weighted_feats = object_feats * weights.unsqueeze(1)  # (N, 4096)\n",
        "\n",
        "        return weighted_feats, weights, boxes  # Transformer 입력으로 사용할 수 있음\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnpF-ZQnblWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 실행 예시\n",
        "if __name__ == '__main__':\n",
        "    detector = ObjectDetector()\n",
        "    attention_model = DummyDriverAttention()\n",
        "    processor = AATDAPreprocessor(detector, attention_model)\n",
        "\n",
        "    # 테스트용 이미지 불러오기\n",
        "    frame = cv2.imread(\"sample_frame.jpg\")  # 실제 경로로 작성하기!!!\n",
        "    if frame is not None:\n",
        "        weighted_feats, weights, boxes = processor.process_frame(frame)\n",
        "        print(\"감지된 객체 수:\", len(boxes))\n",
        "        print(\"Feature shape:\", weighted_feats.shape)  # (N, 4096)\n",
        "    else:\n",
        "        print(\"이미지를 불러올 수 없습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.726751Z",
          "iopub.status.busy": "2025-04-20T04:18:07.726501Z",
          "iopub.status.idle": "2025-04-20T04:18:07.748259Z",
          "shell.execute_reply": "2025-04-20T04:18:07.747283Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.726729Z"
        },
        "id": "oKA0CXEIblWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 비디오 하나에 포함된 여러 프레임을 process_frame()으로 반복 처리\n",
        "# 각 프레임의 객체 feature를 (T, N, 4096) 구조로 패딩하여 Transformer 입력에 맞춤\n",
        "\n",
        "class AATDADataset(Dataset):\n",
        "    def __init__(self, video_data_dict, annotations, processor, num_frames=8):\n",
        "        self.video_data = video_data_dict  # {video_id: [frame1, frame2, ...]}\n",
        "        self.annotations = annotations  # {video_id: {label: ..., alert_time: ...}}\n",
        "        self.processor = processor\n",
        "        self.num_frames = num_frames\n",
        "        self.video_ids = list(video_data_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.video_ids[idx]\n",
        "        frames = self.video_data[video_id]  # raw RGB frames\n",
        "\n",
        "        # 각 프레임에 대해 Object Feature + Attention 처리\n",
        "        sequence_features = []\n",
        "        for frame in frames:\n",
        "            weighted_feats, _, _ = self.processor.process_frame(frame)\n",
        "            sequence_features.append(weighted_feats)\n",
        "\n",
        "        # 시퀀스를 (T, N, 4096) 형태의 tensor로 정리 (padding 가능)\n",
        "        max_objects = max(f.shape[0] for f in sequence_features)\n",
        "        padded_seq = []\n",
        "        for f in sequence_features:\n",
        "            pad_size = max_objects - f.shape[0]\n",
        "            if pad_size > 0:\n",
        "                padded = torch.cat([f, torch.zeros(pad_size, 4096)], dim=0)\n",
        "            else:\n",
        "                padded = f\n",
        "            padded_seq.append(padded)\n",
        "\n",
        "        # 최종 시퀀스: (T, N, 4096)\n",
        "        input_tensor = torch.stack(padded_seq)\n",
        "\n",
        "        label = torch.tensor(self.annotations[video_id]['label']).float()\n",
        "        alert_time = torch.tensor(self.annotations[video_id].get('alert_time', 0)).float()\n",
        "\n",
        "        return {\n",
        "            'video_id': video_id,\n",
        "            'input': input_tensor,  # (T, N, 4096)\n",
        "            'label': label,\n",
        "            'alert_time': alert_time\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBDUYnfQblWK"
      },
      "source": [
        "### **1단계: 일반 전처리**\n",
        "#### **(1) 프레임 추출 - extract_keyframes()**\n",
        "#### **(2) Optical Flow 계산 - compute_optical_flow()**\n",
        "#### **(3) 기본 증강 - ColorJitter, AddRain, AddFog, ToTensor() 등**\n",
        "#### **(4) 데이터 구성 - DashcamDataset 또는 PreprocessDashcamDataset으로 구성**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3It0odGdblWK"
      },
      "source": [
        "### **2단계: AAT-DA 전용 전처리 (기존 데이터에서 Transformer용 구조 변환)**\n",
        "##### AAT-DA는 단순한 영상 프레임이 아니라, \"객체 중심의 시공간 Attention 입력 구조\"를 요구하기 때문에 기존 전처리된 데이터를 바탕으로 추가적인 전처리가 필요\n",
        "#### **(1) 객체 감지 - 프레임에서 객체 감지 (Cascade R-CNN 등)**\n",
        "#### **(2) 객체 특징 추출 - 감지된 박스마다 VGG16 FC7 feature 추출 (4096-dim)**\n",
        "#### **(3) 시선 맵 예측 - Gate-DAP 등으로 driver attention heatmap 생성**\n",
        "#### **(4) 주의 가중치 계산 - 시선 맵 + 객체 위치 → 객체별 attention weight αᵢ 계산**\n",
        "#### **(5) Feature 가중 - 객체 feature αᵢ → 강조된 객체 feature**\n",
        "#### **(6) 시퀀스 구성 - 모든 프레임의 결과를 (T, N, 4096) 시퀀스로 패딩 정리**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:49:20.530493Z",
          "iopub.status.busy": "2025-04-20T04:49:20.530191Z",
          "iopub.status.idle": "2025-04-20T04:49:20.538492Z",
          "shell.execute_reply": "2025-04-20T04:49:20.537799Z",
          "shell.execute_reply.started": "2025-04-20T04:49:20.53047Z"
        },
        "id": "iNlczjmdblWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# AAT-DA 모델 구성\n",
        "# 입력: (B, T, N, 4096)\n",
        "# 구성: Spatial Transformer + Temporal Transformer + Classifier\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 각 프레임 내 객체들 간 상호작용 학습 (Object Self-Attention)\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048):\n",
        "        super().__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, N, 4096)\n",
        "        B, T, N, C = x.shape\n",
        "        x = x.view(B * T, N, C)  # (B*T, N, C)\n",
        "        x = self.encoder(x)  # (B*T, N, C)\n",
        "        x = x.mean(dim=1)  # (B*T, C)\n",
        "        x = x.view(B, T, C)  # (B, T, C)\n",
        "        return x\n",
        "\n",
        "# 시간 순서에 따른 의미 흐름 모델링 (Temporal Attention)\n",
        "class TemporalTransformer(nn.Module):\n",
        "    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048):\n",
        "        super().__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, C)\n",
        "        x = x.transpose(0, 1)  # (T, B, C)\n",
        "        x = self.encoder(x)  # (T, B, C)\n",
        "        x = x.transpose(0, 1)  # (B, T, C)\n",
        "        return x\n",
        "\n",
        "class AATDA(nn.Module):\n",
        "    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.spatial_transformer = SpatialTransformer(d_model, nhead, dim_feedforward)\n",
        "        self.temporal_transformer = TemporalTransformer(d_model, nhead, dim_feedforward)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, N, 4096)\n",
        "        x = self.spatial_transformer(x)     # (B, T, 4096)\n",
        "        x = self.temporal_transformer(x)    # (B, T, 4096)\n",
        "        x = x.mean(dim=1)                   # (B, 4096) - 평균 풀링\n",
        "        out = self.classifier(x)            # (B, 1)\n",
        "        return out.squeeze(-1)              # (B,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FsQfdj3blWL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 데이터 로더 연결\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 초기화 및 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 함수 작성\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        inputs = batch['input'].to(device)         # (B, T, N, 4096)\n",
        "        labels = batch['label'].to(device).float() # (B,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)                    # (B,)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 11038207,
          "sourceId": 92399,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
