{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGFio6_cblWC"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NCHaeRMblWD"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "nexar_collision_prediction_path = kagglehub.competition_download('nexar-collision-prediction')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN6-jqnMblWE"
      },
      "source": [
        "## **ğŸ’¡Reference Notebook - Fernandosr85 - Dashcam Collision Prediction Project ğŸš—**\n",
        "#### **https://www.kaggle.com/code/fernandosr85/dashcam-collision-prediction-project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-20T04:17:54.006303Z",
          "iopub.status.busy": "2025-04-20T04:17:54.006111Z",
          "iopub.status.idle": "2025-04-20T04:17:59.507284Z",
          "shell.execute_reply": "2025-04-20T04:17:59.506332Z",
          "shell.execute_reply.started": "2025-04-20T04:17:54.006255Z"
        },
        "id": "arqneDNWblWF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "max_files = 10\n",
        "\n",
        "count = 0\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "        count += 1\n",
        "        if count >= max_files:\n",
        "            break\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:17:59.50899Z",
          "iopub.status.busy": "2025-04-20T04:17:59.508721Z",
          "iopub.status.idle": "2025-04-20T04:18:07.367697Z",
          "shell.execute_reply": "2025-04-20T04:18:07.366956Z",
          "shell.execute_reply.started": "2025-04-20T04:17:59.508975Z"
        },
        "id": "YIAQ2Xh0blWG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.models\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Check GPU availability and set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.368815Z",
          "iopub.status.busy": "2025-04-20T04:18:07.36844Z",
          "iopub.status.idle": "2025-04-20T04:18:07.414909Z",
          "shell.execute_reply": "2025-04-20T04:18:07.414325Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.368793Z"
        },
        "id": "SBSjDWaublWG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Suppress unnecessary formatting warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# Paths to the CSV files\n",
        "train_csv_path = '/kaggle/input/nexar-collision-prediction/train.csv'\n",
        "test_csv_path = '/kaggle/input/nexar-collision-prediction/test.csv'\n",
        "submission_csv_path = '/kaggle/input/nexar-collision-prediction/sample_submission.csv'\n",
        "\n",
        "# Load the CSV files\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "submission_df = pd.read_csv(submission_csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrames\n",
        "print(\"Train.csv:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTest.csv:\")\n",
        "print(test_df.head())\n",
        "\n",
        "print(\"\\nSample Submission:\")\n",
        "print(submission_df.head())\n",
        "\n",
        "# Optional: handle NaN values if needed, filling with zero or another value\n",
        "train_df['time_of_event'] = train_df['time_of_event'].fillna(0)\n",
        "train_df['time_of_alert'] = train_df['time_of_alert'].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0cpLiI9blWH"
      },
      "source": [
        "## **Data Preprocessing and Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.416443Z",
          "iopub.status.busy": "2025-04-20T04:18:07.416189Z",
          "iopub.status.idle": "2025-04-20T04:18:07.438567Z",
          "shell.execute_reply": "2025-04-20T04:18:07.43764Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.416421Z"
        },
        "id": "sFS0o5sOblWH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ì¼ë°˜ì ìœ¼ë¡œ ì¶©ëŒì´ ë°œìƒí•˜ëŠ” ë§ˆì§€ë§‰ ë¶€ë¶„ì— ì´ˆì ì„ ë§ì¶° ë¹„ë””ì˜¤ì—ì„œ ì£¼ìš” í”„ë ˆì„ì„ ì¶”ì¶œ\n",
        "# ì§€ìˆ˜ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ì— ê°€ê¹Œìš´ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬\n",
        "\n",
        "def extract_keyframes(video_path, num_frames=12, target_size=(160, 160)):\n",
        "    \"\"\"\n",
        "    Extracts key frames from the video, focusing on the final part where collisions typically occur.\n",
        "    Uses exponential distribution to give more weight to frames closer to the end.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path) # ë™ì˜ìƒì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ OpenCVì˜ videoCapture ê°ì²´ ìƒì„±\n",
        "\n",
        "    # íŒŒì¼ì´ ì œëŒ€ë¡œ ì—´ë¦¬ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Could not open the video: {video_path}\")\n",
        "        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n",
        "\n",
        "    # ì´ í”„ë ˆì„ ìˆ˜ì™€ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜(FPS)ë¥¼ ê°€ì ¸ì˜¤ê¸°\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    if total_frames <= 0:\n",
        "        print(f\"Video without frames: {video_path}\")\n",
        "        cap.release()\n",
        "        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)\n",
        "\n",
        "    # ì˜ìƒ ê¸¸ì´(ì´ˆ ë‹¨ìœ„) ê³„ì‚°\n",
        "    duration = total_frames / fps if fps > 0 else 0\n",
        "\n",
        "    # ì§§ì€ ì˜ìƒ (10ì´ˆ ë¯¸ë§Œ): ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì¶”ì¶œ\n",
        "    if duration < 10:\n",
        "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    # ê¸´ ì˜ìƒ (10ì´ˆ ì´ìƒ): í›„ë°˜ë¶€ì— ë” ì§‘ì¤‘í•´ì„œ ì¶”ì¶œ\n",
        "    else:\n",
        "        # ë§ˆì§€ë§‰ 3ì´ˆ ë™ì•ˆ í”„ë ˆì„ì˜ 80% ì§‘ì¤‘(ì¤‘ìš” ì˜ì—­)\n",
        "        end_frames = int(num_frames * 0.8)\n",
        "        start_frames = num_frames - end_frames\n",
        "\n",
        "        # ì§€ë‚œ 3ì´ˆ ë™ì•ˆì˜ ì‹œì‘ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°\n",
        "        last_seconds = 3\n",
        "        last_frame_count = min(int(fps * last_seconds), total_frames - 1)\n",
        "        start_idx = max(0, total_frames - last_frame_count)\n",
        "\n",
        "        # ë§ˆì§€ë§‰ í”„ë ˆì„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì§€ìˆ˜ ë¶„í¬\n",
        "        # ì´ë ‡ê²Œ í•˜ë©´ ë§ˆì§€ë§‰ì— ë” ë°€ì§‘ëœ ì¸ë±ìŠ¤ê°€ ìƒì„±ëœë‹¤ (\"í”„ë ˆì„ì„ ë½‘ëŠ” ê°„ê²©\"ìì²´ë¥¼ ì¡°ì ˆ â†’ ëë¶€ë¶„ì— ë” ë§ì´ ëª°ë¦¬ê²Œ ë§Œë“œëŠ” ë°©ì‹)\n",
        "        end_indices = np.array([\n",
        "            start_idx + int((total_frames - start_idx - 1) * (i/end_frames)**2)\n",
        "            for i in range(1, end_frames + 1)\n",
        "        ])\n",
        "\n",
        "        # contextì— ë§ê²Œ ê· ì¼í•˜ê²Œ ë°°í¬ëœ ì´ˆê¸° í”„ë ˆì„ (ì´ˆë°˜ë¶€ì—ì„œ ê· ë“±í•˜ê²Œ ì¶”ì¶œí•œ í”„ë ˆì„ë“¤)\n",
        "        # contextë€? ì‚¬ê³  ì§ì „ì— ì–´ë–¤ ìƒí™©ì´ í¼ì³ì¡ŒëŠ”ì§€ì— ëŒ€í•œ íë¦„, ë°°ê²½, ë§¥ë½\n",
        "        begin_indices = np.linspace(0, start_idx - 1, start_frames, dtype=int) if start_idx > 0 else np.zeros(start_frames, dtype=int)\n",
        "\n",
        "        # ì¸ë±ìŠ¤ ê²°í•©\n",
        "        frame_indices = np.concatenate([begin_indices, end_indices])\n",
        "\n",
        "    # ì„ íƒí•œ í”„ë ˆì„ ì¶”ì¶œ\n",
        "    for idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            # Use higher resolution and better interpolation\n",
        "            frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        else:\n",
        "            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames, dtype=np.uint8)\n",
        "\n",
        "# ë¨¼ì €, ì „ì—­ ë²”ìœ„ì—ì„œ ë³€í™˜ í´ë˜ìŠ¤ë¥¼ ì •ì˜\n",
        "# ì…ë ¥ëœ ì˜ìƒ í”„ë ˆì„ì„ ì¼ì • í™•ë¥ ë¡œ ì¢Œìš° ë°˜ì „ì‹œì¼œì„œ, ë°ì´í„° ë‹¤ì–‘ì„±ì„ ëŠ˜ë¦¬ëŠ” ì—­í• \n",
        "class RandomHorizontalFlip(object):\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        if np.random.random() < self.p:\n",
        "            return frames[:, :, ::-1, :].copy()  # horizontally flip each frame\n",
        "        return frames\n",
        "\n",
        "# ì˜ìƒ í”„ë ˆì„ì˜ ë°ê¸°ì™€ ëŒ€ë¹„ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¡°ì •í•´, ë‹¤ì–‘í•œ ì¡°ëª… í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ì¦ê°• í´ë˜ìŠ¤\n",
        "class ColorJitter(object):\n",
        "    def __init__(self, brightness=0, contrast=0):\n",
        "        self.brightness = brightness\n",
        "        self.contrast = contrast\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        # Apply brightness jitter\n",
        "        if self.brightness > 0:\n",
        "            brightness_factor = np.random.uniform(max(0, 1-self.brightness), 1+self.brightness)\n",
        "            frames = frames * brightness_factor\n",
        "            frames = np.clip(frames, 0, 255)\n",
        "\n",
        "        # Apply contrast jitter\n",
        "        if self.contrast > 0:\n",
        "            contrast_factor = np.random.uniform(max(0, 1-self.contrast), 1+self.contrast)\n",
        "            frames = (frames - 128) * contrast_factor + 128\n",
        "            frames = np.clip(frames, 0, 255)\n",
        "\n",
        "        return frames\n",
        "\n",
        "# í”„ë ˆì„ì— íë¦¿í•œ ì•ˆê°œ íš¨ê³¼ë¥¼ ë„£ì–´, ì‹œì•¼ê°€ ë‚˜ìœ ë‚ ì”¨ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\n",
        "class AddFog(object):\n",
        "    def __call__(self, frames):\n",
        "        fog = np.random.uniform(0.7, 0.9, frames.shape).astype(np.float32)\n",
        "        return frames * 0.8 + fog * 50  # Adjusted for 0-255 scale\n",
        "\n",
        "# í”„ë ˆì„ì— í°ìƒ‰ ì„ í˜• ë…¸ì´ì¦ˆ(ë¹—ë°©ìš¸)ë¥¼ ì¶”ê°€í•´ ë¹„ ì˜¤ëŠ” ë‚ ì”¨ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤\n",
        "class AddRain(object):\n",
        "    def __call__(self, frames):\n",
        "        h, w = frames.shape[1:3]\n",
        "        rain = np.random.uniform(0, 1, (len(frames), h, w, 1)).astype(np.float32)\n",
        "        rain = (rain > 0.97).astype(np.float32) * 200  # White rain drops\n",
        "        return np.clip(frames * 0.9 + rain, 0, 255)  # Darken a bit and add drops\n",
        "\n",
        "# ì§€ì •ëœ í™•ë¥ ì— ë”°ë¼ ì–´ë–¤ ë³€í™˜ì„ ì ìš©í• ì§€ ë§ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê²°ì •í•˜ëŠ” ì»¨íŠ¸ë¡¤ëŸ¬ í´ë˜ìŠ¤(ëœë¤ì„± ë¶€ì—¬)\n",
        "class RandomApply(object):\n",
        "    def __init__(self, transform, p=0.5):\n",
        "        self.transform = transform\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        if np.random.random() < self.p:\n",
        "            return self.transform(frames)\n",
        "        return frames\n",
        "\n",
        "# ì—¬ëŸ¬ ê°œì˜ ë³€í™˜(Flip, Jitter, Fog ë“±)ì„ ìˆœì„œëŒ€ë¡œ ì ìš©í•˜ëŠ” ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        for t in self.transforms:\n",
        "            frames = t(frames)\n",
        "        return frames\n",
        "\n",
        "# ì˜ìƒ í”„ë ˆì„ ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë°”ê¾¸ê³ , í”½ì…€ ê°’ì„ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ëŠ” í´ë˜ìŠ¤\n",
        "class ToTensor(object):\n",
        "    def __call__(self, frames):\n",
        "        # Convert from (T, H, W, C) to (T, C, H, W)\n",
        "        frames = frames.transpose(0, 3, 1, 2)\n",
        "        # Convert to tensor and normalize to [0, 1]\n",
        "        return torch.from_numpy(frames).float() / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.440093Z",
          "iopub.status.busy": "2025-04-20T04:18:07.439866Z",
          "iopub.status.idle": "2025-04-20T04:18:07.455049Z",
          "shell.execute_reply": "2025-04-20T04:18:07.454188Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.440073Z"
        },
        "id": "ykx3ndJRblWI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ë™ì˜ìƒì—ì„œ ë°ì´í„° ì¦ê°•ì„ ìœ„í•œ ë³€í™˜ì„ ë°˜í™˜\n",
        "\n",
        "def get_video_transforms():\n",
        "    \"\"\"\n",
        "    Returns transformations for data augmentation in videos.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'train': Compose([\n",
        "            RandomHorizontalFlip(p=0.5),\n",
        "            ColorJitter(brightness=0.3, contrast=0.3),\n",
        "            RandomApply(AddFog(), p=0.15),\n",
        "            RandomApply(AddRain(), p=0.15),\n",
        "            RandomApply(RandomNoise(0.05), p=0.2),\n",
        "            RandomApply(RandomOcclusion(), p=0.1),\n",
        "            ToTensor()\n",
        "        ]),\n",
        "        'val': Compose([\n",
        "            ToTensor()  # Only tensor conversion for validation\n",
        "        ])\n",
        "    }\n",
        "\n",
        "# ë¹„ë””ì˜¤ í”„ë ˆì„ì—ì„œ ë¬´ì‘ìœ„ ê°€ìš°ì‹œì•ˆ(ì •ê·œë¶„í¬) ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬, ì‹¤ì œ ì´¬ì˜ í™˜ê²½ì—ì„œ\n",
        "# ë°œìƒí•  ìˆ˜ ìˆëŠ” ì¡ìŒì— ëŒ€í•´ ëª¨ë¸ì´ ë” ê°•ê±´í•´ì§€ë„ë¡ ë§Œë“œëŠ” í´ë˜ìŠ¤\n",
        "class RandomNoise(object):\n",
        "    \"\"\"\n",
        "    Applies random Gaussian noise to video frames for data augmentation.\n",
        "\n",
        "    This transformation helps the model become more robust to noise\n",
        "    that may be present in real-world video data.\n",
        "\n",
        "    Args:\n",
        "        std (float): Standard deviation of the Gaussian noise as a fraction\n",
        "                     of the pixel value range (default: 0.05)\n",
        "    \"\"\"\n",
        "    def __init__(self, std=0.05):\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        \"\"\"\n",
        "        Apply random noise to the input frames.\n",
        "\n",
        "        Args:\n",
        "            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n",
        "                                   where T is number of frames\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Noise-augmented frames, clipped to valid pixel range [0, 255]\n",
        "        \"\"\"\n",
        "        # ì§€ì •ëœ í‘œì¤€ í¸ì°¨ë¥¼ ê°€ì§„ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±\n",
        "        noise = np.random.normal(0, self.std * 255, frames.shape).astype(np.float32)\n",
        "\n",
        "        # ìœ íš¨í•œ í”½ì…€ ë²”ìœ„ì— ë…¸ì´ì¦ˆ ë° í´ë¦½ ì¶”ê°€í•˜ê¸°\n",
        "        # ì˜ìƒì€ ì •ìˆ˜í˜• ë°ì´í„°ì—¬ì•¼ í•˜ë¯€ë¡œ í˜• ë³€í™˜ (astype)\n",
        "        return np.clip(frames + noise, 0, 255).astype(np.uint8)\n",
        "\n",
        "# ì˜ìƒ í”„ë ˆì„ì— ê²€ì€ìƒ‰ ì‚¬ê°í˜•ì„ ë¬´ì‘ìœ„ë¡œ ë®ì–´ ì”Œì›Œ, ì¼ë¶€ ì •ë³´ê°€ ê°€ë ¤ì¡Œì„ ë•Œë„ ëª¨ë¸ì´ ê²¬ë”œ ìˆ˜ ìˆë„ë¡ í›ˆë ¨ì‹œí‚¤ëŠ” í´ë˜ìŠ¤\n",
        "class RandomOcclusion(object):\n",
        "    \"\"\"\n",
        "    Simulates occlusion in video frames by adding black rectangles.\n",
        "\n",
        "    This transformation helps the model learn to handle partial occlusions\n",
        "    that may occur in real-world scenarios when objects block the camera view.\n",
        "    \"\"\"\n",
        "    def __call__(self, frames):\n",
        "        \"\"\"\n",
        "        Apply random occlusion to the input frames.\n",
        "\n",
        "        Args:\n",
        "            frames (numpy.ndarray): Input video frames of shape (T, H, W, C)\n",
        "                                   where T is number of frames\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Frames with random occlusion applied\n",
        "        \"\"\"\n",
        "        # í”„ë ˆì„ í•˜ë‚˜ì˜ ì„¸ë¡œ(h), ê°€ë¡œ(w) ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°\n",
        "        h, w = frames.shape[1:3]\n",
        "\n",
        "        # ì „ì²´ í”„ë ˆì„ í¬ê¸°ì˜ 10%~25% ì‚¬ì´ í¬ê¸°ì˜ ê°€ë¦¼ ì˜ì—­ í¬ê¸° ì„¤ì •\n",
        "        occl_h = np.random.randint(int(h * 0.1), int(h * 0.25))\n",
        "        occl_w = np.random.randint(int(w * 0.1), int(w * 0.25))\n",
        "\n",
        "        # ì´ ê°€ë¦¼ ì˜ì—­ì´ ë“¤ì–´ê°ˆ ë¬´ì‘ìœ„ ìœ„ì¹˜ ì¢Œí‘œ ì„¤ì •\n",
        "        occl_x = np.random.randint(0, w - occl_w)\n",
        "        occl_y = np.random.randint(0, h - occl_h)\n",
        "\n",
        "        # ì›ë³¸ í”„ë ˆì„ì„ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ ë³µì‚¬ë³¸ ë§Œë“¤ê¸°\n",
        "        frames_copy = frames.copy()\n",
        "\n",
        "        # í”½ì…€ì„ 0(ê²€ì •ìƒ‰)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  í”„ë ˆì„ì— occlusion ì ìš©\n",
        "        for i in range(len(frames)):\n",
        "            frames_copy[i, occl_y:occl_y+occl_h, occl_x:occl_x+occl_w, :] = 0\n",
        "\n",
        "        return frames_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.457304Z",
          "iopub.status.busy": "2025-04-20T04:18:07.457088Z",
          "iopub.status.idle": "2025-04-20T04:18:07.475068Z",
          "shell.execute_reply": "2025-04-20T04:18:07.474327Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.457283Z"
        },
        "id": "eV2-OVDNblWI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ë¹„ë””ì˜¤ í”„ë ˆì„ ê°„ ì›€ì§ì„(ëª¨ì…˜)ì„ ì¶”ì í•˜ëŠ” 'optical_flow'ë¥¼ ê³„ì‚°í•´, ê°ì²´ë‚˜ ë°°ê²½ì˜ ì´ë™ ë°©í–¥ê³¼ ì†ë„ë¥¼ ë²¡í„° í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "# ë‘ ì—°ì†ëœ ì´ë¯¸ì§€(ë˜ëŠ” í”„ë ˆì„) ì‚¬ì´ì—ì„œ, ê° í”½ì…€ì´ ì–´ë–»ê²Œ ì´ë™í–ˆëŠ”ì§€ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ  -> optical_flow\n",
        "\n",
        "def compute_optical_flow(frames, skip_frames=1):\n",
        "    \"\"\"Calculates optical flow skipping some frames to reduce processing.\"\"\"\n",
        "    if len(frames) < 2:\n",
        "        return np.zeros((1, frames.shape[1], frames.shape[2], 2), dtype=np.float32)\n",
        "\n",
        "    flows = []\n",
        "\n",
        "    # ì²« í”„ë ˆì„ì„ ê·¸ë ˆì´ìŠ¤ì¼€ì¼(í‘ë°± ì´ë¯¸ì§€)ë¡œ ë³€í™˜\n",
        "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # ì—°ì†ëœ í”„ë ˆì„ ìŒë§ˆë‹¤ optical_flow ê³„ì‚°\n",
        "    for i in range(1, len(frames), skip_frames):\n",
        "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_RGB2GRAY)\n",
        "        try:\n",
        "            # ë” ë¹ ë¥¸ ê³„ì‚°ì„ ìœ„í•´ ë§¤ê°œë³€ìˆ˜ ì¤„ì´ê¸°\n",
        "            flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray,\n",
        "                                               None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "            flows.append(flow)\n",
        "        # ì˜ˆì™¸ ì²˜ë¦¬ (ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ 0ìœ¼ë¡œ ëŒ€ì²´)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating optical flow: {str(e)}\")\n",
        "            flows.append(np.zeros((frames.shape[1], frames.shape[2], 2), dtype=np.float32))\n",
        "\n",
        "        prev_gray = curr_gray\n",
        "\n",
        "    if not flows:\n",
        "        return np.zeros((1, frames.shape[1], frames.shape[2], 2), dtype=np.float32)\n",
        "\n",
        "    return np.array(flows, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.476245Z",
          "iopub.status.busy": "2025-04-20T04:18:07.476046Z",
          "iopub.status.idle": "2025-04-20T04:18:07.496397Z",
          "shell.execute_reply": "2025-04-20T04:18:07.495429Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.476231Z"
        },
        "id": "C5kZLuZmblWI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# í•˜ë‚˜ì˜ ë™ì˜ìƒ íŒŒì¼ì„ ë°›ì•„, ì£¼ìš” í”„ë ˆì„ì„ ì¶”ì¶œí•˜ê³  optical_flow(ì›€ì§ì„ ì •ë³´)ë¥¼ ê³„ì‚°í•´ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë°˜í™˜í•˜ëŠ” ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "\n",
        "def process_video(args):\n",
        "    \"\"\"\n",
        "    Function to process an individual video.\n",
        "    \"\"\"\n",
        "    video_path, video_id, num_frames = args\n",
        "    try:\n",
        "        # ë” ë†’ì€ í•´ìƒë„ë¡œ í”„ë ˆì„ ì¶”ì¶œ\n",
        "        frames = extract_keyframes(video_path, num_frames=num_frames, target_size=(160, 160))\n",
        "\n",
        "        # optical flow ê³„ì‚°\n",
        "        optical_flow = compute_optical_flow(frames, skip_frames=1)\n",
        "\n",
        "        # ë³€í™˜ì„ ì ìš©í•˜ëŠ” ëŒ€ì‹  NumPy ë°°ì—´ì„ ë°˜í™˜\n",
        "        return video_id, {\n",
        "            'frames': frames,\n",
        "            'optical_flow': optical_flow,\n",
        "        }\n",
        "    # ì˜ˆì™¸ ì²˜ë¦¬ (Noneì„ ë¦¬í„´)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_id}: {str(e)}\")\n",
        "        return video_id, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.497584Z",
          "iopub.status.busy": "2025-04-20T04:18:07.497243Z",
          "iopub.status.idle": "2025-04-20T04:18:07.514822Z",
          "shell.execute_reply": "2025-04-20T04:18:07.514063Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.497557Z"
        },
        "id": "QzcT6uueblWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ì—¬ëŸ¬ ê°œì˜ ì˜ìƒì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•´ ë¹ ë¥´ê²Œ ì „ì²˜ë¦¬í•˜ê³ , ê° ì˜ìƒì˜ í”„ë ˆì„ ë° optical flow ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "def parallel_preprocess_dataset(video_dir, video_ids, num_frames=8, num_workers=4):\n",
        "    \"\"\"\n",
        "    Pre-processes multiple videos in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    # ë¹„ë””ì˜¤ ëª©ë¡ ì¤€ë¹„ (íŠœí”Œ í˜•íƒœë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€)\n",
        "    args_list = []\n",
        "    for video_id in video_ids:\n",
        "        video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n",
        "        if os.path.exists(video_path):\n",
        "            args_list.append((video_path, video_id, num_frames))\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(f\"Starting parallel pre-processing of {len(args_list)} videos with {num_workers} workers...\")\n",
        "\n",
        "    processed_data = {}\n",
        "\n",
        "    # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘\n",
        "    with Pool(num_workers) as p:\n",
        "        results = p.map(process_video, args_list)\n",
        "        for video_id, data in results:\n",
        "            if data is not None:\n",
        "                processed_data[video_id] = data\n",
        "\n",
        "    print(f\"Pre-processing completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    print(f\"Processed {len(processed_data)} out of {len(args_list)} videos.\")\n",
        "\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.515891Z",
          "iopub.status.busy": "2025-04-20T04:18:07.515672Z",
          "iopub.status.idle": "2025-04-20T04:18:07.702243Z",
          "shell.execute_reply": "2025-04-20T04:18:07.70132Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.515872Z"
        },
        "id": "rD44YqbwblWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# mp4 ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ì§ì ‘ í”„ë ˆì„ê³¼ optical flowë¥¼ ì¶”ì¶œí•˜ë©° í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±\n",
        "class DashcamDataset(Dataset):\n",
        "   def __init__(self, video_dir, annotations, transform=None, num_frames=8):\n",
        "       self.video_dir = video_dir\n",
        "       self.annotations = annotations\n",
        "       self.transform = transform\n",
        "       self.num_frames = num_frames\n",
        "       self.video_ids = list(annotations.keys())\n",
        "\n",
        "   def __len__(self):\n",
        "       return len(self.video_ids)\n",
        "\n",
        "   def __getitem__(self, idx):\n",
        "       video_id = self.video_ids[idx]\n",
        "       video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "       try:\n",
        "           # Check if the file exists\n",
        "           if not os.path.exists(video_path):\n",
        "               print(f\"Video not found: {video_path}\")\n",
        "               raise FileNotFoundError(f\"File not found: {video_path}\")\n",
        "\n",
        "           # Extract frames with reduced resolution\n",
        "           frames = extract_keyframes(video_path, self.num_frames, target_size=(112, 112))\n",
        "\n",
        "           # Calculate optical flow with skip_frames\n",
        "           optical_flow = compute_optical_flow(frames, skip_frames=1)\n",
        "\n",
        "           # Apply transformations\n",
        "           if self.transform:\n",
        "               frames = self.transform(frames)\n",
        "           else:\n",
        "               # Convert to tensor manually\n",
        "               frames = torch.from_numpy(frames.transpose(0, 3, 1, 2)).float() / 255.0\n",
        "\n",
        "           # Convert optical flow to tensor\n",
        "           optical_flow = torch.from_numpy(optical_flow.transpose(0, 3, 1, 2)).float()\n",
        "\n",
        "           # Load label and alert time\n",
        "           label = self.annotations[video_id]['label']\n",
        "           alert_time = self.annotations[video_id].get('alert_time', 0)\n",
        "\n",
        "           return {\n",
        "               'frames': frames,\n",
        "               'optical_flow': optical_flow,\n",
        "               'label': torch.tensor(label).float(),\n",
        "               'alert_time': torch.tensor(alert_time).float(),\n",
        "               'video_id': video_id\n",
        "           }\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error processing video {video_id}: {str(e)}\")\n",
        "           # Create a placeholder for this video\n",
        "           dummy_frames = torch.zeros((self.num_frames, 3, 112, 112))\n",
        "           dummy_flow = torch.zeros((max(1, self.num_frames-1), 2, 112, 112))\n",
        "           return {\n",
        "               'frames': dummy_frames,\n",
        "               'optical_flow': dummy_flow,\n",
        "               'label': torch.tensor(self.annotations[video_id]['label']).float(),\n",
        "               'alert_time': torch.tensor(self.annotations[video_id].get('alert_time', 0)).float(),\n",
        "               'video_id': video_id\n",
        "           }\n",
        "\n",
        "# ì´ë¯¸ ì „ì²˜ë¦¬ëœ ê²°ê³¼(NumPy ë°°ì—´)ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ë¶ˆëŸ¬ì™€ì„œ ë¹ ë¥´ê²Œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±\n",
        "class PreprocessedDashcamDataset(Dataset):\n",
        "   def __init__(self, processed_data, annotations, transform=None):\n",
        "       self.processed_data = processed_data\n",
        "       self.annotations = annotations\n",
        "       self.transform = transform\n",
        "       self.video_ids = list(processed_data.keys())\n",
        "\n",
        "   def __len__(self):\n",
        "       return len(self.video_ids)\n",
        "\n",
        "   def __getitem__(self, idx):\n",
        "       video_id = self.video_ids[idx]\n",
        "       data = self.processed_data[video_id]\n",
        "\n",
        "       # Get frames and optical flow\n",
        "       frames = data['frames']\n",
        "       optical_flow = data['optical_flow']\n",
        "\n",
        "       # Apply transformations to frames\n",
        "       if self.transform:\n",
        "           frames_tensor = self.transform(frames)\n",
        "       else:\n",
        "           # Convert to tensor manually\n",
        "           frames_tensor = torch.from_numpy(frames.transpose(0, 3, 1, 2)).float() / 255.0\n",
        "\n",
        "       # Convert optical flow to tensor\n",
        "       optical_flow_tensor = torch.from_numpy(optical_flow.transpose(0, 3, 1, 2)).float()\n",
        "\n",
        "       # Load label and alert time\n",
        "       label = self.annotations[video_id]['label']\n",
        "       alert_time = self.annotations[video_id].get('alert_time', 0)\n",
        "\n",
        "       return {\n",
        "           'frames': frames_tensor,\n",
        "           'optical_flow': optical_flow_tensor,\n",
        "           'label': torch.tensor(label).float(),\n",
        "           'alert_time': torch.tensor(alert_time).float(),\n",
        "           'video_id': video_id\n",
        "       }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKdwMpeoblWJ"
      },
      "source": [
        "## **AAT-DA ëª¨ë¸ ì…ë ¥ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì²˜ë¦¬**\n",
        "#### **í•„ìˆ˜ êµ¬ì„±ìš”ì†Œ: Object Detection, VGG Feature ì¶”ì¶œ, Driver Attention ë§µ ê³„ì‚°, Attention Weight ê³„ì‚°**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:45:41.906022Z",
          "iopub.status.busy": "2025-04-20T04:45:41.905755Z",
          "iopub.status.idle": "2025-04-20T04:45:41.9129Z",
          "shell.execute_reply": "2025-04-20T04:45:41.911729Z",
          "shell.execute_reply.started": "2025-04-20T04:45:41.906004Z"
        },
        "id": "jMuWHfTYblWJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ì™¸ë¶€ì—ì„œ object_detectorì™€ driver_attention_modelì„ ì¸ìë¡œ ì£¼ì…ë°›ë„ë¡ ì„¤ê³„\n",
        "\n",
        "# ë”ë¯¸ ê°ì²´ íƒì§€ê¸° í´ë˜ìŠ¤\n",
        "class ObjectDetector:\n",
        "    def detect(self, frame):\n",
        "        h, w, _ = frame.shape\n",
        "        return [(w//3, h//3, 2*w//3, 2*h//3)]  # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°•ìŠ¤\n",
        "\n",
        "# ë”ë¯¸ ì‹œì„  ì˜ˆì¸¡ ëª¨ë¸ í´ë˜ìŠ¤\n",
        "class DummyDriverAttention:\n",
        "    def predict(self, frame):\n",
        "        h, w, _ = frame.shape\n",
        "        attention_map = np.zeros((h, w), dtype=np.float32)\n",
        "        cx, cy = w // 2, h // 2\n",
        "        attention_map[cy-10:cy+10, cx-10:cx+10] = 1.0\n",
        "        return cv2.GaussianBlur(attention_map, (25, 25), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.703409Z",
          "iopub.status.busy": "2025-04-20T04:18:07.703171Z",
          "iopub.status.idle": "2025-04-20T04:18:07.725727Z",
          "shell.execute_reply": "2025-04-20T04:18:07.724961Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.703391Z"
        },
        "id": "MffQj4R4blWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torchvision import models, transforms\n",
        "\n",
        "class AATDAPreprocessor:\n",
        "    def __init__(self, object_detector, driver_attention_model, vgg_model=None):\n",
        "        self.detector = object_detector  # Cascade R-CNN ë“± ì™¸ë¶€ ê°ì§€ê¸°\n",
        "        self.attention_model = driver_attention_model  # Gate-DAP ë“±\n",
        "        self.vgg = vgg_model or models.vgg16(pretrained=True).features.eval()\n",
        "        self.vgg_fc7 = torch.nn.Sequential(*list(models.vgg16(pretrained=True).classifier.children())[:6]).eval()\n",
        "        self.vgg_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # ê°ì§€ëœ ê°ì²´ ì˜ì—­ì„ ì˜ë¼ì„œ VGG FC7 feature(4096-dim) ì¶”ì¶œ\n",
        "    def extract_object_features(self, frame, boxes):\n",
        "        features = []\n",
        "        for (x1, y1, x2, y2) in boxes:\n",
        "            crop = frame[y1:y2, x1:x2]\n",
        "            if crop.shape[0] == 0 or crop.shape[1] == 0:\n",
        "                features.append(torch.zeros(4096))\n",
        "                continue\n",
        "            input_tensor = self.vgg_transform(crop).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                feat = self.vgg(input_tensor)\n",
        "                feat = feat.view(feat.size(0), -1)\n",
        "                fc7 = self.vgg_fc7(feat).squeeze(0)\n",
        "            features.append(fc7)\n",
        "        return torch.stack(features)  # Shape: (N, 4096)\n",
        "\n",
        "    # attention mapê³¼ ê°ì²´ ì˜ì—­ì´ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ í‰ê· ê°’ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
        "    def compute_attention_weights(self, attention_map, boxes):\n",
        "        weights = []\n",
        "        for (x1, y1, x2, y2) in boxes:\n",
        "            patch = attention_map[y1:y2, x1:x2]\n",
        "            weight = np.mean(patch) if patch.size > 0 else 0.0\n",
        "            weights.append(weight)\n",
        "        return torch.tensor(weights, dtype=torch.float32)  # Shape: (N,)\n",
        "\n",
        "    # í•œ í”„ë ˆì„ì— ëŒ€í•´ ì•„ë˜ì™€ ê°™ì´ ìˆœì„œëŒ€ë¡œ ì‘ë™\n",
        "    def process_frame(self, frame):\n",
        "        # 1. ê°ì²´ ê°ì§€\n",
        "        boxes = self.detector.detect(frame)  # returns List of [x1, y1, x2, y2]\n",
        "\n",
        "        # 2. VGG Feature ì¶”ì¶œ\n",
        "        object_feats = self.extract_object_features(frame, boxes)  # (N, 4096)\n",
        "\n",
        "        # 3. ì‹œì„  ë§µ ìƒì„±\n",
        "        attention_map = self.attention_model.predict(frame)  # (H, W), numpy float32\n",
        "\n",
        "        # 4. Attention Weight ê³„ì‚°\n",
        "        weights = self.compute_attention_weights(attention_map, boxes)  # (N,)\n",
        "\n",
        "        # 5. Feature ê°•í™”\n",
        "        weighted_feats = object_feats * weights.unsqueeze(1)  # (N, 4096)\n",
        "\n",
        "        return weighted_feats, weights, boxes  # Transformer ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnpF-ZQnblWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ì‹¤í–‰ ì˜ˆì‹œ\n",
        "if __name__ == '__main__':\n",
        "    detector = ObjectDetector()\n",
        "    attention_model = DummyDriverAttention()\n",
        "    processor = AATDAPreprocessor(detector, attention_model)\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    frame = cv2.imread(\"sample_frame.jpg\")  # ì‹¤ì œ ê²½ë¡œë¡œ ì‘ì„±í•˜ê¸°!!!\n",
        "    if frame is not None:\n",
        "        weighted_feats, weights, boxes = processor.process_frame(frame)\n",
        "        print(\"ê°ì§€ëœ ê°ì²´ ìˆ˜:\", len(boxes))\n",
        "        print(\"Feature shape:\", weighted_feats.shape)  # (N, 4096)\n",
        "    else:\n",
        "        print(\"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:18:07.726751Z",
          "iopub.status.busy": "2025-04-20T04:18:07.726501Z",
          "iopub.status.idle": "2025-04-20T04:18:07.748259Z",
          "shell.execute_reply": "2025-04-20T04:18:07.747283Z",
          "shell.execute_reply.started": "2025-04-20T04:18:07.726729Z"
        },
        "id": "oKA0CXEIblWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ë¹„ë””ì˜¤ í•˜ë‚˜ì— í¬í•¨ëœ ì—¬ëŸ¬ í”„ë ˆì„ì„ process_frame()ìœ¼ë¡œ ë°˜ë³µ ì²˜ë¦¬\n",
        "# ê° í”„ë ˆì„ì˜ ê°ì²´ featureë¥¼ (T, N, 4096) êµ¬ì¡°ë¡œ íŒ¨ë”©í•˜ì—¬ Transformer ì…ë ¥ì— ë§ì¶¤\n",
        "\n",
        "class AATDADataset(Dataset):\n",
        "    def __init__(self, video_data_dict, annotations, processor, num_frames=8):\n",
        "        self.video_data = video_data_dict  # {video_id: [frame1, frame2, ...]}\n",
        "        self.annotations = annotations  # {video_id: {label: ..., alert_time: ...}}\n",
        "        self.processor = processor\n",
        "        self.num_frames = num_frames\n",
        "        self.video_ids = list(video_data_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.video_ids[idx]\n",
        "        frames = self.video_data[video_id]  # raw RGB frames\n",
        "\n",
        "        # ê° í”„ë ˆì„ì— ëŒ€í•´ Object Feature + Attention ì²˜ë¦¬\n",
        "        sequence_features = []\n",
        "        for frame in frames:\n",
        "            weighted_feats, _, _ = self.processor.process_frame(frame)\n",
        "            sequence_features.append(weighted_feats)\n",
        "\n",
        "        # ì‹œí€€ìŠ¤ë¥¼ (T, N, 4096) í˜•íƒœì˜ tensorë¡œ ì •ë¦¬ (padding ê°€ëŠ¥)\n",
        "        max_objects = max(f.shape[0] for f in sequence_features)\n",
        "        padded_seq = []\n",
        "        for f in sequence_features:\n",
        "            pad_size = max_objects - f.shape[0]\n",
        "            if pad_size > 0:\n",
        "                padded = torch.cat([f, torch.zeros(pad_size, 4096)], dim=0)\n",
        "            else:\n",
        "                padded = f\n",
        "            padded_seq.append(padded)\n",
        "\n",
        "        # ìµœì¢… ì‹œí€€ìŠ¤: (T, N, 4096)\n",
        "        input_tensor = torch.stack(padded_seq)\n",
        "\n",
        "        label = torch.tensor(self.annotations[video_id]['label']).float()\n",
        "        alert_time = torch.tensor(self.annotations[video_id].get('alert_time', 0)).float()\n",
        "\n",
        "        return {\n",
        "            'video_id': video_id,\n",
        "            'input': input_tensor,  # (T, N, 4096)\n",
        "            'label': label,\n",
        "            'alert_time': alert_time\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBDUYnfQblWK"
      },
      "source": [
        "### **1ë‹¨ê³„: ì¼ë°˜ ì „ì²˜ë¦¬**\n",
        "#### **(1) í”„ë ˆì„ ì¶”ì¶œ - extract_keyframes()**\n",
        "#### **(2) Optical Flow ê³„ì‚° - compute_optical_flow()**\n",
        "#### **(3) ê¸°ë³¸ ì¦ê°• - ColorJitter, AddRain, AddFog, ToTensor() ë“±**\n",
        "#### **(4) ë°ì´í„° êµ¬ì„± - DashcamDataset ë˜ëŠ” PreprocessDashcamDatasetìœ¼ë¡œ êµ¬ì„±**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3It0odGdblWK"
      },
      "source": [
        "### **2ë‹¨ê³„: AAT-DA ì „ìš© ì „ì²˜ë¦¬ (ê¸°ì¡´ ë°ì´í„°ì—ì„œ Transformerìš© êµ¬ì¡° ë³€í™˜)**\n",
        "##### AAT-DAëŠ” ë‹¨ìˆœí•œ ì˜ìƒ í”„ë ˆì„ì´ ì•„ë‹ˆë¼, \"ê°ì²´ ì¤‘ì‹¬ì˜ ì‹œê³µê°„ Attention ì…ë ¥ êµ¬ì¡°\"ë¥¼ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ê°€ í•„ìš”\n",
        "#### **(1) ê°ì²´ ê°ì§€ - í”„ë ˆì„ì—ì„œ ê°ì²´ ê°ì§€ (Cascade R-CNN ë“±)**\n",
        "#### **(2) ê°ì²´ íŠ¹ì§• ì¶”ì¶œ - ê°ì§€ëœ ë°•ìŠ¤ë§ˆë‹¤ VGG16 FC7 feature ì¶”ì¶œ (4096-dim)**\n",
        "#### **(3) ì‹œì„  ë§µ ì˜ˆì¸¡ - Gate-DAP ë“±ìœ¼ë¡œ driver attention heatmap ìƒì„±**\n",
        "#### **(4) ì£¼ì˜ ê°€ì¤‘ì¹˜ ê³„ì‚° - ì‹œì„  ë§µ + ê°ì²´ ìœ„ì¹˜ â†’ ê°ì²´ë³„ attention weight Î±áµ¢ ê³„ì‚°**\n",
        "#### **(5) Feature ê°€ì¤‘ - ê°ì²´ feature Î±áµ¢ â†’ ê°•ì¡°ëœ ê°ì²´ feature**\n",
        "#### **(6) ì‹œí€€ìŠ¤ êµ¬ì„± - ëª¨ë“  í”„ë ˆì„ì˜ ê²°ê³¼ë¥¼ (T, N, 4096) ì‹œí€€ìŠ¤ë¡œ íŒ¨ë”© ì •ë¦¬**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-20T04:49:20.530493Z",
          "iopub.status.busy": "2025-04-20T04:49:20.530191Z",
          "iopub.status.idle": "2025-04-20T04:49:20.538492Z",
          "shell.execute_reply": "2025-04-20T04:49:20.537799Z",
          "shell.execute_reply.started": "2025-04-20T04:49:20.53047Z"
        },
        "id": "iNlczjmdblWK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# AAT-DA ëª¨ë¸ êµ¬ì„±\n",
        "# ì…ë ¥: (B, T, N, 4096)\n",
        "# êµ¬ì„±: Spatial Transformer + Temporal Transformer + Classifier\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ê° í”„ë ˆì„ ë‚´ ê°ì²´ë“¤ ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ (Object Self-Attention)\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048):\n",
        "        super().__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, N, 4096)\n",
        "        B, T, N, C = x.shape\n",
        "        x = x.view(B * T, N, C)  # (B*T, N, C)\n",
        "        x = self.encoder(x)  # (B*T, N, C)\n",
        "        x = x.mean(dim=1)  # (B*T, C)\n",
        "        x = x.view(B, T, C)  # (B, T, C)\n",
        "        return x\n",
        "\n",
        "# ì‹œê°„ ìˆœì„œì— ë”°ë¥¸ ì˜ë¯¸ íë¦„ ëª¨ë¸ë§ (Temporal Attention)\n",
        "class TemporalTransformer(nn.Module):\n",
        "    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048):\n",
        "        super().__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, C)\n",
        "        x = x.transpose(0, 1)  # (T, B, C)\n",
        "        x = self.encoder(x)  # (T, B, C)\n",
        "        x = x.transpose(0, 1)  # (B, T, C)\n",
        "        return x\n",
        "\n",
        "class AATDA(nn.Module):\n",
        "    def __init__(self, d_model=4096, nhead=8, dim_feedforward=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.spatial_transformer = SpatialTransformer(d_model, nhead, dim_feedforward)\n",
        "        self.temporal_transformer = TemporalTransformer(d_model, nhead, dim_feedforward)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: (B, T, N, 4096)\n",
        "        x = self.spatial_transformer(x)     # (B, T, 4096)\n",
        "        x = self.temporal_transformer(x)    # (B, T, 4096)\n",
        "        x = x.mean(dim=1)                   # (B, 4096) - í‰ê·  í’€ë§\n",
        "        out = self.classifier(x)            # (B, 1)\n",
        "        return out.squeeze(-1)              # (B,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FsQfdj3blWL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¡œë” ì—°ê²°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•™ìŠµ í•¨ìˆ˜ ì‘ì„±\n",
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in loader:\n",
        "        inputs = batch['input'].to(device)         # (B, T, N, 4096)\n",
        "        labels = batch['label'].to(device).float() # (B,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)                    # (B,)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 11038207,
          "sourceId": 92399,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
